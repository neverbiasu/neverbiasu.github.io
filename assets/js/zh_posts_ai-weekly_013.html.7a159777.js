"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[1944],{6262:(e,t)=>{t.A=(e,t)=>{const a=e.__vccOpts||e;for(const[e,i]of t)a[e]=i;return a}},8276:(e,t,a)=>{a.r(t),a.d(t,{comp:()=>r,data:()=>o});var i=a(641);const n={},r=(0,a(6262).A)(n,[["render",function(e,t){return(0,i.uX)(),(0,i.CE)("div",null,t[0]||(t[0]=[(0,i.Fv)('<h1 id="joyvasa突破多模态动画生成-dino-x定义开放世界检测-stylecodes实现风格编码迁移【ai周报】" tabindex="-1"><a class="header-anchor" href="#joyvasa突破多模态动画生成-dino-x定义开放世界检测-stylecodes实现风格编码迁移【ai周报】"><span>JoyVASA突破多模态动画生成 | DINO-X定义开放世界检测 | StyleCodes实现风格编码迁移【AI周报】</span></a></h1><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img18@main/2024/11/24/1732457006798-1774d1ab-5764-437c-a50e-f922f2198952.png" alt="封面" tabindex="0" loading="lazy"><figcaption>封面</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span><strong>摘要</strong></span></a></h2><p>本周生成与检测技术亮点：JoyVASA 利用扩散模型实现音频驱动动画生成，支持人像与动物；DINO-X 面向开放世界目标检测，通过多模态提示提升长尾检测表现；StyleCodes 将图像风格编码为Base64格式，简化迁移过程并提升生成灵活性与质量。详情见正文。</p><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span><strong>目录</strong></span></a></h2><ol><li><a href="#joyvasa%E5%9F%BA%E4%BA%8Ediffusion%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E5%8A%A8%E7%94%BB%E7%94%9F%E6%88%90">JoyVASA：基于Diffusion模型的多模态动画生成</a></li><li><a href="#animateanything%E7%94%A8%E4%BA%8E%E7%94%9F%E6%88%90%E8%A7%86%E9%A2%91%E7%9A%84%E4%B8%80%E8%87%B4%E4%B8%94%E5%8F%AF%E6%8E%A7%E7%9A%84%E5%8A%A8%E7%94%BB">AnimateAnything: 用于生成视频的一致且可控的动画</a></li><li><a href="#echomimic-v2%E9%9F%B3%E9%A2%91%E9%A9%B1%E5%8A%A8%E7%9A%84%E4%BA%BA%E5%83%8F%E5%8A%A8%E7%94%BB%E7%94%9F%E6%88%90">EchoMimic V2：音频驱动的人像动画生成</a></li><li><a href="#wala%E5%9F%BA%E4%BA%8E%E6%B3%A2%E5%BD%A2%E5%8E%8B%E7%BC%A9%E7%9A%843d%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">WaLa：基于波形压缩的3D生成模型</a></li><li><a href="#dino-x%E5%BC%80%E6%94%BE%E4%B8%96%E7%95%8C%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B">DINO-X：开放世界目标检测的统一视觉模型</a></li><li><a href="#stylecodes%E5%9F%BA%E4%BA%8E%E7%BC%96%E7%A0%81%E7%9A%84%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E6%A1%86%E6%9E%B6">StyleCodes：基于编码的风格迁移框架</a></li></ol><hr><h2 id="joyvasa-基于diffusion模型的多模态动画生成" tabindex="-1"><a class="header-anchor" href="#joyvasa-基于diffusion模型的多模态动画生成"><span>JoyVASA：基于Diffusion模型的多模态动画生成</span></a></h2><figure><img src="https://jdh-algo.github.io/JoyVASA/static/images/pipeline_inference.png" alt="JoyVASA Pipeline 图" tabindex="0" loading="lazy"><figcaption>JoyVASA Pipeline 图</figcaption></figure><p><strong>概要</strong>：<strong>JoyVASA</strong> 是由 <strong>京东健康</strong> 和 <strong>浙江大学</strong> 联合开发的动画生成算法，结合Diffusion模型和Transformer架构，专注于音频驱动的表情与头部动作生成。其创新在于将动态动作与静态3D人脸解耦，允许生成长时间、高质量的动画，同时支持人像与动物动画。该方法还能利用多语言音频输入，生成与内容匹配的自然表情和动作。</p><p><strong>标签</strong>：#动画生成 #多模态AI #Diffusion模型 #浙江大学 #TalkingHead</p><hr><h2 id="animateanything-用于生成视频的一致且可控的动画" tabindex="-1"><a class="header-anchor" href="#animateanything-用于生成视频的一致且可控的动画"><span>AnimateAnything: 用于生成视频的一致且可控的动画</span></a></h2><p><img src="https://yu-shaonian.github.io/Animate_Anything/imgs/pipeline.png" alt="AnimateAnything Pipeline 图" loading="lazy"><strong>概要</strong>：<strong>浙大CAD&amp;CG重点实验室</strong> 联合提出了一种统一且可控的视频生成方法，通过将不同类型的控制信号（如物体运动、摄像机轨迹等）转换为统一的光流表示，以实现对视频生成过程中的精确控制。该方法采用多尺度控制特征融合网络构建通用运动表示，并利用频率稳定模块减少大规模运动导致的闪烁问题，确保视频的时间连贯性。实验结果表明，该方法在多种条件下均优于现有技术，展现出强大的泛化能力。</p><p><strong>标签</strong>：#视频生成 #光流 #控制生成 #AdaLN #3D人脸</p><hr><h2 id="echomimic-v2-音频驱动的人像动画生成" tabindex="-1"><a class="header-anchor" href="#echomimic-v2-音频驱动的人像动画生成"><span>EchoMimic V2：音频驱动的人像动画生成</span></a></h2><figure><img src="https://antgroup.github.io/ai/echomimic_v2/asserts/teaser.png" alt="EchoMimic V2 Teaser 图" tabindex="0" loading="lazy"><figcaption>EchoMimic V2 Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>EchoMimic V2</strong> 是由 <strong>蚂蚁集团</strong> 设计的一款音频驱动的数字人生成方法，支持通过音频、面部关键点或两者结合生成自然流畅的人像视频。通过创新的联合训练策略，系统实现了对音频信号和面部动态的高精度建模，解决了传统方法中音频驱动不稳定或关键点驱动动作不自然的问题。实验显示，其生成质量在多种基准数据集上优于现有方法。</p><p><strong>标签</strong>：#音频驱动 #TalkingHead #蚂蚁集团 #多模态AI</p><hr><h2 id="wala-基于波形压缩的3d生成模型" tabindex="-1"><a class="header-anchor" href="#wala-基于波形压缩的3d生成模型"><span>WaLa：基于波形压缩的3D生成模型</span></a></h2><figure><img src="https://autodeskailab.github.io/WaLaProject/assets/images/arch.png" alt="WaLa Architecture 图" tabindex="0" loading="lazy"><figcaption>WaLa Architecture 图</figcaption></figure><p><strong>概要</strong>：<strong>WaLa (Wavelet Latent Diffusion)</strong> 是由 <strong>Autodesk AI Lab</strong> 开发的3D生成模型，采用基于波形的紧凑潜在编码，将256³分辨率的3D形状压缩为12³×4的潜在网格，实现高达2427倍的压缩率，且细节损失最小。其Diffusion模型具备十亿级参数，支持条件与无条件的高效生成，生成时间仅需2至4秒。WaLa 显著提升了3D生成的质量、效率和多样性，并支持多视图图像、文本提示和点云等多种输入。</p><p><strong>标签</strong>：#3D生成 #Diffusion模型 #波形压缩 #Autodesk AI Lab</p><hr><h2 id="dino-x-开放世界目标检测的统一视觉模型" tabindex="-1"><a class="header-anchor" href="#dino-x-开放世界目标检测的统一视觉模型"><span>DINO-X：开放世界目标检测的统一视觉模型</span></a></h2><figure><img src="https://dds-blogs.oss-cn-shenzhen.aliyuncs.com/assets/1732161/173216156_dino_x_teaser_1.jpg" alt="DINO-X Teaser 图" tabindex="0" loading="lazy"><figcaption>DINO-X Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>DINO-X</strong> 是由 <strong>IDEA Research</strong> 研发的一种开放世界目标检测模型，基于 Transformer 编解码架构，通过融合文本、视觉和自定义提示，支持开放词汇目标检测。模型利用了超过1亿高质量数据集（Grounding-100M）进行预训练，显著提升了长尾目标检测能力。在多个基准测试中，DINO-X 实现了SOTA性能，包括在 COCO 和 LVIS 数据集上的检测表现，并在稀有类别识别任务中有显著突破。</p><p><strong>标签</strong>：#目标检测 #开放世界AI #Transformer #IDEA Research #GroundingDINO</p><hr><h2 id="stylecodes-基于编码的风格迁移框架" tabindex="-1"><a class="header-anchor" href="#stylecodes-基于编码的风格迁移框架"><span>StyleCodes：基于编码的风格迁移框架</span></a></h2><figure><img src="https://ciarastrawberry.github.io/stylecodes.github.io/website/Demo-Image_5.png" alt="StyleCodes Teaser 图" tabindex="0" loading="lazy"><figcaption>StyleCodes Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>StyleCodes</strong> 是由 <strong>Ciara Rowles</strong> 提出的创新风格迁移框架，能够将图像风格编码为20字符的Base64格式（style-reference codes, srefs），以简化风格迁移的过程。通过使用潜变量自动编码器和UNet控制模块，StyleCodes 提供了一种高效且开放的风格控制方法，适合图像到图像、文本到图像等生成任务。实验显示，该方法在灵活性和生成质量上均表现优异。</p><p><strong>标签</strong>：#风格迁移 #Diffusion模型 #图像生成 #图像编码</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h3><ol><li><a href="https://360cvgroup.github.io/HiCo_T2I/" target="_blank" rel="noopener noreferrer">HiCo-T2I 项目主页</a></li><li><a href="https://github.com/360CVGroup/HiCo_T2I" target="_blank" rel="noopener noreferrer">HiCo-T2I GitHub 仓库</a></li><li><a href="https://arxiv.org/pdf/2410.14324" target="_blank" rel="noopener noreferrer">HiCo-T2I 论文</a></li><li><a href="https://huggingface.co/qihoo360" target="_blank" rel="noopener noreferrer">HiCo-T2I 模型（Hugging Face）</a></li><li><a href="https://github.com/Tencent/Hunyuan3D-1" target="_blank" rel="noopener noreferrer">Hunyuan3D-1 GitHub 仓库</a></li><li><a href="https://huggingface.co/tencent/Hunyuan3D-1" target="_blank" rel="noopener noreferrer">Hunyuan3D-1 模型（Hugging Face）</a></li><li><a href="https://3d.hunyuan.tencent.com/hunyuan3d.pdf" target="_blank" rel="noopener noreferrer">Hunyuan3D-1 项目介绍 PDF</a></li><li><a href="https://haoyuhsu.github.io/autovfx-website/" target="_blank" rel="noopener noreferrer">AutoVFX 项目主页</a></li><li><a href="https://github.com/haoyuhsu/autovfx" target="_blank" rel="noopener noreferrer">AutoVFX GitHub 仓库</a></li><li><a href="https://arxiv.org/pdf/2411.02394" target="_blank" rel="noopener noreferrer">AutoVFX 论文</a></li><li><a href="https://songkey.github.io/hellomeme/" target="_blank" rel="noopener noreferrer">HelloMeme 项目主页</a></li><li><a href="https://github.com/HelloVision/HelloMeme" target="_blank" rel="noopener noreferrer">HelloMeme GitHub 仓库</a></li><li><a href="https://arxiv.org/pdf/2410.22901" target="_blank" rel="noopener noreferrer">HelloMeme 论文</a></li><li><a href="https://mvpaint.github.io/" target="_blank" rel="noopener noreferrer">MVPaint 项目主页</a></li><li><a href="https://github.com/3DTopia/MVPaint" target="_blank" rel="noopener noreferrer">MVPaint GitHub 仓库</a></li><li><a href="https://arxiv.org/pdf/2411.02336" target="_blank" rel="noopener noreferrer">MVPaint 论文</a></li><li><a href="https://www.yongshengyu.com/PromptFix-Page/" target="_blank" rel="noopener noreferrer">PromptFix 项目主页</a></li><li><a href="https://github.com/yeates/promptfix" target="_blank" rel="noopener noreferrer">PromptFix GitHub 仓库</a></li><li><a href="https://arxiv.org/pdf/2405.16785" target="_blank" rel="noopener noreferrer">PromptFix 论文</a></li><li><a href="https://github.com/Ldhlwh/DomainGallery" target="_blank" rel="noopener noreferrer">DomainGallery GitHub 仓库</a></li><li><a href="https://arxiv.org/pdf/2411.04571" target="_blank" rel="noopener noreferrer">DomainGallery 论文</a></li></ol>',38)]))}]]),o=JSON.parse('{"path":"/zh/posts/ai-weekly/013.html","title":"JoyVASA突破多模态动画生成 | DINO-X定义开放世界检测 | StyleCodes实现风格编码迁移【AI周报】","lang":"zh-CN","frontmatter":{"description":"JoyVASA突破多模态动画生成 | DINO-X定义开放世界检测 | StyleCodes实现风格编码迁移【AI周报】 封面封面 摘要 本周生成与检测技术亮点：JoyVASA 利用扩散模型实现音频驱动动画生成，支持人像与动物；DINO-X 面向开放世界目标检测，通过多模态提示提升长尾检测表现；StyleCodes 将图像风格编码为Base64格式，简...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/013.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"JoyVASA突破多模态动画生成 | DINO-X定义开放世界检测 | StyleCodes实现风格编码迁移【AI周报】"}],["meta",{"property":"og:description","content":"JoyVASA突破多模态动画生成 | DINO-X定义开放世界检测 | StyleCodes实现风格编码迁移【AI周报】 封面封面 摘要 本周生成与检测技术亮点：JoyVASA 利用扩散模型实现音频驱动动画生成，支持人像与动物；DINO-X 面向开放世界目标检测，通过多模态提示提升长尾检测表现；StyleCodes 将图像风格编码为Base64格式，简..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://fastly.jsdelivr.net/gh/bucketio/img18@main/2024/11/24/1732457006798-1774d1ab-5764-437c-a50e-f922f2198952.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"JoyVASA突破多模态动画生成 | DINO-X定义开放世界检测 | StyleCodes实现风格编码迁移【AI周报】\\",\\"image\\":[\\"https://fastly.jsdelivr.net/gh/bucketio/img18@main/2024/11/24/1732457006798-1774d1ab-5764-437c-a50e-f922f2198952.png\\",\\"https://jdh-algo.github.io/JoyVASA/static/images/pipeline_inference.png\\",\\"https://yu-shaonian.github.io/Animate_Anything/imgs/pipeline.png\\",\\"https://antgroup.github.io/ai/echomimic_v2/asserts/teaser.png\\",\\"https://autodeskailab.github.io/WaLaProject/assets/images/arch.png\\",\\"https://dds-blogs.oss-cn-shenzhen.aliyuncs.com/assets/1732161/173216156_dino_x_teaser_1.jpg\\",\\"https://ciarastrawberry.github.io/stylecodes.github.io/website/Demo-Image_5.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"JoyVASA：基于Diffusion模型的多模态动画生成","slug":"joyvasa-基于diffusion模型的多模态动画生成","link":"#joyvasa-基于diffusion模型的多模态动画生成","children":[]},{"level":2,"title":"AnimateAnything: 用于生成视频的一致且可控的动画","slug":"animateanything-用于生成视频的一致且可控的动画","link":"#animateanything-用于生成视频的一致且可控的动画","children":[]},{"level":2,"title":"EchoMimic V2：音频驱动的人像动画生成","slug":"echomimic-v2-音频驱动的人像动画生成","link":"#echomimic-v2-音频驱动的人像动画生成","children":[]},{"level":2,"title":"WaLa：基于波形压缩的3D生成模型","slug":"wala-基于波形压缩的3d生成模型","link":"#wala-基于波形压缩的3d生成模型","children":[]},{"level":2,"title":"DINO-X：开放世界目标检测的统一视觉模型","slug":"dino-x-开放世界目标检测的统一视觉模型","link":"#dino-x-开放世界目标检测的统一视觉模型","children":[]},{"level":2,"title":"StyleCodes：基于编码的风格迁移框架","slug":"stylecodes-基于编码的风格迁移框架","link":"#stylecodes-基于编码的风格迁移框架","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":5.1,"words":1531},"filePathRelative":"zh/posts/ai-weekly/013.md","excerpt":"\\n<figure><img src=\\"https://fastly.jsdelivr.net/gh/bucketio/img18@main/2024/11/24/1732457006798-1774d1ab-5764-437c-a50e-f922f2198952.png\\" alt=\\"封面\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面</figcaption></figure>\\n<h2><strong>摘要</strong></h2>\\n<p>本周生成与检测技术亮点：JoyVASA 利用扩散模型实现音频驱动动画生成，支持人像与动物；DINO-X 面向开放世界目标检测，通过多模态提示提升长尾检测表现；StyleCodes 将图像风格编码为Base64格式，简化迁移过程并提升生成灵活性与质量。详情见正文。</p>","autoDesc":true}')}}]);