"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[1830],{66262:(e,a)=>{a.A=(e,a)=>{const n=e.__vccOpts||e;for(const[e,t]of a)n[e]=t;return n}},17366:(e,a,n)=>{n.r(a),n.d(a,{comp:()=>i,data:()=>o});var t=n(20641);const r={},i=(0,n(66262).A)(r,[["render",function(e,a){return(0,t.uX)(),(0,t.CE)("div",null,a[0]||(a[0]=[(0,t.Fv)('<h1 id="wan2-2突破720p三模态视频生成-z-ai开源355b智能体glm-4-5-intern-s1科学推理235b-moe【hf周报】" tabindex="-1"><a class="header-anchor" href="#wan2-2突破720p三模态视频生成-z-ai开源355b智能体glm-4-5-intern-s1科学推理235b-moe【hf周报】"><span>Wan2.2突破720P三模态视频生成 | Z.ai开源355B智能体GLM-4.5 | Intern-S1科学推理235B MoE【HF周报】</span></a></h1><figure><img src="/assets/images/placeholder.png" alt="封面图" tabindex="0" loading="lazy"><figcaption>封面图</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：Wan2.2发布5B和14B模型支持720P视频生成，Z.ai推出355B MoE架构的GLM-4.5智能体基础模型，Intern-S1发布235B科学推理多模态模型专攻科研领域。详细内容见下文，相关参考链接请见文末。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#wan22">Wan2.2：首个支持文本-图像-视频三模态生成的720P模型</a></li><li><a href="#glm-45">GLM-4.5：355B参数MoE架构的开源智能体基础模型</a></li><li><a href="#intern-s1">Intern-S1：235B MoE科学推理多模态模型</a></li><li><a href="#step3">Step3：321B参数高效多模态推理模型</a></li><li><a href="#dotsocr">dots.ocr：1.7B参数多语言文档解析统一模型</a></li><li><a href="#skywork-unipic-15b">Skywork-UniPic-1.5B：统一视觉理解与生成模型</a></li><li><a href="#gpt-image-edit-15m">GPT-Image-Edit-1.5M：百万级GPT生成图像编辑数据集</a></li><li><a href="#flux1-krea-dev">FLUX.1-Krea-dev：高质量快速图像生成在线体验</a></li></ol><hr><h2 id="wan2-2-首个支持文本-图像-视频三模态生成的720p模型" tabindex="-1"><a class="header-anchor" href="#wan2-2-首个支持文本-图像-视频三模态生成的720p模型"><span>Wan2.2：首个支持文本-图像-视频三模态生成的720P模型</span></a></h2><figure><img src="https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B/resolve/main/assets/performance.png" alt="Wan2.2 Performance 图" tabindex="0" loading="lazy"><figcaption>Wan2.2 Performance 图</figcaption></figure><p><strong>概要</strong>：<strong>Wan-AI</strong>发布Wan2.2视频生成模型，这是业界首个统一支持文本到视频、图像到视频以及文本-图像到视频三种模态的720P高清视频生成系统。该模型引入了创新的MoE架构，通过高噪声专家和低噪声专家的动态切换，在保持计算成本不变的情况下大幅提升模型容量。5B版本采用高压缩Wan2.2-VAE实现4×32×32压缩比，可在单张RTX 4090上9分钟内生成5秒720P@24fps视频，在性能和效率方面均达到行业领先水平。</p><p><strong>标签</strong>：#Wan-AI #MoE架构 #多模态视频生成 #Wan2.2系列 #高清视频合成</p><hr><h2 id="glm-4-5-355b参数moe架构的开源智能体基础模型" tabindex="-1"><a class="header-anchor" href="#glm-4-5-355b参数moe架构的开源智能体基础模型"><span>GLM-4.5：355B参数MoE架构的开源智能体基础模型</span></a></h2><figure><img src="https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/bench.png" alt="GLM-4.5 Benchmark 图" tabindex="0" loading="lazy"><figcaption>GLM-4.5 Benchmark 图</figcaption></figure><p><strong>概要</strong>：<strong>Z.ai</strong>开源GLM-4.5智能体基础模型，这是一个采用355B总参数、32B激活参数的MoE架构大语言模型，专为智能体应用而设计。该模型统一了推理、编程和智能体能力，支持思考模式和非思考模式两种推理方式，在12项行业标准基准测试中获得63.2分的优异成绩，位列开源和商业模型第三位。GLM-4.5-Air紧凑版本采用106B总参数、12B激活参数，在保持高效率的同时达到59.8分的竞争性表现。</p><p><strong>标签</strong>：#Z.ai #智能体模型 #混合推理 #GLM系列 #开源商用</p><hr><h2 id="intern-s1-235b-moe科学推理多模态模型" tabindex="-1"><a class="header-anchor" href="#intern-s1-235b-moe科学推理多模态模型"><span>Intern-S1：235B MoE科学推理多模态模型</span></a></h2><figure><img src="https://cdn-uploads.huggingface.co/production/uploads/642695e5274e7ad464c8a5ba/E43cgEXBRWjVJlU_-hdh6.png" alt="Intern-S1 Logo 图" tabindex="0" loading="lazy"><figcaption>Intern-S1 Logo 图</figcaption></figure><p><strong>概要</strong>：<strong>InternLM团队</strong>发布Intern-S1多模态推理模型，基于235B MoE语言模型和6B视觉编码器构建，在5万亿多模态数据上进行连续预训练，其中超过2.5万亿为科学领域专用数据。该模型在保持强大通用能力的同时，在化学结构解释、蛋白质序列理解、化合物合成路径规划等科学领域任务中表现卓越，成为真实科学应用场景下的强大研究助手，并支持动态分词器实现对分子式、蛋白质序列的原生理解。</p><p><strong>标签</strong>：#InternLM #科学推理 #多模态模型 #Intern-S1 #科研助手</p><hr><h2 id="step3-321b参数高效多模态推理模型" tabindex="-1"><a class="header-anchor" href="#step3-321b参数高效多模态推理模型"><span>Step3：321B参数高效多模态推理模型</span></a></h2><figure><img src="https://huggingface.co/stepfun-ai/step3/resolve/main/figures/step3_bmk.jpeg" alt="Step3 Benchmark 图" tabindex="0" loading="lazy"><figcaption>Step3 Benchmark 图</figcaption></figure><p><strong>概要</strong>：<strong>StepFun</strong>发布Step3多模态推理模型，采用321B总参数、38B激活参数的MoE架构设计，通过多矩阵分解注意力(MFA)和注意力-FFN分解(AFD)技术实现端到端的解码成本最小化。该模型在视觉-语言推理任务中达到顶级性能，同时在旗舰级和低端加速器上都保持出色的推理效率，为成本效益型多模态智能应用提供了理想的解决方案。</p><p><strong>标签</strong>：#StepFun #成本效益 #推理优化 #Step3 #多模态智能</p><hr><h2 id="dots-ocr-1-7b参数多语言文档解析统一模型" tabindex="-1"><a class="header-anchor" href="#dots-ocr-1-7b参数多语言文档解析统一模型"><span>dots.ocr：1.7B参数多语言文档解析统一模型</span></a></h2><figure><img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/chart.png" alt="dots.ocr Evaluation 图" tabindex="0" loading="lazy"><figcaption>dots.ocr Evaluation 图</figcaption></figure><p><strong>概要</strong>：<strong>小红书</strong>发布dots.ocr多语言文档解析模型，这是一个基于1.7B参数LLM的统一视觉-语言模型，在单一架构内同时实现布局检测和内容识别，并保持良好的阅读顺序。该模型在OmniDocBench上达到SOTA性能，支持100多种低资源语言的文档解析，通过简单的提示词切换即可在不同任务间转换，证明了VLM在传统检测任务中的竞争优势，为文档智能化处理提供了更简洁高效的解决方案。</p><p><strong>标签</strong>：#小红书 #文档解析 #多语言支持 #dots.ocr #OCR识别</p><hr><h2 id="skywork-unipic-1-5b-统一视觉理解与生成模型" tabindex="-1"><a class="header-anchor" href="#skywork-unipic-1-5b-统一视觉理解与生成模型"><span>Skywork-UniPic-1.5B：统一视觉理解与生成模型</span></a></h2><figure><img src="https://huggingface.co/Skywork/Skywork-UniPic-1.5B/resolve/main/teaser-1.png" alt="Skywork-UniPic-1.5B Teaser 图" tabindex="0" loading="lazy"><figcaption>Skywork-UniPic-1.5B Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>昆仑万维</strong>发布Skywork-UniPic-1.5B统一多模态模型，这是一个15亿参数的自回归模型，在单一架构内支持图像理解、文本到图像生成和图像编辑三大核心视觉-语言任务。该模型从零开始在大规模多模态语料上训练，在GenEval、DPG-Bench等多项基准测试中取得竞争性结果，为统一图像-文本任务提供了高效的解决方案。</p><p><strong>标签</strong>：#昆仑万维 #统一架构 #自回归模型 #Skywork系列 #图像生成编辑</p><hr><h2 id="gpt-image-edit-1-5m-百万级gpt生成图像编辑数据集" tabindex="-1"><a class="header-anchor" href="#gpt-image-edit-1-5m-百万级gpt生成图像编辑数据集"><span>GPT-Image-Edit-1.5M：百万级GPT生成图像编辑数据集</span></a></h2><figure><img src="https://ucsc-vlaa.github.io/GPT-Image-Edit/static/images/GPT-Edit_teaser_image-cropped.png" alt="GPT-Image-Edit-1.5M Teaser 图" tabindex="0" loading="lazy"><figcaption>GPT-Image-Edit-1.5M Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>UCSC-VLAA</strong>团队发布GPT-Image-Edit-1.5M数据集，这是一个包含155万样本的大规模图像编辑数据集，基于HQ-Edit、UltraEdit、OmniEdit和Complex-Edit构建，所有输出图像均由GPT-Image-1重新生成。数据集涵盖原始、重写和复杂三种指令复杂度级别，统一了多种图像编辑任务，为训练高质量图像编辑模型提供了宝贵的数据资源。</p><p><strong>标签</strong>：#UCSC-VLAA #图像编辑数据集 #GPT生成 #数据集构建 #训练资源</p><hr><h2 id="flux-1-krea-dev-高质量快速图像生成在线体验" tabindex="-1"><a class="header-anchor" href="#flux-1-krea-dev-高质量快速图像生成在线体验"><span>FLUX.1-Krea-dev：高质量快速图像生成在线体验</span></a></h2><figure><img src="https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev/resolve/main/teaser.png" alt="FLUX.1-Krea-dev Teaser 图" tabindex="0" loading="lazy"><figcaption>FLUX.1-Krea-dev Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>Black Forest Labs</strong>推出FLUX.1-Krea-dev在线演示空间，这是基于FLUX.1模型的增强版图像生成服务，专为快速高质量图像创作而优化。该平台结合了FLUX.1的强大生成能力与Krea的用户友好界面，为创作者提供直观便捷的AI图像生成体验，支持多种风格和主题的创意图像制作，展现了商业级图像生成模型的实用价值和应用潜力。</p><p><strong>标签</strong>：#Black Forest Labs #在线服务 #图像生成平台 #FLUX模型 #创意工具</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B" target="_blank" rel="noopener noreferrer">Wan2.2-TI2V-5B模型</a></li><li><a href="https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B" target="_blank" rel="noopener noreferrer">Wan2.2-T2V-A14B模型</a></li><li><a href="https://huggingface.co/zai-org/GLM-4.5" target="_blank" rel="noopener noreferrer">GLM-4.5模型</a></li><li><a href="https://huggingface.co/internlm/Intern-S1" target="_blank" rel="noopener noreferrer">Intern-S1模型</a></li><li><a href="https://huggingface.co/stepfun-ai/step3" target="_blank" rel="noopener noreferrer">Step3模型</a></li><li><a href="https://huggingface.co/rednote-hilab/dots.ocr" target="_blank" rel="noopener noreferrer">dots.ocr模型</a></li><li><a href="https://huggingface.co/Skywork/Skywork-UniPic-1.5B" target="_blank" rel="noopener noreferrer">Skywork-UniPic-1.5B模型</a></li><li><a href="https://huggingface.co/datasets/UCSC-VLAA/GPT-Image-Edit-1.5M" target="_blank" rel="noopener noreferrer">GPT-Image-Edit-1.5M数据集</a></li><li><a href="https://huggingface.co/spaces/black-forest-labs/FLUX.1-Krea-dev" target="_blank" rel="noopener noreferrer">FLUX.1-Krea-dev演示空间</a></li></ol>',50)]))}]]),o=JSON.parse('{"path":"/zh/posts/hf-weekly/006.html","title":"Wan2.2突破720P三模态视频生成 | Z.ai开源355B智能体GLM-4.5 | Intern-S1科学推理235B MoE【HF周报】","lang":"zh-CN","frontmatter":{"description":"Wan2.2突破720P三模态视频生成 | Z.ai开源355B智能体GLM-4.5 | Intern-S1科学推理235B MoE【HF周报】 封面图封面图 摘要 本周亮点：Wan2.2发布5B和14B模型支持720P视频生成，Z.ai推出355B MoE架构的GLM-4.5智能体基础模型，Intern-S1发布235B科学推理多模态模型专攻科研领域...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/hf-weekly/006.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"Wan2.2突破720P三模态视频生成 | Z.ai开源355B智能体GLM-4.5 | Intern-S1科学推理235B MoE【HF周报】"}],["meta",{"property":"og:description","content":"Wan2.2突破720P三模态视频生成 | Z.ai开源355B智能体GLM-4.5 | Intern-S1科学推理235B MoE【HF周报】 封面图封面图 摘要 本周亮点：Wan2.2发布5B和14B模型支持720P视频生成，Z.ai推出355B MoE架构的GLM-4.5智能体基础模型，Intern-S1发布235B科学推理多模态模型专攻科研领域..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://neverbiasu.github.io/assets/images/placeholder.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Wan2.2突破720P三模态视频生成 | Z.ai开源355B智能体GLM-4.5 | Intern-S1科学推理235B MoE【HF周报】\\",\\"image\\":[\\"https://neverbiasu.github.io/assets/images/placeholder.png\\",\\"https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B/resolve/main/assets/performance.png\\",\\"https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/bench.png\\",\\"https://cdn-uploads.huggingface.co/production/uploads/642695e5274e7ad464c8a5ba/E43cgEXBRWjVJlU_-hdh6.png\\",\\"https://huggingface.co/stepfun-ai/step3/resolve/main/figures/step3_bmk.jpeg\\",\\"https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/chart.png\\",\\"https://huggingface.co/Skywork/Skywork-UniPic-1.5B/resolve/main/teaser-1.png\\",\\"https://ucsc-vlaa.github.io/GPT-Image-Edit/static/images/GPT-Edit_teaser_image-cropped.png\\",\\"https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev/resolve/main/teaser.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"Wan2.2：首个支持文本-图像-视频三模态生成的720P模型","slug":"wan2-2-首个支持文本-图像-视频三模态生成的720p模型","link":"#wan2-2-首个支持文本-图像-视频三模态生成的720p模型","children":[]},{"level":2,"title":"GLM-4.5：355B参数MoE架构的开源智能体基础模型","slug":"glm-4-5-355b参数moe架构的开源智能体基础模型","link":"#glm-4-5-355b参数moe架构的开源智能体基础模型","children":[]},{"level":2,"title":"Intern-S1：235B MoE科学推理多模态模型","slug":"intern-s1-235b-moe科学推理多模态模型","link":"#intern-s1-235b-moe科学推理多模态模型","children":[]},{"level":2,"title":"Step3：321B参数高效多模态推理模型","slug":"step3-321b参数高效多模态推理模型","link":"#step3-321b参数高效多模态推理模型","children":[]},{"level":2,"title":"dots.ocr：1.7B参数多语言文档解析统一模型","slug":"dots-ocr-1-7b参数多语言文档解析统一模型","link":"#dots-ocr-1-7b参数多语言文档解析统一模型","children":[]},{"level":2,"title":"Skywork-UniPic-1.5B：统一视觉理解与生成模型","slug":"skywork-unipic-1-5b-统一视觉理解与生成模型","link":"#skywork-unipic-1-5b-统一视觉理解与生成模型","children":[]},{"level":2,"title":"GPT-Image-Edit-1.5M：百万级GPT生成图像编辑数据集","slug":"gpt-image-edit-1-5m-百万级gpt生成图像编辑数据集","link":"#gpt-image-edit-1-5m-百万级gpt生成图像编辑数据集","children":[]},{"level":2,"title":"FLUX.1-Krea-dev：高质量快速图像生成在线体验","slug":"flux-1-krea-dev-高质量快速图像生成在线体验","link":"#flux-1-krea-dev-高质量快速图像生成在线体验","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":6.1,"words":1829},"filePathRelative":"zh/posts/hf-weekly/006.md","excerpt":"\\n<figure><img src=\\"/assets/images/placeholder.png\\" alt=\\"封面图\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面图</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：Wan2.2发布5B和14B模型支持720P视频生成，Z.ai推出355B MoE架构的GLM-4.5智能体基础模型，Intern-S1发布235B科学推理多模态模型专攻科研领域。详细内容见下文，相关参考链接请见文末。</p>\\n<hr>\\n<h2>目录</h2>\\n<ol>\\n<li><a href=\\"#wan22\\">Wan2.2：首个支持文本-图像-视频三模态生成的720P模型</a></li>\\n<li><a href=\\"#glm-45\\">GLM-4.5：355B参数MoE架构的开源智能体基础模型</a></li>\\n<li><a href=\\"#intern-s1\\">Intern-S1：235B MoE科学推理多模态模型</a></li>\\n<li><a href=\\"#step3\\">Step3：321B参数高效多模态推理模型</a></li>\\n<li><a href=\\"#dotsocr\\">dots.ocr：1.7B参数多语言文档解析统一模型</a></li>\\n<li><a href=\\"#skywork-unipic-15b\\">Skywork-UniPic-1.5B：统一视觉理解与生成模型</a></li>\\n<li><a href=\\"#gpt-image-edit-15m\\">GPT-Image-Edit-1.5M：百万级GPT生成图像编辑数据集</a></li>\\n<li><a href=\\"#flux1-krea-dev\\">FLUX.1-Krea-dev：高质量快速图像生成在线体验</a></li>\\n</ol>","autoDesc":true}')}}]);