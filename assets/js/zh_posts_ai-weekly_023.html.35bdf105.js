"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[7959],{6262:(n,e)=>{e.A=(n,e)=>{const a=n.__vccOpts||n;for(const[n,i]of e)a[n]=i;return a}},9527:(n,e,a)=>{a.r(e),a.d(e,{comp:()=>t,data:()=>o});var i=a(641);const r={},t=(0,a(6262).A)(r,[["render",function(n,e){return(0,i.uX)(),(0,i.CE)("div",null,e[0]||(e[0]=[(0,i.Fv)('<h1 id="catv2ton利用扩散transformer实现虚拟试穿-janus-pro多模态理解与生成-qwen2-5-1m扩展输入上限【ai周报】" tabindex="-1"><a class="header-anchor" href="#catv2ton利用扩散transformer实现虚拟试穿-janus-pro多模态理解与生成-qwen2-5-1m扩展输入上限【ai周报】"><span>CatV2TON利用扩散Transformer实现虚拟试穿|Janus-Pro多模态理解与生成|Qwen2.5-1M扩展输入上限【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/1b61e25e-d444-4ca5-9a95-4db03f2aafc8/anim=false,width=450/54073781.jpeg" alt="封面源自C站作者roxin282" tabindex="0" loading="lazy"><figcaption>封面源自C站作者roxin282</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：CatV2TON利用DiT统一视频虚拟试穿；Janus-Pro增强多模态理解生成；Baichuan-Omni-1.5开源全模态模型；Qwen2.5-1M突破128K长文本生成；AtlaAI提出小型语言模型评估器。详情见正文。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#catv2ton%E5%88%A9%E7%94%A8%E6%89%A9%E6%95%A3transformer%E5%AE%9E%E7%8E%B0%E8%A7%86%E9%A2%91%E8%99%9A%E6%8B%9F%E8%AF%95%E7%A9%BF">CatV2TON：利用扩散Transformer实现视频虚拟试穿</a></li><li><a href="#janus-pro%E7%BB%9F%E4%B8%80%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">Janus-Pro：统一的多模态理解与生成模型</a></li><li><a href="#baichuan-omni-15%E7%99%BE%E5%B7%9D%E6%9C%80%E6%96%B0%E5%BC%80%E6%BA%90%E5%A4%9A%E8%AF%AD%E8%A8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B">Baichuan-Omni-1.5：百川最新开源多语言多模态模型</a></li><li><a href="#qwen25-1m%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E6%8E%A8%E5%87%BA%E7%9A%84%E6%9C%80%E6%96%B0%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">Qwen2.5-1M：阿里巴巴推出的最新长上下文大语言模型</a></li><li><a href="#atla-selene-mini%E9%80%9A%E7%94%A8%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B">Atla Selene Mini：通用评估模型</a></li><li><a href="#openai%E5%8F%91%E5%B8%83o3-mini%E6%A8%A1%E5%9E%8B%E6%8F%90%E5%8D%87%E6%8E%A8%E7%90%86%E6%80%A7%E8%83%BD">OpenAI发布o3-mini模型：提升推理性能</a></li><li><a href="#animagine-xl-40%E7%BB%88%E6%9E%81%E5%8A%A8%E6%BC%AB%E4%B8%BB%E9%A2%98%E5%BE%AE%E8%B0%83-sdxl-%E6%A8%A1%E5%9E%8B">Animagine XL 4.0：终极动漫主题微调 SDXL 模型</a></li></ol><hr><h2 id="catv2ton-利用扩散transformer实现视频虚拟试穿" tabindex="-1"><a class="header-anchor" href="#catv2ton-利用扩散transformer实现视频虚拟试穿"><span>CatV2TON：利用扩散Transformer实现视频虚拟试穿</span></a></h2><figure><img src="https://arxiv.org/html/2501.11325v1/x2.png" alt="CatV2TON Overview 图" tabindex="0" loading="lazy"><figcaption>CatV2TON Overview 图</figcaption></figure><p><strong>概要</strong>：<strong>CatV2TON</strong> 是一项针对虚拟试穿（Virtual Try-On，VTON）的新研究，提出了一种统一的图像和视频试穿方法。该方法采用DiT（扩散Transformer）架构，通过时间串联技术，确保服装在图像和视频试穿中的一致性。实验结果表明，CatV2TON在图像和视频试穿任务中均优于现有方法，提供了一个多功能且可靠的解决方案。</p><p><strong>标签</strong>：#CatV2TON #虚拟试穿 #DiT #计算机视觉</p><hr><h2 id="janus-pro-统一的多模态理解与生成模型" tabindex="-1"><a class="header-anchor" href="#janus-pro-统一的多模态理解与生成模型"><span>Janus-Pro：统一的多模态理解与生成模型</span></a></h2><figure><img src="https://github.com/deepseek-ai/Janus/raw/main/images/teaser_januspro.png" alt="Janus-Pro Teaser 图" tabindex="0" loading="lazy"><figcaption>Janus-Pro Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>Janus-Pro</strong> 是 <strong>DeepSeek-AI</strong> 推出的 <strong>Janus</strong> 模型的增强版本，旨在统一多模态理解与生成。该模型通过优化训练策略、扩展训练数据以及扩大模型规模，在多模态理解和文本到图像的指令跟随能力上取得了显著提升，同时增强了图像生成的稳定性。</p><p><strong>标签</strong>：#JanusPro #多模态 #DeepSeek #图像生成</p><hr><h2 id="baichuan-omni-1-5-百川最新开源多语言多模态模型" tabindex="-1"><a class="header-anchor" href="#baichuan-omni-1-5-百川最新开源多语言多模态模型"><span>Baichuan-Omni-1.5：百川最新开源多语言多模态模型</span></a></h2><figure><img src="https://github.com/baichuan-inc/Baichuan-Omni-1.5/raw/main/assets/architecture.png" alt="Baichuan-Omni-1.5 Architecture  图" tabindex="0" loading="lazy"><figcaption>Baichuan-Omni-1.5 Architecture 图</figcaption></figure><p><strong>概要</strong>：<strong>Baichuan-Omni-1.5</strong> 是由 <strong>百川智能科技</strong> 推出的开源 7B 全模态大模型，支持处理文本、图像、视频和音频等多种模态输入，并能够生成高质量的文本和语音输出。 该模型基于 Qwen2.5-7B 语言模型构建，采用了高质量的多模态预训练数据和多阶段训练策略，以实现各模态间的有效协同。 在多模态医疗基准测试中，Baichuan-Omni-1.5 的表现与领先模型相当，展示了其在多模态理解和生成任务中的强大能力。</p><p><strong>标签</strong>：#Baichuan-Omni-1.5 #多模态 #大语言模型 #百川智能</p><hr><h2 id="qwen2-5-1m-阿里巴巴推出的最新长上下文大语言模型" tabindex="-1"><a class="header-anchor" href="#qwen2-5-1m-阿里巴巴推出的最新长上下文大语言模型"><span>Qwen2.5-1M：阿里巴巴推出的最新长上下文大语言模型</span></a></h2><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/dca_ablation.png#center" alt="Qwen2.5 Ablation 图" tabindex="0" loading="lazy"><figcaption>Qwen2.5 Ablation 图</figcaption></figure><p><strong>概要</strong>：<strong>Qwen2.5</strong> 是 <strong>阿里巴巴集团</strong> 旗下 <strong>Qwen 团队</strong> 最新发布的大语言模型系列。该模型在最新的大规模数据集上进行了预训练，包含多达 18 万亿个 tokens。相较于之前的版本，Qwen2.5 在知识掌握、编程能力和数学能力方面有了显著提升。此外，模型在指令执行、长文本生成（支持最长 128K tokens）、结构化数据理解（如表格）以及生成结构化输出（特别是 JSON）方面表现出色。Qwen2.5 提供从 5 亿到 720 亿参数的多种模型规模，适用于不同的应用场景。</p><p><strong>标签</strong>：#Qwen2.5 #阿里巴巴 #LLM #输入扩展</p><hr><h2 id="atla-selene-mini-通用评估模型" tabindex="-1"><a class="header-anchor" href="#atla-selene-mini-通用评估模型"><span>Atla Selene Mini：通用评估模型</span></a></h2><figure><img src="https://arxiv.org/html/2501.17195v1/extracted/6157738/figures/Fig1.png" alt="Atla Selene Mini Benchmark 图" tabindex="0" loading="lazy"><figcaption>Atla Selene Mini Benchmark 图</figcaption></figure><p><strong>概要</strong>：<strong>Atla Selene Mini</strong> 是由 <strong>Atla AI</strong> 推出的最先进的小型语言模型评估器（SLMJ）。该模型在11个分布外基准测试中表现出色，超越了其他SLMJ和GPT-4o-mini，涵盖了绝对评分、分类和成对偏好任务。在RewardBench上，Selene Mini成为得分最高的8B生成模型，超过了GPT-4o等强大基线模型。通过引入合成批评数据并进行严格的数据过滤，Selene Mini在金融和医疗等行业数据集上与人类专家评估的零样本一致性显著提高。此外，该模型对提示格式的变化具有鲁棒性，并在社区驱动的评估竞技场中排名第一。模型权重已在HuggingFace和Ollama上发布，鼓励广泛的社区采用。</p><p><strong>标签</strong>：#AtlaSeleneMini #SLMJ #模型评估 #GPT-4o #AtlaAI</p><hr><h2 id="openai发布o3-mini模型-提升推理性能" tabindex="-1"><a class="header-anchor" href="#openai发布o3-mini模型-提升推理性能"><span>OpenAI发布o3-mini模型：提升推理性能</span></a></h2><figure><img src="https://images.ctfassets.net/kftzwdyauwt9/3ww3UedB0YdteOeII6yGKI/ae9c0e372f7a294ecc93b638fc958b07/SWE.png?w=2048&amp;q=80&amp;fm=webp" alt="OpenAI o3-mini SWEbench 图" tabindex="0" loading="lazy"><figcaption>OpenAI o3-mini SWEbench 图</figcaption></figure><p><strong>概要</strong>：OpenAI推出了最新的推理模型<strong>o3-mini</strong>，该模型在科学、数学和编程等STEM领域表现出色。o3-mini在保持低成本和低延迟的同时，提供了更快且更准确的响应。它已集成到ChatGPT及其API服务中，免费用户也可有限制地访问。此外，OpenAI还提供了性能更高的o3-mini-high版本，专为付费用户设计，特别适用于编码任务。</p><p><strong>标签</strong>：#OpenAI #o3-mini #推理模型 #STEM #ChatGPT</p><hr><h2 id="animagine-xl-4-0-终极动漫主题微调-sdxl-模型" tabindex="-1"><a class="header-anchor" href="#animagine-xl-4-0-终极动漫主题微调-sdxl-模型"><span>Animagine XL 4.0：终极动漫主题微调 SDXL 模型</span></a></h2><figure><img src="https://cdn-uploads.huggingface.co/production/uploads/6365c8dbf31ef76df4042821/_tsxjwf3VPu94xh9wJSbo.png" alt="Animagine XL 4.0 Teaser 图" tabindex="0" loading="lazy"><figcaption>Animagine XL 4.0 Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>Animagine XL 4.0</strong> 是由 <strong>Cagliostro Research Lab</strong> 研发的动漫主题专用生成模型。作为 Animagine 系列最新迭代，该模型基于 Stable Diffusion XL 1.0 架构，通过 840 万张精选动漫图像数据集进行深度微调（耗时 2650 GPU 小时）。其创新性采用标签排序训练法，实现了对角色特征与艺术风格的精准控制，支持中日英多语言 prompt 输入，可生成从 1024x1024 到 2048x2048 的高清动漫图像。与同类模型相比，Animagine XL 4.0 在角色细节还原度（如服饰/五官刻画）和多角色协同生成方面表现突出，已集成至 Hugging Face Spaces、ComfyUI 和 Diffusers 等主流生成框架。</p><p><strong>标签</strong>：#动漫生成 #SDXL优化 #多语言模型 #高分辨率生成 #Diffusers</p><hr><h3 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span><strong>参考文献</strong></span></a></h3><ol><li><a href="https://github.com/Zheng-Chong/CatV2TON" target="_blank" rel="noopener noreferrer">CatV2TON GitHub</a></li><li><a href="https://arxiv.org/html/2501.11325" target="_blank" rel="noopener noreferrer">CatV2TON 论文</a></li><li><a href="https://github.com/deepseek-ai/Janus" target="_blank" rel="noopener noreferrer">Janus GitHub</a></li><li><a href="https://arxiv.org/html/2501.17811v1" target="_blank" rel="noopener noreferrer">Janus 论文</a></li><li><a href="https://github.com/baichuan-inc/Baichuan-Omni-1.5" target="_blank" rel="noopener noreferrer">Baichuan-Omni-1.5 GitHub</a></li><li><a href="https://arxiv.org/html/2501.15368v1" target="_blank" rel="noopener noreferrer">Baichuan-Omni-1.5 论文</a></li><li><a href="https://arxiv.org/pdf/2501.15383" target="_blank" rel="noopener noreferrer">Qwen2.5-1M-Demo 论文</a></li><li><a href="https://huggingface.co/spaces/Qwen/Qwen2.5-1M-Demo" target="_blank" rel="noopener noreferrer">Qwen2.5-1M-Demo</a></li><li><a href="https://www.atla-ai.com/" target="_blank" rel="noopener noreferrer">ATLA-AI</a></li><li><a href="https://arxiv.org/html/2501.17195" target="_blank" rel="noopener noreferrer">ATLA-AI 论文</a></li><li><a href="https://openai.com/index/openai-o3-mini/" target="_blank" rel="noopener noreferrer">OpenAI O3 Mini</a></li><li><a href="https://huggingface.co/cagliostrolab/animagine-xl-4.0" target="_blank" rel="noopener noreferrer">Animagine-XL-4.0 GitHub</a></li><li><a href="https://huggingface.co/spaces/cagliostrolab/animagine-xl-4.0" target="_blank" rel="noopener noreferrer">Animagine-XL-4.0</a></li></ol>',45)]))}]]),o=JSON.parse('{"path":"/zh/posts/ai-weekly/023.html","title":"CatV2TON利用扩散Transformer实现虚拟试穿|Janus-Pro多模态理解与生成|Qwen2.5-1M扩展输入上限【AI周报】","lang":"zh-CN","frontmatter":{"description":"CatV2TON利用扩散Transformer实现虚拟试穿|Janus-Pro多模态理解与生成|Qwen2.5-1M扩展输入上限【AI周报】 封面源自C站作者roxin282封面源自C站作者roxin282 摘要 本周亮点：CatV2TON利用DiT统一视频虚拟试穿；Janus-Pro增强多模态理解生成；Baichuan-Omni-1.5开源全模态模型...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/023.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"CatV2TON利用扩散Transformer实现虚拟试穿|Janus-Pro多模态理解与生成|Qwen2.5-1M扩展输入上限【AI周报】"}],["meta",{"property":"og:description","content":"CatV2TON利用扩散Transformer实现虚拟试穿|Janus-Pro多模态理解与生成|Qwen2.5-1M扩展输入上限【AI周报】 封面源自C站作者roxin282封面源自C站作者roxin282 摘要 本周亮点：CatV2TON利用DiT统一视频虚拟试穿；Janus-Pro增强多模态理解生成；Baichuan-Omni-1.5开源全模态模型..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/1b61e25e-d444-4ca5-9a95-4db03f2aafc8/anim=false,width=450/54073781.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"CatV2TON利用扩散Transformer实现虚拟试穿|Janus-Pro多模态理解与生成|Qwen2.5-1M扩展输入上限【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/1b61e25e-d444-4ca5-9a95-4db03f2aafc8/anim=false,width=450/54073781.jpeg\\",\\"https://arxiv.org/html/2501.11325v1/x2.png\\",\\"https://github.com/deepseek-ai/Janus/raw/main/images/teaser_januspro.png\\",\\"https://github.com/baichuan-inc/Baichuan-Omni-1.5/raw/main/assets/architecture.png\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/dca_ablation.png#center\\",\\"https://arxiv.org/html/2501.17195v1/extracted/6157738/figures/Fig1.png\\",\\"https://images.ctfassets.net/kftzwdyauwt9/3ww3UedB0YdteOeII6yGKI/ae9c0e372f7a294ecc93b638fc958b07/SWE.png?w=2048&q=80&fm=webp\\",\\"https://cdn-uploads.huggingface.co/production/uploads/6365c8dbf31ef76df4042821/_tsxjwf3VPu94xh9wJSbo.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"CatV2TON：利用扩散Transformer实现视频虚拟试穿","slug":"catv2ton-利用扩散transformer实现视频虚拟试穿","link":"#catv2ton-利用扩散transformer实现视频虚拟试穿","children":[]},{"level":2,"title":"Janus-Pro：统一的多模态理解与生成模型","slug":"janus-pro-统一的多模态理解与生成模型","link":"#janus-pro-统一的多模态理解与生成模型","children":[]},{"level":2,"title":"Baichuan-Omni-1.5：百川最新开源多语言多模态模型","slug":"baichuan-omni-1-5-百川最新开源多语言多模态模型","link":"#baichuan-omni-1-5-百川最新开源多语言多模态模型","children":[]},{"level":2,"title":"Qwen2.5-1M：阿里巴巴推出的最新长上下文大语言模型","slug":"qwen2-5-1m-阿里巴巴推出的最新长上下文大语言模型","link":"#qwen2-5-1m-阿里巴巴推出的最新长上下文大语言模型","children":[]},{"level":2,"title":"Atla Selene Mini：通用评估模型","slug":"atla-selene-mini-通用评估模型","link":"#atla-selene-mini-通用评估模型","children":[]},{"level":2,"title":"OpenAI发布o3-mini模型：提升推理性能","slug":"openai发布o3-mini模型-提升推理性能","link":"#openai发布o3-mini模型-提升推理性能","children":[]},{"level":2,"title":"Animagine XL 4.0：终极动漫主题微调 SDXL 模型","slug":"animagine-xl-4-0-终极动漫主题微调-sdxl-模型","link":"#animagine-xl-4-0-终极动漫主题微调-sdxl-模型","children":[{"level":3,"title":"参考文献","slug":"参考文献","link":"#参考文献","children":[]}]}],"readingTime":{"minutes":5.75,"words":1724},"filePathRelative":"zh/posts/ai-weekly/023.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/1b61e25e-d444-4ca5-9a95-4db03f2aafc8/anim=false,width=450/54073781.jpeg\\" alt=\\"封面源自C站作者roxin282\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面源自C站作者roxin282</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：CatV2TON利用DiT统一视频虚拟试穿；Janus-Pro增强多模态理解生成；Baichuan-Omni-1.5开源全模态模型；Qwen2.5-1M突破128K长文本生成；AtlaAI提出小型语言模型评估器。详情见正文。</p>","autoDesc":true}')}}]);