"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[8784],{6262:(e,t)=>{t.A=(e,t)=>{const r=e.__vccOpts||e;for(const[e,o]of t)r[e]=o;return r}},4274:(e,t,r)=>{r.r(t),r.d(t,{comp:()=>n,data:()=>i});var o=r(641);const a={},n=(0,r(6262).A)(a,[["render",function(e,t){return(0,o.uX)(),(0,o.CE)("div",null,t[0]||(t[0]=[(0,o.Fv)('<h1 id="阿里开源animate-x革新ai角色动画-zerocomp实现创新3d对象合成-f5-tts提升tts语音的自然度【ai周报】" tabindex="-1"><a class="header-anchor" href="#阿里开源animate-x革新ai角色动画-zerocomp实现创新3d对象合成-f5-tts提升tts语音的自然度【ai周报】"><span><strong>阿里开源Animate-X革新AI角色动画|ZeroComp实现创新3D对象合成|F5-TTS提升TTS语音的自然度【AI周报】</strong></span></a></h1><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img12@main/2024/10/20/1729433178592-bd22ef03-8cac-448b-9ab1-3f2dcddac6c3.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span><strong>摘要</strong></span></a></h2><p>本周AI周报关注几项前沿生成模型：ZeroComp在3D合成领域开辟新路径，CtrLoRA实现可控图像生成的高效框架，F5-TTS通过流匹配技术提升语音生成效果，HyperDreamBooth加快个性化文本到图像的速度。其余成果详见正文。</p><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span><strong>目录</strong></span></a></h2><ul><li><a href="#%E9%98%BF%E9%87%8C%E5%BC%80%E6%BA%90animate-x%E9%9D%A9%E6%96%B0ai%E8%A7%92%E8%89%B2%E5%8A%A8%E7%94%BBzerocomp%E5%AE%9E%E7%8E%B0%E5%88%9B%E6%96%B03d%E5%AF%B9%E8%B1%A1%E5%90%88%E6%88%90f5-tts%E6%8F%90%E5%8D%87tts%E8%AF%AD%E9%9F%B3%E7%9A%84%E8%87%AA%E7%84%B6%E5%BA%A6ai%E5%91%A8%E6%8A%A5"><strong>阿里开源Animate-X革新AI角色动画|ZeroComp实现创新3D对象合成|F5-TTS提升TTS语音的自然度【AI周报】</strong></a><ul><li><a href="#%E6%91%98%E8%A6%81"><strong>摘要</strong></a></li><li><a href="#%E7%9B%AE%E5%BD%95"><strong>目录</strong></a></li><li><a href="#zerocomp-%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%AF%B9%E8%B1%A1%E5%90%88%E6%88%90"><strong>ZeroComp: 零样本对象合成</strong></a></li><li><a href="#ctrlora-controlnet%E5%92%8Clora%E9%AB%98%E6%95%88%E7%BB%93%E5%90%88"><strong>CtrLoRA: ControlNet和LoRA高效结合</strong></a></li><li><a href="#animate-x-%E9%80%9A%E7%94%A8%E8%A7%92%E8%89%B2%E5%9B%BE%E5%83%8F%E5%8A%A8%E7%94%BB"><strong>Animate-X: 通用角色图像动画</strong></a></li><li><a href="#f5-tts-%E5%9F%BA%E4%BA%8E%E6%B5%81%E5%8C%B9%E9%85%8D%E7%9A%84%E9%AB%98%E6%95%88%E6%96%87%E6%9C%AC%E5%88%B0%E8%AF%AD%E9%9F%B3%E7%B3%BB%E7%BB%9F"><strong>F5-TTS: 基于流匹配的高效文本到语音系统</strong></a></li><li><a href="#hyperdreambooth-%E5%BF%AB%E9%80%9F%E4%B8%AA%E6%80%A7%E5%8C%96%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%B6%85%E7%BD%91%E7%BB%9C"><strong>HyperDreamBooth: 快速个性化文本到图像模型的超网络</strong></a></li><li><a href="#janus-%E7%BB%9F%E4%B8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E6%A1%86%E6%9E%B6"><strong>Janus: 统一多模态理解与生成框架</strong></a><ul><li><a href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><strong>参考链接</strong></a></li></ul></li></ul></li></ul><hr><h2 id="zerocomp-零样本对象合成" tabindex="-1"><a class="header-anchor" href="#zerocomp-零样本对象合成"><span><strong>ZeroComp: 零样本对象合成</strong></span></a></h2><figure><img src="https://lvsn.github.io/ZeroComp/assets/Pipeline1_or4.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>ZeroComp Pipeline 图</p><p><strong>概要</strong>：<strong>ZeroComp</strong>[1][2] 是<strong>丰田</strong>开发的一种零样本3D对象合成方法，利用图像内在特性实现无需配对图像的合成。它结合了ControlNet和Stable Diffusion模型，能够无缝地将虚拟3D对象集成到场景中，并在各类场景中表现出色，尤其是在室外合成方面。</p><p><strong>标签</strong>：#3D合成 #ControlNet #Diffusion 模型 #零样本学习</p><hr><h2 id="ctrlora-controlnet和lora高效结合" tabindex="-1"><a class="header-anchor" href="#ctrlora-controlnet和lora高效结合"><span><strong>CtrLoRA: ControlNet和LoRA高效结合</strong></span></a></h2><figure><img src="https://github.com/xyfJASON/ctrlora/raw/main/assets/banner.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>CtrlLoRA Banner 图</p><p><strong>概要</strong>: <strong>CtrLoRA</strong>[3][4] 是<strong>中科院</strong>提出的一个可扩展的高效框架，用于可控图像生成。它通过一个基础ControlNet模型学习图像生成的通用知识，结合特定条件的LoRA，使用户可以快速适应新条件，减少90%的可学习参数。这一方法显著降低了训练成本，使得新手用户也能在短时间内实现良好结果。</p><p><strong>标签</strong>: #ControlNet #LoRA #图像生成 #Diffusion 模型</p><hr><h2 id="animate-x-通用角色图像动画" tabindex="-1"><a class="header-anchor" href="#animate-x-通用角色图像动画"><span><strong>Animate-X: 通用角色图像动画</strong></span></a></h2><figure><img src="https://img.alicdn.com/imgextra/i3/O1CN01QZs1bU1LoW68dhIb2_!!6000000001346-0-tps-1783-856.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Animate-X Results 图</p><p><strong>概要</strong>: <strong>Animate-X</strong>[5][6] 是由阿里研究院提出的一个通用角色动画框架。该系统基于 LDM 模型，通过引入隐式和显式姿势指示器，增强对运动模式的表示，实现高质量动画生成，支持人类和拟人角色。其新提出的 <strong>A²Bench</strong> 基准测试用于评估动画效果，实验表明其在性能上超越现有方法。</p><p><strong>标签</strong>: #角色动画 #阿里 #运动表示 #LDM</p><hr><h2 id="f5-tts-基于流匹配的高效文本到语音系统" tabindex="-1"><a class="header-anchor" href="#f5-tts-基于流匹配的高效文本到语音系统"><span><strong>F5-TTS: 基于流匹配的高效文本到语音系统</strong></span></a></h2><figure><img src="https://swivid.github.io/F5-TTS/pics/f5tts_overview.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>F5-TTS Overview 图</p><p><strong>概要</strong>: <strong>F5-TTS</strong>[7][8][9] 是<strong>上交</strong>、<strong>剑桥</strong>和<strong>吉利公司</strong>一同研发的一个完全非自回归的文本到语音系统，基于流匹配和Diffusion Transformer (DiT) 模型。该系统通过填充标记和去噪生成语音，无需复杂的持续时间模型和文本编码器。F5-TTS展现出高自然度和表达力，支持无缝语言切换，训练在100K小时的多语言数据集上完成，实时生成效率达到0.15，极大提高了性能和效率。</p><p><strong>标签</strong>: #文本到语音 #流匹配 #Diffusion Transformer #多语言</p><hr><h2 id="hyperdreambooth-快速个性化文本到图像模型的超网络" tabindex="-1"><a class="header-anchor" href="#hyperdreambooth-快速个性化文本到图像模型的超网络"><span><strong>HyperDreamBooth: 快速个性化文本到图像模型的超网络</strong></span></a></h2><figure><img src="https://hyperdreambooth.github.io/files/teaser_v2.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>HyperDreamBooth Teaser 图</p><p><strong>概要</strong>: <strong>HyperDreamBooth</strong>[10][11] 由 <strong>Google Research</strong> 提出，利用单张图像个人化文本到图像Diffusion模型，速度比DreamBooth快25倍。该方法采用超网络生成个性化权重，结合快速微调，能在约20秒内完成个性化，且生成的模型仅需100KB，展现出高效性和保真度。</p><p><strong>标签</strong>: #超网络 #个性化生成 #Google #Diffusion 模型</p><hr><h2 id="janus-统一多模态理解与生成框架" tabindex="-1"><a class="header-anchor" href="#janus-统一多模态理解与生成框架"><span><strong>Janus: 统一多模态理解与生成框架</strong></span></a></h2><figure><img src="https://github.com/deepseek-ai/Janus/raw/main/images/teaser.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Janus Teaser 图</p><p><strong>概要</strong>: <strong>Janus</strong>[12][13] 是<strong>deepseek</strong>提出的一个新型自回归框架，旨在统一多模态理解与生成。通过分离的视觉编码路径，该模型解决了传统方法的局限性，提升了灵活性与性能。实验显示，Janus在多项任务中超越了现有的统一模型和特定任务模型，成为下一代多模态模型的有力候选者。</p><p><strong>标签</strong>: #多模态 #视觉编码 #自回归模型 #deepseek</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://lvsn.github.io/ZeroComp" target="_blank" rel="noopener noreferrer">ZeroComp 项目主页</a></li><li><a href="https://arxiv.org/pdf/2410.08168" target="_blank" rel="noopener noreferrer">ZeroComp 论文</a></li><li><a href="https://github.com/xyfJASON/ctrlora" target="_blank" rel="noopener noreferrer">CtrLoRA GitHub 仓库</a></li><li><a href="https://arxiv.org/pdf/2410.09400" target="_blank" rel="noopener noreferrer">CtrLoRA 论文</a></li><li><a href="https://lucaria-academy.github.io/Animate-X" target="_blank" rel="noopener noreferrer">Animate-X 项目主页</a></li><li><a href="https://github.com/Lucaria-Academy/Animate-X" target="_blank" rel="noopener noreferrer">Animate-X Github 仓库</a></li><li><a href="https://swivid.github.io/F5-TTS" target="_blank" rel="noopener noreferrer">F5-TTS 项目主页</a></li><li><a href="https://github.com/SWivid/F5-TTS" target="_blank" rel="noopener noreferrer">F5-TTS Github 仓库</a></li><li><a href="https://arxiv.org/pdf/2410.06885" target="_blank" rel="noopener noreferrer">F5-TTS 论文</a></li><li><a href="https://hyperdreambooth.github.io/" target="_blank" rel="noopener noreferrer">HyperDreamBooth 项目主页</a></li><li><a href="https://github.com/JiauZhang/hyperdreambooth" target="_blank" rel="noopener noreferrer">HyperDreamBooth GitHub 仓库</a></li><li><a href="https://github.com/deepseek-ai/Janus" target="_blank" rel="noopener noreferrer">Janus GitHub 仓库</a></li><li><a href="https://arxiv.org/pdf/2410.13848" target="_blank" rel="noopener noreferrer">Janus 论文</a></li></ol>',45)]))}]]),i=JSON.parse('{"path":"/zh/posts/ai-weekly/008.html","title":"阿里开源Animate-X革新AI角色动画|ZeroComp实现创新3D对象合成|F5-TTS提升TTS语音的自然度【AI周报】","lang":"zh-CN","frontmatter":{"description":"阿里开源Animate-X革新AI角色动画|ZeroComp实现创新3D对象合成|F5-TTS提升TTS语音的自然度【AI周报】 摘要 本周AI周报关注几项前沿生成模型：ZeroComp在3D合成领域开辟新路径，CtrLoRA实现可控图像生成的高效框架，F5-TTS通过流匹配技术提升语音生成效果，HyperDreamBooth加快个性化文本到图像的速度...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/008.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"阿里开源Animate-X革新AI角色动画|ZeroComp实现创新3D对象合成|F5-TTS提升TTS语音的自然度【AI周报】"}],["meta",{"property":"og:description","content":"阿里开源Animate-X革新AI角色动画|ZeroComp实现创新3D对象合成|F5-TTS提升TTS语音的自然度【AI周报】 摘要 本周AI周报关注几项前沿生成模型：ZeroComp在3D合成领域开辟新路径，CtrLoRA实现可控图像生成的高效框架，F5-TTS通过流匹配技术提升语音生成效果，HyperDreamBooth加快个性化文本到图像的速度..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://fastly.jsdelivr.net/gh/bucketio/img12@main/2024/10/20/1729433178592-bd22ef03-8cac-448b-9ab1-3f2dcddac6c3.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"阿里开源Animate-X革新AI角色动画|ZeroComp实现创新3D对象合成|F5-TTS提升TTS语音的自然度【AI周报】\\",\\"image\\":[\\"https://fastly.jsdelivr.net/gh/bucketio/img12@main/2024/10/20/1729433178592-bd22ef03-8cac-448b-9ab1-3f2dcddac6c3.jpeg\\",\\"https://lvsn.github.io/ZeroComp/assets/Pipeline1_or4.png\\",\\"https://github.com/xyfJASON/ctrlora/raw/main/assets/banner.jpg\\",\\"https://img.alicdn.com/imgextra/i3/O1CN01QZs1bU1LoW68dhIb2_!!6000000001346-0-tps-1783-856.jpg\\",\\"https://swivid.github.io/F5-TTS/pics/f5tts_overview.png\\",\\"https://hyperdreambooth.github.io/files/teaser_v2.png\\",\\"https://github.com/deepseek-ai/Janus/raw/main/images/teaser.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"ZeroComp: 零样本对象合成","slug":"zerocomp-零样本对象合成","link":"#zerocomp-零样本对象合成","children":[]},{"level":2,"title":"CtrLoRA: ControlNet和LoRA高效结合","slug":"ctrlora-controlnet和lora高效结合","link":"#ctrlora-controlnet和lora高效结合","children":[]},{"level":2,"title":"Animate-X: 通用角色图像动画","slug":"animate-x-通用角色图像动画","link":"#animate-x-通用角色图像动画","children":[]},{"level":2,"title":"F5-TTS: 基于流匹配的高效文本到语音系统","slug":"f5-tts-基于流匹配的高效文本到语音系统","link":"#f5-tts-基于流匹配的高效文本到语音系统","children":[]},{"level":2,"title":"HyperDreamBooth: 快速个性化文本到图像模型的超网络","slug":"hyperdreambooth-快速个性化文本到图像模型的超网络","link":"#hyperdreambooth-快速个性化文本到图像模型的超网络","children":[]},{"level":2,"title":"Janus: 统一多模态理解与生成框架","slug":"janus-统一多模态理解与生成框架","link":"#janus-统一多模态理解与生成框架","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":4.36,"words":1309},"filePathRelative":"zh/posts/ai-weekly/008.md","excerpt":"\\n<figure><img src=\\"https://fastly.jsdelivr.net/gh/bucketio/img12@main/2024/10/20/1729433178592-bd22ef03-8cac-448b-9ab1-3f2dcddac6c3.jpeg\\" alt=\\"\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption></figcaption></figure>\\n<h2><strong>摘要</strong></h2>\\n<p>本周AI周报关注几项前沿生成模型：ZeroComp在3D合成领域开辟新路径，CtrLoRA实现可控图像生成的高效框架，F5-TTS通过流匹配技术提升语音生成效果，HyperDreamBooth加快个性化文本到图像的速度。其余成果详见正文。</p>","autoDesc":true}')}}]);