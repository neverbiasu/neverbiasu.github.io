"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[1001],{66262:(t,r)=>{r.A=(t,r)=>{const i=t.__vccOpts||t;for(const[t,d]of r)i[t]=d;return i}},81450:(t,r,i)=>{i.r(r),i.d(r,{comp:()=>a,data:()=>n});var d=i(20641);const e={},a=(0,i(66262).A)(e,[["render",function(t,r){return(0,d.uX)(),(0,d.CE)("div",null,r[0]||(r[0]=[(0,d.Fv)('<h1 id="【论文精读】icedit-in-context-edit——大规模扩散transformer的指令图像编辑新范式" tabindex="-1"><a class="header-anchor" href="#【论文精读】icedit-in-context-edit——大规模扩散transformer的指令图像编辑新范式"><span>【论文精读】ICEdit：In-Context Edit——大规模扩散Transformer的指令图像编辑新范式</span></a></h1><figure><img src="https://github.com/River-Zhang/ICEdit/raw/main/docs/images/teaser.png" alt="ICEdit多轮编辑示例" tabindex="0" loading="lazy"><figcaption>ICEdit多轮编辑示例</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>ICEdit 基于大规模 Diffusion Transformer，提出 in-context 编辑、LoRA-MoE 微调和 Early Filter 策略，实现高效高质的指令图像编辑。仅用极少数据和参数即超越 SOTA，具备强泛化与实际应用潜力。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li><li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li><li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li><li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li></ol><hr><h2 id="背景与研究目标" tabindex="-1"><a class="header-anchor" href="#背景与研究目标"><span>背景与研究目标</span></a></h2><h3 id="领域与任务定义" tabindex="-1"><a class="header-anchor" href="#领域与任务定义"><span>领域与任务定义</span></a></h3><p>指令图像编辑（Instruction-based Image Editing）是指通过自然语言对图像进行内容、风格等多维度的自动化编辑，广泛应用于内容创作、辅助设计、智能交互等场景。任务目标是：输入一张原始图片和一条自然语言编辑指令，输出符合指令要求的编辑后图片，要求编辑区域精准、未编辑区域保持一致，且整体视觉质量高。</p><h3 id="现有方法局限与挑战" tabindex="-1"><a class="header-anchor" href="#现有方法局限与挑战"><span>现有方法局限与挑战</span></a></h3><ul><li>微调类方法（如 InstructPix2Pix、Emu Edit、UltraEdit）需大规模数据和高昂算力，虽精度高但效率低，泛化性有限。</li><li>免训练方法（如 Prompt-to-Prompt、StableFlow 等）高效但难以理解复杂指令，编辑质量有限，适用范围受限。</li></ul><figure><img src="https://arxiv.org/html/2504.20690v1/x2.png" alt="数据效率与性能对比" tabindex="0" loading="lazy"><figcaption>数据效率与性能对比</figcaption></figure><h3 id="相关文献与数据集" tabindex="-1"><a class="header-anchor" href="#相关文献与数据集"><span>相关文献与数据集</span></a></h3><ul><li><strong>主要参考文献</strong>：InstructPix2Pix（CVPR 2023）、Emu Edit（arXiv 2023）、UltraEdit（NeurIPS 2024）、Flux/FLUX.1（基础DiT架构）、MagicBrush/OmniEdit（数据集）</li><li><strong>数据集</strong>：MagicBrush（9K）、OmniEdit（40K）等公开指令编辑数据集，涵盖多类型编辑任务。</li></ul><h3 id="核心问题" tabindex="-1"><a class="header-anchor" href="#核心问题"><span>核心问题</span></a></h3><p>如何在不牺牲编辑精度和泛化能力的前提下，极大提升训练和推理效率，实现高质量、低成本、易扩展的指令图像编辑范式。</p><hr><h2 id="方法与创新点" tabindex="-1"><a class="header-anchor" href="#方法与创新点"><span>方法与创新点</span></a></h2><h3 id="方法整体流程" tabindex="-1"><a class="header-anchor" href="#方法整体流程"><span>方法整体流程</span></a></h3><figure><img src="https://arxiv.org/html/2504.20690v1/x4.png" alt="ICEdit整体流程与in-context编辑框架" tabindex="0" loading="lazy"><figcaption>ICEdit整体流程与in-context编辑框架</figcaption></figure><ol><li><strong>In-Context 编辑框架</strong>：借鉴语言模型 in-context learning 思想，设计“IC Prompt”结构，将原图与编辑指令并列输入，模型通过上下文理解实现零样本编辑。 <blockquote><p>“a side-by-side image of the same {subject}: the left depicts the original {description}, while the right mirrors the left but applies {edit instruction}.” <img src="https://arxiv.org/html/2504.20690v1/x3.png" alt="IC Prompt注意力可视化" loading="lazy"></p></blockquote></li><li><strong>LoRA-MoE 混合微调</strong>：在 DiT 基础上引入参数高效的 LoRA 适配器，并结合 MoE 专家路由机制，动态激活不同编辑专家，提升多样编辑能力。 <img src="https://arxiv.org/html/2504.20690v1/x5.png" alt="LoRA-MoE结构示意" loading="lazy"></li><li><strong>Early Filter 推理筛选</strong>：推理阶段采样多组初始噪声，利用视觉语言模型（VLM）快速筛选最优噪声，大幅提升编辑一致性和质量。 <img src="https://arxiv.org/html/2504.20690v1/x6.png" alt="Early Filter推理筛选流程" loading="lazy"></li></ol><h3 id="主要创新点" tabindex="-1"><a class="header-anchor" href="#主要创新点"><span>主要创新点</span></a></h3><ul><li><strong>IC Prompt 结构</strong>：无需结构改动即可理解并执行复杂编辑指令，显著提升泛化性和编辑一致性。</li><li><strong>LoRA-MoE 混合架构</strong>：多专家 LoRA 并行，路由器动态分配编辑专家，兼顾参数效率与编辑多样性。</li><li><strong>推理 Early Filter</strong>：少步去噪+VLM 筛选，提升编辑鲁棒性和主观质量。</li></ul><h3 id="关键技术细节" tabindex="-1"><a class="header-anchor" href="#关键技术细节"><span>关键技术细节</span></a></h3><ul><li>微调数据仅 5 万条，参数量仅为全量微调的 1%。</li><li>支持 text-to-image 和 inpainting 两种 DiT 框架，兼容多种编辑场景。</li><li>推理时可选用如 Qwen-VL 等大模型作为 VLM 评判器。</li></ul><hr><h2 id="实验与结果分析" tabindex="-1"><a class="header-anchor" href="#实验与结果分析"><span>实验与结果分析</span></a></h2><h3 id="与sota方法对比" tabindex="-1"><a class="header-anchor" href="#与sota方法对比"><span>与SOTA方法对比</span></a></h3><ul><li><strong>数据与参数效率</strong>：仅用 0.5% 训练数据、1% 参数量即可超越 SOTA。</li><li><strong>编辑质量</strong>：在 MagicBrush、Emu Edit 等基准上，ICEdit 在编辑精度、未编辑区域保持、主观质量等方面均优于主流方法。</li><li><strong>推理筛选增益</strong>：Early Filter 策略带来 VIE-Score 显著提升，复杂编辑场景下优势更明显。</li></ul><h4 id="magicbrush-测试集主客观指标对比" tabindex="-1"><a class="header-anchor" href="#magicbrush-测试集主客观指标对比"><span>MagicBrush 测试集主客观指标对比</span></a></h4><figure><img src="https://arxiv.org/html/2504.20690v1/x7.png" alt="MagicBrush主观与客观指标对比" tabindex="0" loading="lazy"><figcaption>MagicBrush主观与客观指标对比</figcaption></figure><table><thead><tr><th>方法</th><th>训练参数量</th><th>L1↓</th><th>CLIP-I↑</th><th>DINO↑</th></tr></thead><tbody><tr><td>InstructP2P</td><td>0.9B</td><td>0.114</td><td>0.851</td><td>0.744</td></tr><tr><td>MagicBrush</td><td>0.9B</td><td>0.074</td><td>0.908</td><td>0.847</td></tr><tr><td>UltraEdit</td><td>2.5B</td><td>0.066</td><td>0.904</td><td>0.852</td></tr><tr><td>FluxEdit</td><td>12B</td><td>0.114</td><td>0.779</td><td>0.663</td></tr><tr><td>FLUX.1 Fill</td><td>-</td><td>0.192</td><td>0.795</td><td>0.669</td></tr><tr><td>RF-Solver Edit</td><td>-</td><td>0.112</td><td>0.766</td><td>0.675</td></tr><tr><td>ACE++</td><td>12B</td><td>0.195</td><td>0.741</td><td>0.591</td></tr><tr><td>ICEdit (Ours)</td><td><strong>0.2B</strong></td><td><strong>0.060</strong></td><td><strong>0.928</strong></td><td><strong>0.853</strong></td></tr></tbody></table><h4 id="emu-测试集主客观指标对比" tabindex="-1"><a class="header-anchor" href="#emu-测试集主客观指标对比"><span>Emu 测试集主客观指标对比</span></a></h4><table><thead><tr><th>方法</th><th>数据量</th><th>CLIP-I↑</th><th>CLIP-Out↑</th><th>DINO↑</th><th>GPT-4o主观分数</th></tr></thead><tbody><tr><td>InstructP2P</td><td>0.45M</td><td>0.856</td><td>0.292</td><td>0.773</td><td>0.36</td></tr><tr><td>MagicBrush</td><td>0.47M</td><td>0.877</td><td>0.298</td><td>0.807</td><td>0.48</td></tr><tr><td>EmuEdit</td><td>10M</td><td>0.877</td><td><strong>0.306</strong></td><td>0.844</td><td><strong>0.72</strong></td></tr><tr><td>UltraEdit</td><td>3M</td><td>0.880</td><td>0.304</td><td>0.847</td><td>0.54</td></tr><tr><td>FluxEdit</td><td>1.2M</td><td>0.852</td><td>0.282</td><td>0.760</td><td>0.22</td></tr><tr><td>FLUX.1 Fill</td><td>-</td><td>0.794</td><td>0.273</td><td>0.659</td><td>0.24</td></tr><tr><td>RF-Solver Edit</td><td>-</td><td>0.797</td><td>0.309</td><td>0.683</td><td>0.32</td></tr><tr><td>ACE++</td><td>54M</td><td>0.791</td><td>0.280</td><td>0.687</td><td>0.24</td></tr><tr><td>ICEdit (Ours)</td><td><strong>0.05M</strong></td><td><strong>0.907</strong></td><td>0.305</td><td><strong>0.866</strong></td><td>0.68</td></tr></tbody></table><figure><img src="https://arxiv.org/html/2504.20690v1/x8.png" alt="VIE-Score与主观评测对比" tabindex="0" loading="lazy"><figcaption>VIE-Score与主观评测对比</figcaption></figure><h3 id="消融实验" tabindex="-1"><a class="header-anchor" href="#消融实验"><span>消融实验</span></a></h3><ul><li><strong>消融实验</strong>：IC Prompt、LoRA-MoE、Early Filter 三者均为性能提升关键。</li></ul><h4 id="消融实验表" tabindex="-1"><a class="header-anchor" href="#消融实验表"><span>消融实验表</span></a></h4><table><thead><tr><th>设置</th><th>参数量</th><th>CLIP-I↑</th><th>CLIP-T↑</th><th>GPT↑</th></tr></thead><tbody><tr><td>Training-free w/o IC prompt</td><td>-</td><td>0.681</td><td>0.258</td><td>0.14</td></tr><tr><td>Training-free w/ IC prompt</td><td>-</td><td>0.794</td><td>0.273</td><td>0.24</td></tr><tr><td>Only MoE module</td><td><strong>130M</strong></td><td><strong>0.929</strong></td><td>0.300</td><td>0.51</td></tr><tr><td>LoRA (r=64) w/ IC prompt</td><td>240M</td><td>0.911</td><td>0.301</td><td>0.60</td></tr><tr><td>Ours w/o IC prompt</td><td>214M</td><td>0.896</td><td>0.300</td><td>0.62</td></tr><tr><td>Ours</td><td>214M</td><td>0.907</td><td><strong>0.305</strong></td><td><strong>0.68</strong></td></tr></tbody></table><h4 id="复杂编辑与风格迁移能力" tabindex="-1"><a class="header-anchor" href="#复杂编辑与风格迁移能力"><span>复杂编辑与风格迁移能力</span></a></h4><figure><img src="https://arxiv.org/html/2504.20690v1/x11.png" alt="复杂编辑与风格迁移示例" tabindex="0" loading="lazy"><figcaption>复杂编辑与风格迁移示例</figcaption></figure><ul><li>ICEdit 在风格迁移、元素添加、局部重绘等复杂编辑任务上表现出更强的泛化和一致性。</li></ul><h3 id="应用与实际场景" tabindex="-1"><a class="header-anchor" href="#应用与实际场景"><span>应用与实际场景</span></a></h3><figure><img src="https://arxiv.org/html/2504.20690v1/x12.png" alt="实际应用与多样编辑能力展示" tabindex="0" loading="lazy"><figcaption>实际应用与多样编辑能力展示</figcaption></figure><ul><li>支持手部细化、风格化、水印去除、重光照等多种实际应用，无需额外微调即可适配多任务。</li></ul><hr><h2 id="模型启发与方法延伸" tabindex="-1"><a class="header-anchor" href="#模型启发与方法延伸"><span>模型启发与方法延伸</span></a></h2><ul><li><strong>通用性</strong>：ICEdit 框架可迁移至其他扩散模型、跨模态编辑等任务。</li><li><strong>工程启发</strong>：LoRA-MoE 结构为大模型高效微调提供新思路，推理筛选机制适用于生成式任务的质量控制。</li><li><strong>应用前景</strong>：适用于内容创作、辅助设计、风格迁移、局部修复等多种实际场景。</li></ul><hr><h2 id="结论与未来展望" tabindex="-1"><a class="header-anchor" href="#结论与未来展望"><span>结论与未来展望</span></a></h2><ul><li><strong>贡献总结</strong>：ICEdit 首次将 in-context learning 范式引入指令图像编辑，结合 LoRA-MoE 和推理筛选，实现高效、高质、低成本的编辑能力。</li><li><strong>优势与不足</strong>：极大降低数据和算力门槛，复杂编辑场景表现优异，但对 VLM 评判器和编辑指令表述仍有依赖。</li><li><strong>未来方向</strong>： <ul><li>探索更通用、可控的 IC Prompt 设计，例如支持多轮对话式编辑、复合指令、区域/对象级控制等，提升模型对复杂编辑需求的理解和执行能力</li><li>结合更大规模多模态模型（如更强的视觉语言模型）提升编辑指令的理解力和生成质量</li><li>推广至视频编辑、跨模态生成等更广泛领域，实现时序一致性和多模态协同</li><li>拓展到图像风格迁移、文字添加、服饰更换等更细致和多样化的图像编辑任务</li></ul></li></ul><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h3><ol><li><a href="https://arxiv.org/abs/2504.20690" target="_blank" rel="noopener noreferrer">论文原文</a></li><li><a href="https://river-zhang.github.io/ICEdit-gh-pages/" target="_blank" rel="noopener noreferrer">项目主页</a></li><li><a href="https://github.com/River-Zhang/ICEdit" target="_blank" rel="noopener noreferrer">代码仓库</a></li><li><a href="https://huggingface.co/sanaka87/ICEdit-MoE-LoRA" target="_blank" rel="noopener noreferrer">模型权重</a></li><li><a href="https://neverbiasu.github.io/zh/posts/papers/icedit-blog/" target="_blank" rel="noopener noreferrer">alphaXiv 博客解读</a></li><li><a href="https://arxiv.org/abs/2311.10089" target="_blank" rel="noopener noreferrer">Emu Edit 论文</a></li><li><a href="https://arxiv.org/abs/2312.14867" target="_blank" rel="noopener noreferrer">UltraEdit 论文</a></li></ol>',56)]))}]]),n=JSON.parse('{"path":"/zh/posts/papers/icedit.html","title":"【论文精读】ICEdit：In-Context Edit——大规模扩散Transformer的指令图像编辑新范式","lang":"zh-CN","frontmatter":{"description":"【论文精读】ICEdit：In-Context Edit——大规模扩散Transformer的指令图像编辑新范式 ICEdit多轮编辑示例ICEdit多轮编辑示例 摘要 ICEdit 基于大规模 Diffusion Transformer，提出 in-context 编辑、LoRA-MoE 微调和 Early Filter 策略，实现高效高质的指令图像...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/papers/icedit.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"【论文精读】ICEdit：In-Context Edit——大规模扩散Transformer的指令图像编辑新范式"}],["meta",{"property":"og:description","content":"【论文精读】ICEdit：In-Context Edit——大规模扩散Transformer的指令图像编辑新范式 ICEdit多轮编辑示例ICEdit多轮编辑示例 摘要 ICEdit 基于大规模 Diffusion Transformer，提出 in-context 编辑、LoRA-MoE 微调和 Early Filter 策略，实现高效高质的指令图像..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://github.com/River-Zhang/ICEdit/raw/main/docs/images/teaser.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"【论文精读】ICEdit：In-Context Edit——大规模扩散Transformer的指令图像编辑新范式\\",\\"image\\":[\\"https://github.com/River-Zhang/ICEdit/raw/main/docs/images/teaser.png\\",\\"https://arxiv.org/html/2504.20690v1/x2.png\\",\\"https://arxiv.org/html/2504.20690v1/x4.png\\",\\"https://arxiv.org/html/2504.20690v1/x3.png\\",\\"https://arxiv.org/html/2504.20690v1/x5.png\\",\\"https://arxiv.org/html/2504.20690v1/x6.png\\",\\"https://arxiv.org/html/2504.20690v1/x7.png\\",\\"https://arxiv.org/html/2504.20690v1/x8.png\\",\\"https://arxiv.org/html/2504.20690v1/x11.png\\",\\"https://arxiv.org/html/2504.20690v1/x12.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"背景与研究目标","slug":"背景与研究目标","link":"#背景与研究目标","children":[{"level":3,"title":"领域与任务定义","slug":"领域与任务定义","link":"#领域与任务定义","children":[]},{"level":3,"title":"现有方法局限与挑战","slug":"现有方法局限与挑战","link":"#现有方法局限与挑战","children":[]},{"level":3,"title":"相关文献与数据集","slug":"相关文献与数据集","link":"#相关文献与数据集","children":[]},{"level":3,"title":"核心问题","slug":"核心问题","link":"#核心问题","children":[]}]},{"level":2,"title":"方法与创新点","slug":"方法与创新点","link":"#方法与创新点","children":[{"level":3,"title":"方法整体流程","slug":"方法整体流程","link":"#方法整体流程","children":[]},{"level":3,"title":"主要创新点","slug":"主要创新点","link":"#主要创新点","children":[]},{"level":3,"title":"关键技术细节","slug":"关键技术细节","link":"#关键技术细节","children":[]}]},{"level":2,"title":"实验与结果分析","slug":"实验与结果分析","link":"#实验与结果分析","children":[{"level":3,"title":"与SOTA方法对比","slug":"与sota方法对比","link":"#与sota方法对比","children":[]},{"level":3,"title":"消融实验","slug":"消融实验","link":"#消融实验","children":[]},{"level":3,"title":"应用与实际场景","slug":"应用与实际场景","link":"#应用与实际场景","children":[]}]},{"level":2,"title":"模型启发与方法延伸","slug":"模型启发与方法延伸","link":"#模型启发与方法延伸","children":[]},{"level":2,"title":"结论与未来展望","slug":"结论与未来展望","link":"#结论与未来展望","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":5.99,"words":1797},"filePathRelative":"zh/posts/papers/icedit.md","excerpt":"\\n<figure><img src=\\"https://github.com/River-Zhang/ICEdit/raw/main/docs/images/teaser.png\\" alt=\\"ICEdit多轮编辑示例\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>ICEdit多轮编辑示例</figcaption></figure>\\n<h2>摘要</h2>\\n<p>ICEdit 基于大规模 Diffusion Transformer，提出 in-context 编辑、LoRA-MoE 微调和 Early Filter 策略，实现高效高质的指令图像编辑。仅用极少数据和参数即超越 SOTA，具备强泛化与实际应用潜力。</p>","autoDesc":true}')}}]);