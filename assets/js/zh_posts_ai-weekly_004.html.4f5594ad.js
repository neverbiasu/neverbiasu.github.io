"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[9092],{6262:(e,n)=>{n.A=(e,n)=>{const t=e.__vccOpts||e;for(const[e,r]of n)t[e]=r;return t}},7950:(e,n,t)=>{t.r(n),t.d(n,{comp:()=>i,data:()=>o});var r=t(641);const a={},i=(0,t(6262).A)(a,[["render",function(e,n){return(0,r.uX)(),(0,r.CE)("div",null,n[0]||(n[0]=[(0,r.Fv)('<h1 id="qwen-2-5-coder-多语言代码生成新高度-instantdrag-实时交互式图像生成-omnigen-多模态生成研究进展【ai-周报】" tabindex="-1"><a class="header-anchor" href="#qwen-2-5-coder-多语言代码生成新高度-instantdrag-实时交互式图像生成-omnigen-多模态生成研究进展【ai-周报】"><span>Qwen 2.5 Coder: 多语言代码生成新高度，InstantDrag 实时交互式图像生成，Omnigen 多模态生成研究进展【AI 周报】</span></a></h1><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本期周报涵盖 Qwen 2.5 Coder 的代码生成进展、InstantDrag 的交互式图像生成工具、以及 Omnigen 的多模态生成研究。此外，还介绍了 Diffusion-e2e-ft 扩散模型优化、LVCD 的线稿视频上色方法和 MoCoop 多模态合作学习。</p><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li>Qwen 2.5 Coder: 代码生成的最新突破</li><li>InstantDrag: 交互式图像拖拽生成新工具</li><li>Diffusion-e2e-ft: 自监督扩散模型优化</li><li>Omnigen: 灵活生成模型的研究进展</li><li>LVCD: 基于扩散模型的线稿视频上色</li><li>MoCoop: 多模态合作学习的新方法</li></ol><hr><h2 id="qwen-2-5-coder-代码生成的最新突破" tabindex="-1"><a class="header-anchor" href="#qwen-2-5-coder-代码生成的最新突破"><span>Qwen 2.5 Coder: 代码生成的最新突破</span></a></h2><figure><img src="https://camo.githubusercontent.com/3ff15392a5127c6a076441206b831c87e0137a55b7da7eb469ba0d6df64d89b2/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e322e352f5177656e322e352d436f6465722f636f6465722d6d61696e2e706e67" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>概要</strong>: Qwen 2.5 Coder 是 QwenLM 团队推出的最新代码生成模型，专注于多语言编程任务。它通过精细化的预训练和微调，显著提高了代码生成、自动补全和代码理解的能力。该模型能够处理更复杂的代码逻辑，并支持多种编程语言，适用于各类编程场景和开发需求。Qwen 2.5 Coder 在主流代码生成基准上刷新了多个记录，展示了其在智能编程助手领域的广泛应用前景。</p><p><strong>标签</strong>: #QwenLM #代码生成 #多语言支持 #编程工具 # LLM</p><hr><h2 id="instantdrag-交互式图像拖拽生成新工具" tabindex="-1"><a class="header-anchor" href="#instantdrag-交互式图像拖拽生成新工具"><span>InstantDrag: 交互式图像拖拽生成新工具</span></a></h2><figure><img src="https://github.com/SNU-VGILab/InstantDrag/raw/main/assets/demo.gif" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>概要</strong>: InstantDrag 是由 SNU-VGILab 团队开发的交互式图像生成工具。用户只需通过简单的拖拽操作，即可实时调整图像中的细节，生成符合需求的视觉效果。该工具结合了深度学习技术与用户交互界面，支持实时图像修改，适合应用于艺术创作、广告设计等需要高效视觉调整的领域。InstantDrag 展现了交互式 AI 生成技术在设计中的潜力。</p><p><strong>标签</strong>: #图像生成 #深度学习 #实时交互 #设计工具 #SNU-VGILab</p><hr><h2 id="diffusion-e2e-ft-自监督扩散模型优化" tabindex="-1"><a class="header-anchor" href="#diffusion-e2e-ft-自监督扩散模型优化"><span>Diffusion-e2e-ft: 自监督扩散模型优化</span></a></h2><figure><img src="https://gonzalomartingarcia.github.io/diffusion-e2e-ft/static/e2e_ft_imgs/teaser_plot.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>概要</strong>: Diffusion-e2e-ft 是由 Visual Computing Institute 提出的端到端自监督学习方法，用于优化扩散模型的训练过程。该方法通过结合端到端的微调技术，大幅提升了扩散模型在图像生成任务中的效率和效果。Diffusion-e2e-ft 不仅缩短了训练时间，还提升了生成图像的质量，特别适用于大规模数据集的处理和生成任务。</p><p><strong>标签</strong>: #扩散模型 #自监督学习 #端到端微调 #图像生成 #VisualComputingInstitute</p><hr><h2 id="omnigen-灵活生成模型的研究进展" tabindex="-1"><a class="header-anchor" href="#omnigen-灵活生成模型的研究进展"><span>Omnigen: 灵活生成模型的研究进展</span></a></h2><figure><img src="https://github.com/VectorSpaceLab/OmniGen/raw/main/imgs/overall.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>概要</strong>: Omnigen 是由 VectorSpaceLab 提出的多模态生成模型，专注于处理不同类型的输入数据，如图像、文本、音频等。该模型通过创新的生成架构，能够有效解耦和处理不同模态之间的转换与一致性问题。Omnigen 展示了其在跨模态生成任务中的潜力，适用于多领域数据的生成与转换，例如图像到文本的生成或多模态内容合成。</p><p><strong>标签</strong>: #多模态生成 #灵活模型 #数据转换 #跨模态 #VectorSpaceLab</p><hr><h2 id="lvcd-基于扩散模型的线稿视频上色" tabindex="-1"><a class="header-anchor" href="#lvcd-基于扩散模型的线稿视频上色"><span>LVCD: 基于扩散模型的线稿视频上色</span></a></h2><figure><img src="https://luckyhzt.github.io/lvcd/figures/architecture.svg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>概要</strong>: LVCD 是第一个用于参考线稿视频上色的扩散模型框架，利用大规模预训练的视频扩散模型生成颜色一致的动画视频。该方法通过 <strong>Sketch-guided ControlNet</strong> 控制线稿生成动画视频，并引入 <strong>参考注意力机制</strong> 实现颜色从参考帧到其他帧的迁移。LVCD 提出了一种序列采样方案，能够生成长时间一致的动画视频，特别适用于大幅动作场景。</p><p><strong>标签</strong>: #视频扩散模型 #线稿上色 #时间一致性 #动画生成</p><hr><h2 id="mocoop-多模态合作学习的新方法" tabindex="-1"><a class="header-anchor" href="#mocoop-多模态合作学习的新方法"><span>MoCoop: 多模态合作学习的新方法</span></a></h2><figure><img src="https://arxiv.org/html/2409.12011v1/extracted/5863488/figures/fig1.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><strong>概要</strong>: MoCoop（Multimodal Cooperative Learning）提出了一种新的多模态合作学习方法，通过不同模态数据的协同学习，提升各模态的特征共享能力。该方法尤其适合视觉和文本等不同模态数据的处理，显著提高了多模态任务的效率和性能。MoCoop 在多个多模态基准上取得了优异表现，展示了其在跨模态理解和生成中的潜力。</p><p><strong>标签</strong>: #多模态学习 #合作学习 #特征共享 #深度学习</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h3><ol><li><a href="https://qwenlm.github.io/blog/qwen2.5-coder/" target="_blank" rel="noopener noreferrer">Qwen 2.5 Coder 博客</a></li><li><a href="https://github.com/QwenLM/Qwen2.5-Coder.git" target="_blank" rel="noopener noreferrer">Qwen 2.5 Coder GitHub 仓库</a></li><li><a href="https://arxiv.org/pdf/2409.12186" target="_blank" rel="noopener noreferrer">Qwen 2.5 Coder 论文</a></li><li><a href="https://joonghyuk.com/instantdrag-web/" target="_blank" rel="noopener noreferrer">InstantDrag 演示</a></li><li><a href="https://github.com/SNU-VGILab/InstantDrag" target="_blank" rel="noopener noreferrer">InstantDrag GitHub 仓库</a></li><li><a href="https://arxiv.org/abs/2409.08857" target="_blank" rel="noopener noreferrer">InstantDrag 论文</a></li><li><a href="https://gonzalomartingarcia.github.io/diffusion-e2e-ft/" target="_blank" rel="noopener noreferrer">Diffusion-e2e-ft 演示</a></li><li><a href="https://github.com/VisualComputingInstitute/diffusion-e2e-ft" target="_blank" rel="noopener noreferrer">Diffusion-e2e-ft GitHub 仓库</a></li><li><a href="https://arxiv.org/pdf/2409.11355" target="_blank" rel="noopener noreferrer">Diffusion-e2e-ft 论文</a></li><li><a href="https://github.com/vectorspacelab/omnigen" target="_blank" rel="noopener noreferrer">Omnigen GitHub 仓库</a></li><li><a href="https://arxiv.org/pdf/2409.11340" target="_blank" rel="noopener noreferrer">Omnigen 论文</a></li><li><a href="https://luckyhzt.github.io/lvcd" target="_blank" rel="noopener noreferrer">LVCD 演示</a></li><li><a href="https://arxiv.org/pdf/2409.12960" target="_blank" rel="noopener noreferrer">LVCD 论文</a></li><li><a href="https://anonymous.4open.science/r/mocoop-6387/README.md" target="_blank" rel="noopener noreferrer">MoCoop GitHub 仓库</a></li><li><a href="https://arxiv.org/pdf/2409.12011" target="_blank" rel="noopener noreferrer">MoCoop 论文</a></li></ol>',38)]))}]]),o=JSON.parse('{"path":"/zh/posts/ai-weekly/004.html","title":"Qwen 2.5 Coder: 多语言代码生成新高度，InstantDrag 实时交互式图像生成，Omnigen 多模态生成研究进展【AI 周报】","lang":"zh-CN","frontmatter":{"description":"Qwen 2.5 Coder: 多语言代码生成新高度，InstantDrag 实时交互式图像生成，Omnigen 多模态生成研究进展【AI 周报】 摘要 本期周报涵盖 Qwen 2.5 Coder 的代码生成进展、InstantDrag 的交互式图像生成工具、以及 Omnigen 的多模态生成研究。此外，还介绍了 Diffusion-e2e-ft 扩散...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/004.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"Qwen 2.5 Coder: 多语言代码生成新高度，InstantDrag 实时交互式图像生成，Omnigen 多模态生成研究进展【AI 周报】"}],["meta",{"property":"og:description","content":"Qwen 2.5 Coder: 多语言代码生成新高度，InstantDrag 实时交互式图像生成，Omnigen 多模态生成研究进展【AI 周报】 摘要 本期周报涵盖 Qwen 2.5 Coder 的代码生成进展、InstantDrag 的交互式图像生成工具、以及 Omnigen 的多模态生成研究。此外，还介绍了 Diffusion-e2e-ft 扩散..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://camo.githubusercontent.com/3ff15392a5127c6a076441206b831c87e0137a55b7da7eb469ba0d6df64d89b2/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e322e352f5177656e322e352d436f6465722f636f6465722d6d61696e2e706e67"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Qwen 2.5 Coder: 多语言代码生成新高度，InstantDrag 实时交互式图像生成，Omnigen 多模态生成研究进展【AI 周报】\\",\\"image\\":[\\"https://camo.githubusercontent.com/3ff15392a5127c6a076441206b831c87e0137a55b7da7eb469ba0d6df64d89b2/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e322e352f5177656e322e352d436f6465722f636f6465722d6d61696e2e706e67\\",\\"https://github.com/SNU-VGILab/InstantDrag/raw/main/assets/demo.gif\\",\\"https://gonzalomartingarcia.github.io/diffusion-e2e-ft/static/e2e_ft_imgs/teaser_plot.png\\",\\"https://github.com/VectorSpaceLab/OmniGen/raw/main/imgs/overall.jpg\\",\\"https://luckyhzt.github.io/lvcd/figures/architecture.svg\\",\\"https://arxiv.org/html/2409.12011v1/extracted/5863488/figures/fig1.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"Qwen 2.5 Coder: 代码生成的最新突破","slug":"qwen-2-5-coder-代码生成的最新突破","link":"#qwen-2-5-coder-代码生成的最新突破","children":[]},{"level":2,"title":"InstantDrag: 交互式图像拖拽生成新工具","slug":"instantdrag-交互式图像拖拽生成新工具","link":"#instantdrag-交互式图像拖拽生成新工具","children":[]},{"level":2,"title":"Diffusion-e2e-ft: 自监督扩散模型优化","slug":"diffusion-e2e-ft-自监督扩散模型优化","link":"#diffusion-e2e-ft-自监督扩散模型优化","children":[]},{"level":2,"title":"Omnigen: 灵活生成模型的研究进展","slug":"omnigen-灵活生成模型的研究进展","link":"#omnigen-灵活生成模型的研究进展","children":[]},{"level":2,"title":"LVCD: 基于扩散模型的线稿视频上色","slug":"lvcd-基于扩散模型的线稿视频上色","link":"#lvcd-基于扩散模型的线稿视频上色","children":[]},{"level":2,"title":"MoCoop: 多模态合作学习的新方法","slug":"mocoop-多模态合作学习的新方法","link":"#mocoop-多模态合作学习的新方法","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":4.26,"words":1278},"filePathRelative":"zh/posts/ai-weekly/004.md","excerpt":"\\n<h2>摘要</h2>\\n<p>本期周报涵盖 Qwen 2.5 Coder 的代码生成进展、InstantDrag 的交互式图像生成工具、以及 Omnigen 的多模态生成研究。此外，还介绍了 Diffusion-e2e-ft 扩散模型优化、LVCD 的线稿视频上色方法和 MoCoop 多模态合作学习。</p>\\n<h2>目录</h2>\\n<ol>\\n<li>Qwen 2.5 Coder: 代码生成的最新突破</li>\\n<li>InstantDrag: 交互式图像拖拽生成新工具</li>\\n<li>Diffusion-e2e-ft: 自监督扩散模型优化</li>\\n<li>Omnigen: 灵活生成模型的研究进展</li>\\n<li>LVCD: 基于扩散模型的线稿视频上色</li>\\n<li>MoCoop: 多模态合作学习的新方法</li>\\n</ol>","autoDesc":true}')}}]);