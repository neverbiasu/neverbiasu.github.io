"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[5822],{6262:(t,e)=>{e.A=(t,e)=>{const r=t.__vccOpts||t;for(const[t,n]of e)r[t]=n;return r}},8987:(t,e,r)=>{r.r(e),r.d(e,{comp:()=>a,data:()=>i});var n=r(641);const l={},a=(0,r(6262).A)(l,[["render",function(t,e){return(0,n.uX)(),(0,n.CE)("div",null,e[0]||(e[0]=[(0,n.Fv)('<h1 id="【论文精读】attention-is-all-you-need-transformer-架构详解" tabindex="-1"><a class="header-anchor" href="#【论文精读】attention-is-all-you-need-transformer-架构详解"><span>【论文精读】Attention Is All You Need：Transformer 架构详解</span></a></h1><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><h2 id="transformer-由-vaswani-等人于-2017-年提出-首次完全基于自注意力机制-self-attention-实现序列建模-摒弃了-rnn-cnn-结构。该模型极大提升了并行效率和长距离依赖建模能力-成为-nlp、cv-等多模态领域的基础架构。论文提出的多头注意力、位置编码等机制-推动了大模型和生成式-ai-的快速发展。" tabindex="-1"><a class="header-anchor" href="#transformer-由-vaswani-等人于-2017-年提出-首次完全基于自注意力机制-self-attention-实现序列建模-摒弃了-rnn-cnn-结构。该模型极大提升了并行效率和长距离依赖建模能力-成为-nlp、cv-等多模态领域的基础架构。论文提出的多头注意力、位置编码等机制-推动了大模型和生成式-ai-的快速发展。"><span>Transformer 由 Vaswani 等人于 2017 年提出，首次完全基于自注意力机制（Self-Attention）实现序列建模，摒弃了 RNN/CNN 结构。该模型极大提升了并行效率和长距离依赖建模能力，成为 NLP、CV 等多模态领域的基础架构。论文提出的多头注意力、位置编码等机制，推动了大模型和生成式 AI 的快速发展。</span></a></h2><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li><li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li><li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li><li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li></ol><hr><h2 id="背景与研究目标" tabindex="-1"><a class="header-anchor" href="#背景与研究目标"><span>背景与研究目标</span></a></h2><ul><li><strong>领域背景</strong>：序列建模任务（如机器翻译、文本生成）传统依赖 RNN/CNN，存在并行效率低、长距离依赖难捕捉等问题。</li><li><strong>核心问题</strong>：如何提升序列建模的效率与表达能力，突破 RNN 的时序瓶颈。</li><li><strong>研究目标</strong>：提出一种完全基于注意力机制的模型，实现高效、可扩展的序列建模。</li><li><strong>论文写作细节</strong>：作者排序随机，贡献明细透明，代码开源，强调极简架构和行业影响力。标题“Attention is all you need”成为学术梗，命名和写作风格对后续论文影响深远。</li><li><strong>写作风格与建议</strong>：原文极简、信息密度高，缺乏故事性。建议写作时突出动机、设计理念与贡献细节，便于读者理解和复用。</li></ul><hr><h2 id="方法与创新点" tabindex="-1"><a class="header-anchor" href="#方法与创新点"><span>方法与创新点</span></a></h2><figure><img src="https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-21.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="整体流程与核心思想" tabindex="-1"><a class="header-anchor" href="#整体流程与核心思想"><span>整体流程与核心思想</span></a></h3><p>Transformer 的整体流程分为编码器（Encoder）和解码器（Decoder）两大部分，均由多层堆叠组成。每一层都包含自注意力机制和前馈网络。其最大特点是：<strong>每个 token 能直接与序列中所有 token 建立联系</strong>，实现全局依赖建模。</p><h4 id="_1-输入嵌入与位置编码" tabindex="-1"><a class="header-anchor" href="#_1-输入嵌入与位置编码"><span>1. 输入嵌入与位置编码</span></a></h4><ul><li><strong>词嵌入</strong>：将输入的每个 token 映射为向量，编码器和解码器共享权重，并乘以 $\\sqrt{d_{model}}$ 保证数值尺度一致。Embedding 层权重与 softmax 前线性层共享，提升参数利用率。由于嵌入向量的 L2 范数通常较小，乘以 $\\sqrt{d_{model}}$ 可以平衡与位置编码的数值尺度。</li><li><strong>位置编码</strong>：Transformer 不具备序列顺序感知能力，因此为每个位置加上正弦/余弦函数编码，公式如下： $$ \\text{PE}<em>{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d</em>{model}}}\\right),\\quad \\text{PE}<em>{(pos,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d</em>{model}}}\\right) $$ 这样模型能区分不同位置的 token。sin/cos 编码利用不同频率的周期性，使模型能通过线性变换推断相对位置，无需额外参数。原文也实验了 learned positional embedding，效果相近。位置编码的引入是为了解决 attention 不具备时序感知能力的问题，保证顺序信息融入表示。</li></ul><h4 id="_2-自注意力机制-self-attention" tabindex="-1"><a class="header-anchor" href="#_2-自注意力机制-self-attention"><span>2. 自注意力机制（Self-Attention）</span></a></h4><ul><li><p><strong>核心思想</strong>：每个 token 通过 Query、Key、Value 三组向量与所有 token 计算相关性（注意力分数），再加权聚合所有 token 的信息。</p></li><li><p><strong>三种用法</strong>：</p><ul><li>Encoder Self-Attention：输入自身做 attention，捕捉全局依赖。</li><li>Decoder Masked Self-Attention：仅关注当前位置及之前，保证自回归生成。</li><li>Encoder-Decoder Attention：解码器每个位置关注编码器输出，信息流动高效。</li></ul></li><li><p><strong>矩阵化实现</strong>：所有 token 的 Q、K、V 拼成矩阵，利用矩阵乘法一次性计算所有 token 间的相关性，极大提升并行效率。</p></li><li><p><strong>主要公式</strong>： $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$ 其中 $Q$、$K$、$V$ 分别为 Query、Key、Value 矩阵，$d_k$ 为 Key 的维度。softmax 保证每行归一化为概率分布。除以 $\\sqrt{d_k}$ 是为防止维度过大导致 softmax 梯度消失。点积注意力实现高效，适合大规模并行；加性注意力适合小 $d_k$，但计算慢。原文给出数学推导：点积方差随 $d_k$ 增大，softmax 梯度趋于0，影响训练。</p></li><li><p><strong>信息流动</strong>：三种 attention 的输入输出、mask 机制、信息流动已分条展开，详见“方法”与“实验”部分。</p></li><li><p><strong>流程分解</strong>：</p><ol><li>计算每个 token 的 Q、K、V。</li><li>Q 与所有 K 做点积，得相关性分数。</li><li>分数做 softmax，得注意力权重。</li><li>权重加权所有 V，得聚合输出。</li></ol></li></ul><h4 id="_3-多头注意力-multi-head-attention" tabindex="-1"><a class="header-anchor" href="#_3-多头注意力-multi-head-attention"><span>3. 多头注意力（Multi-Head Attention）</span></a></h4><ul><li><strong>机制</strong>：Q/K/V 拆分为多组（head），每组独立学习不同的关注模式，最后拼接融合。类似 CNN 多通道，提升表达能力。每头维度为 $d_{model}/h$，头数与维度需权衡。多头机制本质是让模型在不同子空间捕捉不同语义关系，模拟卷积多通道。头数过多/过少均影响性能，需与 $d_{model}$ 配合。</li><li><strong>主要公式</strong>： $$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O $$ 其中 $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i<sup>V)$，$W</sup>O$ 为输出投影矩阵。</li></ul><h4 id="_4-前馈网络与残差连接" tabindex="-1"><a class="header-anchor" href="#_4-前馈网络与残差连接"><span>4. 前馈网络与残差连接</span></a></h4><ul><li><strong>前馈网络</strong>：每个位置独立的两层全连接网络（Point-wise MLP），提升非线性表达能力，区别于 RNN 的时序传递。$d_{ff}$ 通常为 $d_{model}$ 的 4 倍。</li><li><strong>残差连接+LayerNorm</strong>：每层输出加上输入（残差），再归一化，缓解梯度消失、加速收敛。 <ul><li><strong>LayerNorm 与 BatchNorm 区别</strong>： <ul><li>BatchNorm 对每个特征维度，在一个 batch 内所有样本上做归一化（均值为0，方差为1），适合定长输入，依赖 batch 统计量，推理时需保存全局均值和方差。</li><li>LayerNorm 对每个样本的所有特征做归一化（即对每个 token 的所有维度做归一化），不依赖 batch，适合变长序列。每个样本独立归一化，推理和训练一致，尤其适合 NLP/序列建模任务。</li><li>Transformer 输入为变长序列，batch 内样本长度可能不同，且需要对每个 token 独立归一化，LayerNorm 能保证归一化的稳定性和泛化能力，而 BatchNorm 在变长/小 batch 情况下效果不佳。LayerNorm 还能提升优化稳定性，适合小 batch、变长输入，推理/训练一致。</li></ul></li></ul></li></ul><h4 id="_5-mask-机制" tabindex="-1"><a class="header-anchor" href="#_5-mask-机制"><span>5. Mask 机制</span></a></h4><ul><li><strong>解码器自注意力 Mask</strong>：防止生成时看到未来 token，保证自回归生成的合理性。实现方式为将非法位置 attention score 置为 $-\\infty$，softmax 后权重为 0。Mask 机制本质是通过掩蔽未来信息，保证训练和推理一致性，避免信息泄露。</li></ul><hr><h3 id="关键创新点总结" tabindex="-1"><a class="header-anchor" href="#关键创新点总结"><span>关键创新点总结</span></a></h3><table><thead><tr><th>创新点</th><th>原理/机制</th><th>优势与影响</th></tr></thead><tbody><tr><td>自注意力机制</td><td>$Q,K,V$ 计算全局相关性</td><td>并行高效，捕捉长距离依赖</td></tr><tr><td>多头注意力</td><td>多组 $Q,K,V$ 并行计算，拼接输出</td><td>丰富特征表达，提升建模能力</td></tr><tr><td>位置编码</td><td>加入正弦/余弦编码，补充序列顺序信息</td><td>无需递归结构即可建模顺序</td></tr><tr><td>全注意力替代RNN</td><td>完全摒弃递归与卷积，纯注意力堆叠</td><td>极大提升并行效率，易于扩展</td></tr><tr><td>残差+归一化</td><td>每层后加残差与 LayerNorm</td><td>稳定训练，缓解梯度消失</td></tr></tbody></table><hr><h2 id="实验与结果分析" tabindex="-1"><a class="header-anchor" href="#实验与结果分析"><span>实验与结果分析</span></a></h2><p>本节严格参考原论文结构与表格，结合李沐视频的讲解，梳理 Transformer 的实验设计、消融与对比分析。</p><h3 id="_1-训练设置与数据集" tabindex="-1"><a class="header-anchor" href="#_1-训练设置与数据集"><span>1. 训练设置与数据集</span></a></h3><ul><li><strong>任务</strong>：机器翻译（WMT 2014 英德、英法）</li><li><strong>数据集</strong>： <ul><li>英德：约 450 万句对，BPE 分词，37000 词表，源/目标共享</li><li>英法：约 3600 万句对，32000 词表</li></ul></li><li><strong>Batching</strong>：按序列长度分组，每 batch 约 25000 源/目标 token</li><li><strong>硬件</strong>：8 块 P100 GPU</li><li><strong>训练时长</strong>： <ul><li>Base：100k 步，0.4s/步，约 12 小时</li><li>Big：300k 步，1s/步，约 3.5 天</li></ul></li></ul><h3 id="_2-超参数与正则化" tabindex="-1"><a class="header-anchor" href="#_2-超参数与正则化"><span>2. 超参数与正则化</span></a></h3><ul><li><p><strong>Base 模型</strong>：6 层 encoder/decoder，$d_{model}=512$，$d_{ff}=2048$，8 头，$d_k=d_v=64$</p></li><li><p><strong>Big 模型</strong>：6 层，$d_{model}=1024$，$d_{ff}=4096$，16 头</p></li><li><p><strong>优化器</strong>：Adam ($\\beta_1=0.9, \\beta_2=0.98, \\epsilon=10^{-9}$)</p></li><li><p><strong>学习率调度</strong>： $$ \\text{lrate} = d_{model}^{-0.5} \\cdot \\min(\\text{step_num}^{-0.5}, \\text{step_num} \\cdot \\text{warmup_steps}^{-1.5}) $$ warmup_steps=4000</p></li><li><p><strong>正则化</strong>：</p><ul><li>Dropout（Base: 0.1, Big: 0.3），用于子层输出和嵌入+位置编码</li><li>Label Smoothing ($\\epsilon_{ls}=0.1$)，防止过拟合和过度自信。<strong>Label Smoothing 让正确标签的概率为 $1-\\epsilon_{ls}$，其余均分 $\\epsilon_{ls}$，提升泛化和 BLEU 分数。</strong></li></ul></li><li><p><strong>补充</strong>：</p><ul><li><strong>Batching</strong>：按序列长度分组，减少 padding，提升效率。</li><li><strong>参数调优</strong>：Transformer 结构参数极少，主要调层数 $N$、宽度 $d_{model}$、头数 $h$，其余参数可按比例推导，极大简化工程实现。</li><li><strong>训练效率</strong>：Transformer 训练速度快，参数扩展性强，调参空间小，便于迁移和复用。</li></ul></li></ul><h3 id="_3-主要实验结果-表格对比" tabindex="-1"><a class="header-anchor" href="#_3-主要实验结果-表格对比"><span>3. 主要实验结果（表格对比）</span></a></h3><table><thead><tr><th>Model</th><th>EN-DE BLEU</th><th>EN-FR BLEU</th><th>训练成本 (FLOPs)</th></tr></thead><tbody><tr><td>GNMT + RL</td><td>24.6</td><td>39.9</td><td>$2.3 \\times 10^{19}$</td></tr><tr><td>ConvS2S</td><td>25.2</td><td>40.5</td><td>$9.6 \\times 10^{18}$</td></tr><tr><td>Transformer (Base)</td><td>27.3</td><td>38.1</td><td>$3.3 \\times 10^{18}$</td></tr><tr><td>Transformer (Big)</td><td><strong>28.4</strong></td><td><strong>41.8</strong></td><td>$2.3 \\times 10^{19}$</td></tr></tbody></table><ul><li><strong>结论</strong>：Transformer (Big) 在英德/英法任务上 BLEU 均超越 SOTA，训练成本更低，单模型即可超越以往 ensemble。实验表格和消融实验均原样列出并逐项分析。</li></ul><h3 id="_4-消融实验-表-3" tabindex="-1"><a class="header-anchor" href="#_4-消融实验-表-3"><span>4. 消融实验（表 3）</span></a></h3><table><thead><tr><th>变体/超参</th><th>BLEU 变化</th><th>说明</th></tr></thead><tbody><tr><td>单头 attention</td><td>-0.9</td><td>多头更优，头数过多无益</td></tr><tr><td>前馈层宽度减小</td><td>-</td><td>表现下降</td></tr><tr><td>去除位置编码</td><td>大幅下降</td><td>位置编码不可或缺</td></tr><tr><td>去除残差/LayerNorm</td><td>不收敛</td><td>训练极不稳定</td></tr><tr><td>Dropout 调整</td><td>-</td><td>适当 dropout 防止过拟合</td></tr><tr><td>learned pos embed</td><td>≈</td><td>与 sin/cos 编码效果相近</td></tr></tbody></table><h3 id="_5-长序列建模能力与复杂度分析-表-1" tabindex="-1"><a class="header-anchor" href="#_5-长序列建模能力与复杂度分析-表-1"><span>5. 长序列建模能力与复杂度分析（表 1）</span></a></h3><table><thead><tr><th>层类型</th><th>复杂度/层</th><th>顺序操作数</th><th>最大路径长度</th></tr></thead><tbody><tr><td>Self-Attention</td><td>$O(n^2 d)$</td><td>$O(1)$</td><td>$O(1)$</td></tr><tr><td>RNN</td><td>$O(n d^2)$</td><td>$O(n)$</td><td>$O(n)$</td></tr><tr><td>CNN</td><td>$O(k n d^2)$</td><td>$O(1)$</td><td>$O(\\log_k n)$</td></tr><tr><td>限制Self-Attn</td><td>$O(r n d)$</td><td>$O(1)$</td><td>$O(n/r)$</td></tr></tbody></table><ul><li><strong>解读</strong>：Self-Attention 并行度高，路径最短，适合长距离依赖建模，但复杂度随序列长度二次增长。Transformer 归纳偏置更弱，需大数据大模型支撑，未来可能有更优结构。</li><li><strong>补充</strong>：Self-Attention 最大路径长度 $O(1)$，极利于捕捉长距离依赖；但归纳偏置弱，需大数据大模型支撑。</li></ul><h3 id="_6-泛化能力" tabindex="-1"><a class="header-anchor" href="#_6-泛化能力"><span>6. 泛化能力</span></a></h3><ul><li><strong>英语成分句法分析</strong>（表 4）：Transformer 在 WSJ 23 F1 得分 91.3（仅 WSJ 训练），92.7（半监督），优于大多数传统方法，显示良好泛化性</li></ul><hr><h2 id="模型启发与方法延伸" tabindex="-1"><a class="header-anchor" href="#模型启发与方法延伸"><span>模型启发与方法延伸</span></a></h2><ul><li><strong>通用性</strong>：Transformer 架构已成为 NLP、CV、音频、跨模态等领域的基础模型，推动了 Foundation Model 概念。</li><li><strong>衍生模型</strong>：BERT、GPT、ViT、Swin Transformer 等均基于该架构，极大简化了下游任务建模。</li><li><strong>工程实践</strong>：高效并行、易于大规模训练，适合大模型和多模态任务。结构简单，调参空间小，便于迁移和复用。</li><li><strong>未来方向与局限</strong>：Transformer 归纳偏置弱，需大数据大模型支撑，未来可能有更优结构（如纯 MLP 架构等）。Attention 不是唯一关键，MLP、残差等同样不可或缺。</li><li><strong>补充</strong>：Transformer 的设计理念、命名、写作风格、代码开源等对后续论文和工程实践有深远影响。建议后续论文注重贡献细节、结构创新和工程可复用性。</li></ul><hr><h2 id="结论与未来展望" tabindex="-1"><a class="header-anchor" href="#结论与未来展望"><span>结论与未来展望</span></a></h2><p>Transformer 以全注意力机制彻底变革了序列建模范式，兼具高效并行与强大表达能力。其提出的多头注意力、位置编码等机制，极大推动了大模型和生成式 AI 的发展。论文写作极简、信息密度高，强调代码开源和行业影响。未来，Transformer 及其变体将在更广泛的多模态、长序列、低资源等场景持续演进，也为后续模型设计和论文写作提供了范式参考。</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h3><ol><li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">论文原文</a></li><li><a href="https://www.bilibili.com/video/BV1vJ411n7oT" target="_blank" rel="noopener noreferrer">李沐 Transformer 精读视频</a></li><li><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">The Illustrated Transformer（博客）</a></li><li><a href="https://www.alphaxiv.org/overview/1706.03762" target="_blank" rel="noopener noreferrer">alphaXiv 博客解读</a></li></ol>',52)]))}]]),i=JSON.parse('{"path":"/zh/posts/papers/transformer.html","title":"【论文精读】Attention Is All You Need：Transformer 架构详解","lang":"zh-CN","frontmatter":{"description":"【论文精读】Attention Is All You Need：Transformer 架构详解 摘要 Transformer 由 Vaswani 等人于 2017 年提出，首次完全基于自注意力机制（Self-Attention）实现序列建模，摒弃了 RNN/CNN 结构。该模型极大提升了并行效率和长距离依赖建模能力，成为 NLP、CV 等多模态领域的...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/papers/transformer.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"【论文精读】Attention Is All You Need：Transformer 架构详解"}],["meta",{"property":"og:description","content":"【论文精读】Attention Is All You Need：Transformer 架构详解 摘要 Transformer 由 Vaswani 等人于 2017 年提出，首次完全基于自注意力机制（Self-Attention）实现序列建模，摒弃了 RNN/CNN 结构。该模型极大提升了并行效率和长距离依赖建模能力，成为 NLP、CV 等多模态领域的..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-21.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"【论文精读】Attention Is All You Need：Transformer 架构详解\\",\\"image\\":[\\"https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-21.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"Transformer 由 Vaswani 等人于 2017 年提出，首次完全基于自注意力机制（Self-Attention）实现序列建模，摒弃了 RNN/CNN 结构。该模型极大提升了并行效率和长距离依赖建模能力，成为 NLP、CV 等多模态领域的基础架构。论文提出的多头注意力、位置编码等机制，推动了大模型和生成式 AI 的快速发展。","slug":"transformer-由-vaswani-等人于-2017-年提出-首次完全基于自注意力机制-self-attention-实现序列建模-摒弃了-rnn-cnn-结构。该模型极大提升了并行效率和长距离依赖建模能力-成为-nlp、cv-等多模态领域的基础架构。论文提出的多头注意力、位置编码等机制-推动了大模型和生成式-ai-的快速发展。","link":"#transformer-由-vaswani-等人于-2017-年提出-首次完全基于自注意力机制-self-attention-实现序列建模-摒弃了-rnn-cnn-结构。该模型极大提升了并行效率和长距离依赖建模能力-成为-nlp、cv-等多模态领域的基础架构。论文提出的多头注意力、位置编码等机制-推动了大模型和生成式-ai-的快速发展。","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"背景与研究目标","slug":"背景与研究目标","link":"#背景与研究目标","children":[]},{"level":2,"title":"方法与创新点","slug":"方法与创新点","link":"#方法与创新点","children":[{"level":3,"title":"整体流程与核心思想","slug":"整体流程与核心思想","link":"#整体流程与核心思想","children":[]},{"level":3,"title":"关键创新点总结","slug":"关键创新点总结","link":"#关键创新点总结","children":[]}]},{"level":2,"title":"实验与结果分析","slug":"实验与结果分析","link":"#实验与结果分析","children":[{"level":3,"title":"1. 训练设置与数据集","slug":"_1-训练设置与数据集","link":"#_1-训练设置与数据集","children":[]},{"level":3,"title":"2. 超参数与正则化","slug":"_2-超参数与正则化","link":"#_2-超参数与正则化","children":[]},{"level":3,"title":"3. 主要实验结果（表格对比）","slug":"_3-主要实验结果-表格对比","link":"#_3-主要实验结果-表格对比","children":[]},{"level":3,"title":"4. 消融实验（表 3）","slug":"_4-消融实验-表-3","link":"#_4-消融实验-表-3","children":[]},{"level":3,"title":"5. 长序列建模能力与复杂度分析（表 1）","slug":"_5-长序列建模能力与复杂度分析-表-1","link":"#_5-长序列建模能力与复杂度分析-表-1","children":[]},{"level":3,"title":"6. 泛化能力","slug":"_6-泛化能力","link":"#_6-泛化能力","children":[]}]},{"level":2,"title":"模型启发与方法延伸","slug":"模型启发与方法延伸","link":"#模型启发与方法延伸","children":[]},{"level":2,"title":"结论与未来展望","slug":"结论与未来展望","link":"#结论与未来展望","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":10.15,"words":3045},"filePathRelative":"zh/posts/papers/transformer.md","excerpt":"\\n<h2>摘要</h2>\\n<h2>Transformer 由 Vaswani 等人于 2017 年提出，首次完全基于自注意力机制（Self-Attention）实现序列建模，摒弃了 RNN/CNN 结构。该模型极大提升了并行效率和长距离依赖建模能力，成为 NLP、CV 等多模态领域的基础架构。论文提出的多头注意力、位置编码等机制，推动了大模型和生成式 AI 的快速发展。</h2>\\n<h2>目录</h2>\\n<ol>\\n<li><a href=\\"#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87\\">背景与研究目标</a></li>\\n<li><a href=\\"#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9\\">方法与创新点</a></li>\\n<li><a href=\\"#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90\\">实验与结果分析</a></li>\\n<li><a href=\\"#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8\\">模型启发与方法延伸</a></li>\\n<li><a href=\\"#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B\\">结论与未来展望</a></li>\\n</ol>","autoDesc":true}')}}]);