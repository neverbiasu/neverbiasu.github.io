"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[3475],{66262:(t,r)=>{r.A=(t,r)=>{const e=t.__vccOpts||t;for(const[t,n]of r)e[t]=n;return e}},95467:(t,r,e)=>{e.r(r),e.d(r,{comp:()=>a,data:()=>i});var n=e(20641);const o={},a=(0,e(66262).A)(o,[["render",function(t,r){return(0,n.uX)(),(0,n.CE)("div",null,r[0]||(r[0]=[(0,n.Fv)('<h1 id="intern-s1发布多模态基础模型-meta推出dinov3-微软开源poml【ai周报】" tabindex="-1"><a class="header-anchor" href="#intern-s1发布多模态基础模型-meta推出dinov3-微软开源poml【ai周报】"><span>Intern-S1发布多模态基础模型 | Meta推出DINOv3 | 微软开源POML【AI周报】</span></a></h1><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：InternLM 推出多模态大模型 Intern‑S1，提升感知、理解与生成；Meta 发布 DINOv3 推进自监督视觉；微软开源 POML；MobileAgent、Ovis、Thyme 等亦亮相，覆盖智能体与多模态应用。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#intern-s1%E7%A7%91%E5%AD%A6%E9%A2%86%E5%9F%9F%E5%BC%BA%E5%8C%96%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B">Intern‑S1：科学领域强化的多模态基础模型</a></li><li><a href="#mobileagent%E9%9D%A2%E5%90%91%E7%A7%BB%E5%8A%A8%E8%AE%BE%E5%A4%87%E7%9A%84%E9%80%9A%E7%94%A8%E5%A4%9A%E6%A8%A1%E6%80%81%E6%99%BA%E8%83%BD%E4%BD%93">MobileAgent：面向移动设备的通用多模态智能体</a></li><li><a href="#dinov3%E8%87%AA%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E6%96%B0%E8%8C%83%E5%BC%8F">DINOv3：自监督视觉表征学习新范式</a></li><li><a href="#ovis%E7%BB%9F%E4%B8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E4%B8%8E%E7%90%86%E8%A7%A3%E6%A8%A1%E5%9E%8B">Ovis：统一多模态视频生成与理解模型</a></li><li><a href="#thyme%E8%B7%A8%E6%A8%A1%E6%80%81%E6%97%B6%E9%97%B4%E6%84%9F%E7%9F%A5%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">Thyme：跨模态时间感知视觉语言模型</a></li><li><a href="#matrix-game-2%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%9A%E5%BC%88%E7%8E%AF%E5%A2%83">Matrix-Game 2：大规模多智能体博弈环境</a></li><li><a href="#poml%E8%B7%A8%E6%A8%A1%E6%80%81%E7%A8%8B%E5%BA%8F%E5%8C%96%E8%AF%AD%E8%A8%80">POML：跨模态程序化语言</a></li><li><a href="#colorctrl%E5%8F%AF%E6%8E%A7%E5%9B%BE%E5%83%8F%E9%A2%9C%E8%89%B2%E7%BC%96%E8%BE%91">ColorCtrl：可控图像颜色编辑</a></li></ol><hr><h2 id="intern‐s1-科学领域强化的多模态基础模型" tabindex="-1"><a class="header-anchor" href="#intern‐s1-科学领域强化的多模态基础模型"><span>Intern‑S1：科学领域强化的多模态基础模型</span></a></h2><figure><img src="https://arxiv.org/html/2508.15763v1/x4.png" alt="Intern‑S1 Architecture 图" tabindex="0" loading="lazy"><figcaption>Intern‑S1 Architecture 图</figcaption></figure><p><strong>概要</strong>：<strong>Intern‑S1</strong> 由 <strong>上海人工智能实验室（InternLM）</strong> 团队发布，定位为“面向科学”的多模态基础模型。论文披露其为 <strong>Mixture‑of‑Experts（MoE）</strong> 架构：总参数约 <strong>241B</strong>、激活参数 <strong>28B</strong>，在 <strong>5T</strong> token 上持续预训练（其中 <strong>2.5T</strong> 为科学域数据），并在 <strong>InternBootCamp</strong> 框架下进行离线+在线强化学习，提出 <strong>Mixture‑of‑Rewards（MoR）</strong> 同时对千余任务优化。官方评测显示，Intern‑S1 在通用推理保持开源一线水准，在专业科学任务（分子合成规划、反应条件预测、晶体热力学稳定性等）上显著超越开源模型，并在若干项目上逼近/超越闭源强 baseline。已提供多种推理与部署形态（如 GGUF、vLLM 支持）。</p><p><strong>标签</strong>：#多模态推理 #MoE架构 #科学计算 #强化学习对齐 #大规模预训练</p><hr><h2 id="mobileagent-面向移动设备的通用多模态智能体" tabindex="-1"><a class="header-anchor" href="#mobileagent-面向移动设备的通用多模态智能体"><span>MobileAgent：面向移动设备的通用多模态智能体</span></a></h2><p><img src="https://github.com/X-PLUG/MobileAgent/raw/main/assets/series.png" alt="MobileAgent Series 图" loading="lazy"><strong>概要</strong>：<strong>MobileAgent</strong> 由 <strong>清华大学与北京智源研究院</strong> 团队提出，是首个专注于 <strong>移动设备真实交互</strong> 的多模态智能体框架。该系统通过结合视觉、语音与文本输入，能够直接在智能手机上执行复杂操作，如应用控制、任务自动化与跨模态信息检索。论文中提出了 <strong>真实设备操作轨迹收集与模拟环境结合</strong> 的训练策略，大幅提升了在真实手机上的泛化能力。实验显示，MobileAgent 在跨应用多任务执行的成功率显著超越现有方法，为多模态大模型的 <strong>落地应用与人机交互</strong> 提供了可行路径。</p><p><strong>标签</strong>：#多模态智能体 #移动设备 #人机交互 #任务自动化 #真实环境</p><hr><h2 id="dinov3-自监督视觉表征学习新范式" tabindex="-1"><a class="header-anchor" href="#dinov3-自监督视觉表征学习新范式"><span>DINOv3：自监督视觉表征学习新范式</span></a></h2><figure><img src="https://scontent.fyyc6-1.fna.fbcdn.net/v/t39.2365-6/533218684_738909568994563_5324898431385079354_n.png?_nc_cat=103&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=BId3_-q_AW4Q7kNvwGNcw1a&amp;_nc_oc=AdlWr0DN3ri06Diz0d2DqsXuZ93A5ewj_FoQ6rqpsDQf5oF7qBIMHayVq8A_8ChFmcg&amp;_nc_zt=14&amp;_nc_ht=scontent.fyyc6-1.fna&amp;_nc_gid=QA94fzBTpvuEYJWsKiLvpQ&amp;oh=00_AfWAvd1ONy4dnleyvfclhEOqxzV1wEetOmdWbx60j-k1Bw&amp;oe=68C56861" alt="DINOv3 Benchmark 图" tabindex="0" loading="lazy"><figcaption>DINOv3 Benchmark 图</figcaption></figure><p><strong>概要</strong>：<strong>DINOv3</strong> 由 <strong>Meta AI Research</strong> 提出，是继 DINO 与 DINOv2 之后的最新一代自监督视觉模型。该版本在训练中引入 <strong>更高效的对比学习策略与掩码建模结合机制</strong>，在无需人工标注的情况下，学得更具泛化性的图像特征。DINOv3 在 <strong>ImageNet 分类、目标检测、语义分割</strong> 等多项任务中刷新自监督方法的性能基线，并在下游小样本学习与跨域迁移中展现出显著优势。研究进一步强调了大规模自监督学习在计算机视觉中的核心地位，为构建更强的 <strong>通用视觉基础模型</strong> 提供了新方向。</p><p><strong>标签</strong>：#自监督学习 #视觉表征 #对比学习 #掩码建模 #计算机视觉</p><hr><h2 id="ovis-统一多模态视频生成与理解模型" tabindex="-1"><a class="header-anchor" href="#ovis-统一多模态视频生成与理解模型"><span>Ovis：统一多模态视频生成与理解模型</span></a></h2><figure><img src="https://github.com/AIDC-AI/Ovis/raw/main/docs/Ovis25_arch.png" alt="Ovis Architecture 图" tabindex="0" loading="lazy"><figcaption>Ovis Architecture 图</figcaption></figure><p><strong>概要</strong>：<strong>Ovis</strong> 由 <strong>AIDC-AI 团队</strong> 提出，旨在构建一个统一的 <strong>视频生成与理解</strong> 框架。模型基于 <strong>扩散架构与跨模态对齐机制</strong>，不仅能进行高质量的视频生成，还能处理视频字幕、问答与编辑等理解类任务。Ovis 引入 <strong>跨模态一致性约束</strong>，确保视觉与文本之间的精准对齐，并在多种公开视频生成与理解基准上取得领先成绩。该工作展示了 <strong>统一架构驱动多模态任务</strong> 的潜力，为未来的 <strong>视频多模态大模型</strong> 奠定了基础。</p><p><strong>标签</strong>：#视频生成 #视频理解 #多模态模型 #扩散模型 #跨模态对齐</p><hr><h2 id="thyme-跨模态时间感知视觉语言模型" tabindex="-1"><a class="header-anchor" href="#thyme-跨模态时间感知视觉语言模型"><span>Thyme：跨模态时间感知视觉语言模型</span></a></h2><figure><img src="https://thyme-vl.github.io/static/images/method.png" alt="Thyme Pipeline 图" tabindex="0" loading="lazy"><figcaption>Thyme Pipeline 图</figcaption></figure><p><strong>概要</strong>：<strong>Thyme</strong> 由 <strong>复旦大学与上海人工智能实验室</strong> 提出，是首个显式建模 <strong>时间感知能力</strong> 的视觉语言模型（Vision-Language Model, VLM）。Thyme 在多模态 Transformer 中引入 <strong>时间编码与跨帧对齐机制</strong>，能够理解和推理跨帧动态信息，从而在视频问答、动作识别、事件理解等任务中表现优异。实验表明，Thyme 显著提升了 <strong>视频时序理解与跨模态推理</strong> 的性能，为构建具备 <strong>时序推理能力的多模态大模型</strong> 提供了新的解决方案。</p><p><strong>标签</strong>：#视觉语言模型 #时序建模 #视频理解 #跨模态推理 #多模态学习</p><hr><h2 id="matrix-game-2-大规模多智能体博弈环境" tabindex="-1"><a class="header-anchor" href="#matrix-game-2-大规模多智能体博弈环境"><span>Matrix-Game 2：大规模多智能体博弈环境</span></a></h2><figure><img src="https://matrix-game-v2.github.io/static/imgs/architecture.png" alt="Matrix-Game 2 Architecture 图" tabindex="0" loading="lazy"><figcaption>Matrix-Game 2 Architecture 图</figcaption></figure><p><strong>概要</strong>：<strong>Matrix-Game 2</strong> 由 <strong>SkyworkAI 团队</strong> 提出，是一个专为 <strong>多智能体博弈与强化学习</strong> 研究设计的大规模基准环境。该框架基于 <strong>矩阵博弈与可扩展环境构建</strong>，支持数十万智能体的对抗、协作与混合策略训练。Matrix-Game 2 提供了丰富的对局场景与高效并行机制，能够系统性评估智能体在 <strong>策略演化、合作机制与博弈均衡</strong> 中的表现。该工作推动了 <strong>大规模多智能体系统研究</strong>，为通用人工智能中的 <strong>群体智能探索</strong> 提供了坚实平台。</p><p><strong>标签</strong>：#多智能体 #博弈论 #强化学习 #群体智能 #AI基准</p><hr><h2 id="poml-跨模态程序化语言" tabindex="-1"><a class="header-anchor" href="#poml-跨模态程序化语言"><span>POML：跨模态程序化语言</span></a></h2><figure><img src="https://microsoft.github.io/poml/latest/media/vscode-test.png" alt="POML Test 图" tabindex="0" loading="lazy"><figcaption>POML Test 图</figcaption></figure><p><strong>概要</strong>：<strong>POML（Procedural Object Markup Language）</strong> 由 <strong>微软研究院</strong> 提出，是一种用于 <strong>跨模态场景建模与生成</strong> 的新型语言框架。POML 结合 <strong>程序化描述与多模态表示</strong>，可支持图像、视频、三维场景等内容的统一生成与编辑。通过与大模型结合，POML 能够高效完成 <strong>视觉生成、交互模拟与可控编辑</strong>，并在虚拟现实、游戏设计及多模态 AI 研究中展现出强大潜力。该工作为 <strong>结构化多模态生成</strong> 提供了通用接口，推动了 AI 在 <strong>内容创作与可控生成</strong> 方向的发展。</p><p><strong>标签</strong>：#多模态 #程序化生成 #场景建模 #内容创作 #可控生成</p><hr><h2 id="colorctrl-可控图像颜色编辑" tabindex="-1"><a class="header-anchor" href="#colorctrl-可控图像颜色编辑"><span>ColorCtrl：可控图像颜色编辑</span></a></h2><figure><img src="https://arxiv.org/html/2508.09131v2/x1.png" alt="ColorCtrl Teaser 图" tabindex="0" loading="lazy"><figcaption>ColorCtrl Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>ColorCtrl</strong> 由 <strong>香港中文大学与微软亚洲研究院</strong> 合作提出，旨在实现 <strong>高精度的可控图像颜色编辑</strong>。该方法结合 <strong>扩散模型与颜色感知控制机制</strong>，能够在不破坏原始结构与细节的前提下，对图像中的 <strong>局部或整体颜色</strong> 进行自然修改。ColorCtrl 支持多种交互方式，包括 <strong>文本驱动、调色板约束与区域指定</strong>，并在设计、艺术创作与图像修复等任务中表现出色。该研究为 <strong>可控图像生成与编辑</strong> 提供了新思路，提升了模型的灵活性与用户友好性。</p><p><strong>标签</strong>：#图像编辑 #颜色控制 #扩散模型 #用户交互 #视觉生成</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://github.com/InternLM/Intern-S1" target="_blank" rel="noopener noreferrer">Intern-S1 Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.15763v1" target="_blank" rel="noopener noreferrer">Intern-S1 论文</a></li><li><a href="https://github.com/X-PLUG/MobileAgent" target="_blank" rel="noopener noreferrer">MobileAgent Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.15144" target="_blank" rel="noopener noreferrer">MobileAgent 论文</a></li><li><a href="https://ai.meta.com/blog/dinov3-self-supervised-vision-model/" target="_blank" rel="noopener noreferrer">DINOv3 Meta AI 博客</a></li><li><a href="https://github.com/facebookresearch/dinov3" target="_blank" rel="noopener noreferrer">DINOv3 Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.10104" target="_blank" rel="noopener noreferrer">DINOv3 论文</a></li><li><a href="https://github.com/AIDC-AI/Ovis" target="_blank" rel="noopener noreferrer">Ovis Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.11737" target="_blank" rel="noopener noreferrer">Ovis 论文</a></li><li><a href="https://thyme-vl.github.io/" target="_blank" rel="noopener noreferrer">Thyme 项目主页</a></li><li><a href="https://github.com/yfzhang114/Thyme" target="_blank" rel="noopener noreferrer">Thyme Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.11630" target="_blank" rel="noopener noreferrer">Thyme 论文</a></li><li><a href="https://matrix-game-v2.github.io/" target="_blank" rel="noopener noreferrer">Matrix-Game 2 项目主页</a></li><li><a href="https://github.com/SkyworkAI/Matrix-Game/tree/main/Matrix-Game-2" target="_blank" rel="noopener noreferrer">Matrix-Game 2 Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.13009" target="_blank" rel="noopener noreferrer">Matrix-Game 2 论文</a></li><li><a href="https://microsoft.github.io/poml/latest/" target="_blank" rel="noopener noreferrer">POML 项目主页</a></li><li><a href="https://github.com/microsoft/poml" target="_blank" rel="noopener noreferrer">POML Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.09131" target="_blank" rel="noopener noreferrer">POML 论文</a></li><li><a href="https://zxyin.github.io/ColorCtrl/" target="_blank" rel="noopener noreferrer">ColorCtrl 项目主页</a></li><li><a href="https://arxiv.org/html/2508.09131" target="_blank" rel="noopener noreferrer">ColorCtrl 论文</a></li></ol>',48)]))}]]),i=JSON.parse('{"path":"/zh/posts/ai-weekly/052.html","title":"Intern-S1发布多模态基础模型 | Meta推出DINOv3 | 微软开源POML【AI周报】","lang":"zh-CN","frontmatter":{"description":"Intern-S1发布多模态基础模型 | Meta推出DINOv3 | 微软开源POML【AI周报】 摘要 本周亮点：InternLM 推出多模态大模型 Intern‑S1，提升感知、理解与生成；Meta 发布 DINOv3 推进自监督视觉；微软开源 POML；MobileAgent、Ovis、Thyme 等亦亮相，覆盖智能体与多模态应用。 目录 In...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/052.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"Intern-S1发布多模态基础模型 | Meta推出DINOv3 | 微软开源POML【AI周报】"}],["meta",{"property":"og:description","content":"Intern-S1发布多模态基础模型 | Meta推出DINOv3 | 微软开源POML【AI周报】 摘要 本周亮点：InternLM 推出多模态大模型 Intern‑S1，提升感知、理解与生成；Meta 发布 DINOv3 推进自监督视觉；微软开源 POML；MobileAgent、Ovis、Thyme 等亦亮相，覆盖智能体与多模态应用。 目录 In..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://arxiv.org/html/2508.15763v1/x4.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Intern-S1发布多模态基础模型 | Meta推出DINOv3 | 微软开源POML【AI周报】\\",\\"image\\":[\\"https://arxiv.org/html/2508.15763v1/x4.png\\",\\"https://github.com/X-PLUG/MobileAgent/raw/main/assets/series.png\\",\\"https://scontent.fyyc6-1.fna.fbcdn.net/v/t39.2365-6/533218684_738909568994563_5324898431385079354_n.png?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=BId3_-q_AW4Q7kNvwGNcw1a&_nc_oc=AdlWr0DN3ri06Diz0d2DqsXuZ93A5ewj_FoQ6rqpsDQf5oF7qBIMHayVq8A_8ChFmcg&_nc_zt=14&_nc_ht=scontent.fyyc6-1.fna&_nc_gid=QA94fzBTpvuEYJWsKiLvpQ&oh=00_AfWAvd1ONy4dnleyvfclhEOqxzV1wEetOmdWbx60j-k1Bw&oe=68C56861\\",\\"https://github.com/AIDC-AI/Ovis/raw/main/docs/Ovis25_arch.png\\",\\"https://thyme-vl.github.io/static/images/method.png\\",\\"https://matrix-game-v2.github.io/static/imgs/architecture.png\\",\\"https://microsoft.github.io/poml/latest/media/vscode-test.png\\",\\"https://arxiv.org/html/2508.09131v2/x1.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"Intern‑S1：科学领域强化的多模态基础模型","slug":"intern‐s1-科学领域强化的多模态基础模型","link":"#intern‐s1-科学领域强化的多模态基础模型","children":[]},{"level":2,"title":"MobileAgent：面向移动设备的通用多模态智能体","slug":"mobileagent-面向移动设备的通用多模态智能体","link":"#mobileagent-面向移动设备的通用多模态智能体","children":[]},{"level":2,"title":"DINOv3：自监督视觉表征学习新范式","slug":"dinov3-自监督视觉表征学习新范式","link":"#dinov3-自监督视觉表征学习新范式","children":[]},{"level":2,"title":"Ovis：统一多模态视频生成与理解模型","slug":"ovis-统一多模态视频生成与理解模型","link":"#ovis-统一多模态视频生成与理解模型","children":[]},{"level":2,"title":"Thyme：跨模态时间感知视觉语言模型","slug":"thyme-跨模态时间感知视觉语言模型","link":"#thyme-跨模态时间感知视觉语言模型","children":[]},{"level":2,"title":"Matrix-Game 2：大规模多智能体博弈环境","slug":"matrix-game-2-大规模多智能体博弈环境","link":"#matrix-game-2-大规模多智能体博弈环境","children":[]},{"level":2,"title":"POML：跨模态程序化语言","slug":"poml-跨模态程序化语言","link":"#poml-跨模态程序化语言","children":[]},{"level":2,"title":"ColorCtrl：可控图像颜色编辑","slug":"colorctrl-可控图像颜色编辑","link":"#colorctrl-可控图像颜色编辑","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":7.37,"words":2210},"filePathRelative":"zh/posts/ai-weekly/052.md","excerpt":"\\n<h2>摘要</h2>\\n<p>本周亮点：InternLM 推出多模态大模型 Intern‑S1，提升感知、理解与生成；Meta 发布 DINOv3 推进自监督视觉；微软开源 POML；MobileAgent、Ovis、Thyme 等亦亮相，覆盖智能体与多模态应用。</p>\\n<hr>\\n<h2>目录</h2>\\n<ol>\\n<li><a href=\\"#intern-s1%E7%A7%91%E5%AD%A6%E9%A2%86%E5%9F%9F%E5%BC%BA%E5%8C%96%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B\\">Intern‑S1：科学领域强化的多模态基础模型</a></li>\\n<li><a href=\\"#mobileagent%E9%9D%A2%E5%90%91%E7%A7%BB%E5%8A%A8%E8%AE%BE%E5%A4%87%E7%9A%84%E9%80%9A%E7%94%A8%E5%A4%9A%E6%A8%A1%E6%80%81%E6%99%BA%E8%83%BD%E4%BD%93\\">MobileAgent：面向移动设备的通用多模态智能体</a></li>\\n<li><a href=\\"#dinov3%E8%87%AA%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E6%96%B0%E8%8C%83%E5%BC%8F\\">DINOv3：自监督视觉表征学习新范式</a></li>\\n<li><a href=\\"#ovis%E7%BB%9F%E4%B8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E4%B8%8E%E7%90%86%E8%A7%A3%E6%A8%A1%E5%9E%8B\\">Ovis：统一多模态视频生成与理解模型</a></li>\\n<li><a href=\\"#thyme%E8%B7%A8%E6%A8%A1%E6%80%81%E6%97%B6%E9%97%B4%E6%84%9F%E7%9F%A5%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B\\">Thyme：跨模态时间感知视觉语言模型</a></li>\\n<li><a href=\\"#matrix-game-2%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%9A%E5%BC%88%E7%8E%AF%E5%A2%83\\">Matrix-Game 2：大规模多智能体博弈环境</a></li>\\n<li><a href=\\"#poml%E8%B7%A8%E6%A8%A1%E6%80%81%E7%A8%8B%E5%BA%8F%E5%8C%96%E8%AF%AD%E8%A8%80\\">POML：跨模态程序化语言</a></li>\\n<li><a href=\\"#colorctrl%E5%8F%AF%E6%8E%A7%E5%9B%BE%E5%83%8F%E9%A2%9C%E8%89%B2%E7%BC%96%E8%BE%91\\">ColorCtrl：可控图像颜色编辑</a></li>\\n</ol>","autoDesc":true}')}}]);