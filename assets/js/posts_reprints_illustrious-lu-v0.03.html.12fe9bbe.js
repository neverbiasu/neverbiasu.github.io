"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[843],{6262:(e,t)=>{t.A=(e,t)=>{const a=e.__vccOpts||e;for(const[e,i]of t)a[e]=i;return a}},9179:(e,t,a)=>{a.r(t),a.d(t,{comp:()=>n,data:()=>s});var i=a(641);const o={},n=(0,a(6262).A)(o,[["render",function(e,t){return(0,i.uX)(),(0,i.CE)("div",null,t[0]||(t[0]=[(0,i.Fv)('<h1 id="illustrious-lu-v0-03" tabindex="-1"><a class="header-anchor" href="#illustrious-lu-v0-03"><span>Illustrious-LU v0.03</span></a></h1><p>SD XL has been suffering from CLIP – I think this is true, at least partially. Recent models have shown some potential related to natural language, like understanding &quot;left is red, right is blue&quot;. However, since CLIP was not trained with natural language sentences, base SD XL and its finetuned variants were significantly limited regarding processing it.</p><p>Flux and SD3, like other DiT models, have shown better capabilities with T5. Especially, it has been shown that T5 is very important in processing natural language information to correctly generate text or compositions. However, T5 is very big and still limited, so there have been attempts to use LLMs directly as text encoders. Also, the DiT models have been very big. Even without T5, 12B models are not really feasible enough, much like SD XL.</p><p>Flow matching is also interesting. However, it seems inevitable that DiT&#39;s intuitive structure enabled much useful research. Unfortunately, flow matching with SD XL didn&#39;t show evidence supporting this; rather, it raised more questions about SD XL&#39;s VAE problems.</p><p>So, the minimal requirements were:</p><ol><li>A good-sized backbone, 2-6B parameters without TE</li><li>A small text encoder, perhaps not T5</li><li>RoPE implementation (unlike some specific models)</li><li>A good VAE – like the Flux VAE</li></ol><p>The interesting models to test were:</p><ol><li>Infinity: An 8B model with VAR Lumina 2.0</li><li>Lumina: A 2B model with an LLM + DiT backbone</li><li>Flux: A 12B model – very standard and appealing, but large</li></ol><p>In January 2025, I decided to leave Flux finetuning to a talented researcher and tested Infinity while preparing Lumina. As a result, Flux-Chroma was developed, which may also be an interesting base to consider. The Infinity experiment showed that &quot;the compute is not enough&quot;. Unfortunately, training an 8B scale model requires at least months with an H100 node. In fact, training on 8 million images for 10 epochs would require months to train the model. I&#39;d definitely like to train those 8B models, but perhaps not now.</p><p>So Lumina was prepared for training – with numerous fixes and tests. In fact, it was likely only trained with high-quality photographs, which is not suitable for illustrations. But if it is undertrained, we can potentially fix it. We can check if it can be further &quot;trained&quot; with a dataset and whether it can absorb new knowledge. However, whether we need to train the LLM part required testing.</p><p>Illustrious-Lumina is a test-train model with the following objectives:</p><ol><li>Can the model be trained without the LLM part for better understanding of characters/knowledge?</li><li>Does the model adapt faster than SD XL?</li><li>Is the resolution arbitrary?</li></ol><p>Here, I showcase the result of Illustrious-lumina-experimental-v0.03, trained for 22M image samples. Please note that the original model&#39;s performance regarding illustration is &quot;severely bad&quot;.</p><p>For example, it does not understand any well-known characters.</p><p>However, after training – we can see that the model understands shirakami fubuki, Misaka Mikoto, or some popular characters.</p><figure><img src="https://illustrious-prod.s3.ap-northeast-2.amazonaws.com/blog/2025-04-18T09:43:42.451Z/2025-04-14(1,2).png" alt="Image 4" tabindex="0" loading="lazy"><figcaption>Image 4</figcaption></figure><figure><img src="https://illustrious-prod.s3.ap-northeast-2.amazonaws.com/blog/2025-04-18T09:41:47.722Z/2025-04-14(3,4).png" alt="Image 5" tabindex="0" loading="lazy"><figcaption>Image 5</figcaption></figure><p>Surprisingly, with just 22M images seen – it can understand 2.5k sample character, saten ruiko or shirai Kuroko too.</p><figure><img src="https://illustrious-prod.s3.ap-northeast-2.amazonaws.com/blog/2025-04-18T09:41:55.453Z/2025-04-14(5,6).png" alt="Image 6" tabindex="0" loading="lazy"><figcaption>Image 6</figcaption></figure><p>Not accurate as previous ones, however distinctly fast.</p><p>Finally, we can see that actually, three famous popular characters are also working-</p><figure><img src="https://illustrious-prod.s3.ap-northeast-2.amazonaws.com/blog/2025-04-18T09:42:05.180Z/2025-04-14(7,8).png" alt="Image 7" tabindex="0" loading="lazy"><figcaption>Image 7</figcaption></figure><p>The 26500 step, 768 batch size training has shown the successful result – however, it is 15% of v0.1 – at least 8x of current compute amount, is expected to be required for training.</p><p>The model has several limitations, and requires improvements.</p><p>It includes some synthetic examples, specific style tests such as pixel arts, and post-training with high quality images.</p><p>Also, the promised text generation capability, were not found – it requires some sophisticated dataset based training too.</p><p>The training journey is currently stopped – I am focusing on dataset cleanup &amp; code fixes for demo first. The model and the inference demo code – with improved setup, will be released soon.</p>',27)]))}]]),s=JSON.parse('{"path":"/posts/reprints/illustrious-lu-v0.03.html","title":"Illustrious-LU v0.03","lang":"en-US","frontmatter":{"title":"Illustrious-LU v0.03","icon":"fa-solid:microscope","cover":"https://illustrious-prod.s3.ap-northeast-2.amazonaws.com/blog/2025-04-18T09:43:42.451Z/2025-04-14(1,2).png","date":"2025-04-18T00:00:00.000Z","category":["Model Development","reprint"],"tag":["Illustrious","LU","Lumina","AI Model","Image Generation","Training"],"author":"Angelbottomless","footer":"Reprinted from Illustrious Tech Blog","copyright":"reprint","description":"Illustrious-LU v0.03 SD XL has been suffering from CLIP – I think this is true, at least partially. Recent models have shown some potential related to natural language, like und...","gitInclude":[],"head":[["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://neverbiasu.github.io/zh/posts/reprints/illustrious-lu-v0.03.html"}],["meta",{"property":"og:url","content":"https://neverbiasu.github.io/posts/reprints/illustrious-lu-v0.03.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"Illustrious-LU v0.03"}],["meta",{"property":"og:description","content":"Illustrious-LU v0.03 SD XL has been suffering from CLIP – I think this is true, at least partially. Recent models have shown some potential related to natural language, like und..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://illustrious-prod.s3.ap-northeast-2.amazonaws.com/blog/2025-04-18T09:43:42.451Z/2025-04-14(1,2).png"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://illustrious-prod.s3.ap-northeast-2.amazonaws.com/blog/2025-04-18T09:43:42.451Z/2025-04-14(1,2).png"}],["meta",{"name":"twitter:image:alt","content":"Illustrious-LU v0.03"}],["meta",{"property":"article:author","content":"Angelbottomless"}],["meta",{"property":"article:tag","content":"Illustrious"}],["meta",{"property":"article:tag","content":"LU"}],["meta",{"property":"article:tag","content":"Lumina"}],["meta",{"property":"article:tag","content":"AI Model"}],["meta",{"property":"article:tag","content":"Image Generation"}],["meta",{"property":"article:tag","content":"Training"}],["meta",{"property":"article:published_time","content":"2025-04-18T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Illustrious-LU v0.03\\",\\"image\\":[\\"https://illustrious-prod.s3.ap-northeast-2.amazonaws.com/blog/2025-04-18T09:43:42.451Z/2025-04-14(1,2\\",\\"https://illustrious-prod.s3.ap-northeast-2.amazonaws.com/blog/2025-04-18T09:41:47.722Z/2025-04-14(3,4\\",\\"https://illustrious-prod.s3.ap-northeast-2.amazonaws.com/blog/2025-04-18T09:41:55.453Z/2025-04-14(5,6\\",\\"https://illustrious-prod.s3.ap-northeast-2.amazonaws.com/blog/2025-04-18T09:42:05.180Z/2025-04-14(7,8\\"],\\"datePublished\\":\\"2025-04-18T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Angelbottomless\\"}]}"]]},"headers":[],"readingTime":{"minutes":2.55,"words":766},"filePathRelative":"posts/reprints/illustrious-lu-v0.03.md","localizedDate":"April 18, 2025","excerpt":"\\n<p>SD XL has been suffering from CLIP – I think this is true, at least partially. Recent models have shown some potential related to natural language, like understanding \\"left is red, right is blue\\". However, since CLIP was not trained with natural language sentences, base SD XL and its finetuned variants were significantly limited regarding processing it.</p>","autoDesc":true}')}}]);