"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[2296],{6262:(e,t)=>{t.A=(e,t)=>{const i=e.__vccOpts||e;for(const[e,s]of t)i[e]=s;return i}},9134:(e,t,i)=>{i.r(t),i.d(t,{comp:()=>r,data:()=>o});var s=i(641);const a={},r=(0,i(6262).A)(a,[["render",function(e,t){return(0,s.uX)(),(0,s.CE)("div",null,t[0]||(t[0]=[(0,s.Fv)('<h1 id="illustrious-xl-3-0-3-5-vpred-2048-resolution-and-natural-language" tabindex="-1"><a class="header-anchor" href="#illustrious-xl-3-0-3-5-vpred-2048-resolution-and-natural-language"><span>Illustrious XL 3.0-3.5-vpred: 2048 Resolution and Natural Language</span></a></h1><p>Illustrious XL 3.0–3.5-vpred represents a major advancement in Stable Diffusion XL (SD XL) modeling, notably supporting resolutions ranging seamlessly from 256 up to 2048. The v3.5-vpred variant particularly emphasizes robust natural language understanding capabilities, comparable in sophistication to miniaturized large language models (LLMs), achieved through extensive simultaneous training of both CLIP and UNet components.</p><h2 id="training-objectives-and-summary-eps-vs-vpred" tabindex="-1"><a class="header-anchor" href="#training-objectives-and-summary-eps-vs-vpred"><span>Training Objectives and Summary: eps vs. vpred</span></a></h2><p>The Illustrious v3.0-v3.5 series was designed with two distinct training objectives to explore behavioral differences:</p><ul><li><p><strong>V3.0-epsilon</strong> employs epsilon-prediction (noise prediction), establishing itself as a stable &quot;base&quot; model suitable for future training tasks, notably demonstrating compatibility with LoRA training. The model shows distinct, stylish outputs compared to vpred variants at default - sometimes it is best model with certain aesthetic scoring.</p></li><li><p><strong>V3.0-vpred</strong> utilizes velocity-prediction (v-parameterization), demonstrating improved compositional understanding yet initially plagued by significant issues, including catastrophic forgetting, domain shifts, oversaturated colors, and collapsed color palettes due to flawed zero terminal SNR implementation.</p></li><li><p><strong>V3.5-vpred</strong> is trained with experimental setups, to mitigate the mentioned problems. The model has shown significantly more stable colors, however does not naturally generate vibrant colors. The functionality has moved to certain controlling tokens.</p></li></ul><p>Unfortunately, the v-parameterization variants have certain problems regard to finetuning / PEFT, which I consider as very critical feature for Illustrious series. The thoughtful tests are ongoing regard to the model to resolve the method for stable finetuning. I will include the current hypothesis regarding to the phenomenon.</p><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/0.png" alt="Single and 4-grid result from v3.0vpred model" tabindex="0" loading="lazy"><figcaption>Single and 4-grid result from v3.0vpred model</figcaption></figure><p><img src="/assets/images/reprints/illustrious/v3.0-3.5/1.png" alt="Single and 4-grid result from v3.0vpred model" loading="lazy"><em>58.46% of the pixels are &#39;exactly white, #FFFFFF&#39;. This is an abnormal result, which shows how v-parameterization objective models can collapse.</em></p><h2 id="technical-challenges-in-v-parameterization" tabindex="-1"><a class="header-anchor" href="#technical-challenges-in-v-parameterization"><span>Technical Challenges in V-Parameterization</span></a></h2><p>The primary issue with v-parameterization involved incorrect variance estimation attributed to zero terminal SNR. This miscalculation caused the model to excessively remove noise early on, overestimating the clarity of images prematurely, thus resulting in saturated or unrealistic colors that persisted throughout denoising steps.</p><p>Despite these challenges, the v-parameterization brought substantial compositional improvements. For example, the model successfully interpreted complex directional prompts (e.g., &quot;left is black, right is red&quot;), a significant achievement previously unattainable with epsilon-based models.</p><p>(Epsilon models, however, has shown significant improvements on compositions which can&#39;t be explained with just randomness. See below).</p><h2 id="the-comparative-tests—color-separation-really-complex-prompts" tabindex="-1"><a class="header-anchor" href="#the-comparative-tests—color-separation-really-complex-prompts"><span>The Comparative Tests—Color Separation, Really Complex Prompts</span></a></h2><p>I showcase here the following three distinct setups:</p><ol><li><strong>Simple Prompt</strong>: Simple enough (&lt;75 tokens) prompts are given, and model is asked for inference.</li><li><strong>Complex Prompt (424 tokens)</strong>: The really long, complicated, and &#39;paraphrased&#39; sentences are given to confuse the model.</li><li><strong>FFT with simple prompts</strong>: The model intermediate latents are decoded and analyzed with Fast Fourier Transform, to analyze low frequency signal changes.</li></ol><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/2.png" alt="v3.0-vpred&#39;s simple prompt inference result" tabindex="0" loading="lazy"><figcaption>v3.0-vpred&#39;s simple prompt inference result</figcaption></figure><p><em>The model is partially following the prompt, &quot;There are two girls in the image. The girl in the left is red hair with sea-black eyes. The girl in the right is blue hair, and firey ruby eyes&quot; with high success rate, however exposes oversaturation in colors.</em></p><p>The simple prompt based inference result, also shows the interesting results in v3.0 models.</p><p>v3.0 Epsilon also shows significant improvement, which is still not explained well by myself.</p><blockquote><p>&quot;Two girls are depicted in the image, the left side is red hair girl with yellow eyes, blue skirt. The right side is blue-toned silver hair girl with yellow earring, blue pupils and silver dress.&quot; With , Sampler: Euler, Schedule type: Normal, CFG scale: 7.5.</p></blockquote><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/3.png" alt="v3.0-vpred&#39;s inference result" tabindex="0" loading="lazy"><figcaption>v3.0-vpred&#39;s inference result</figcaption></figure><blockquote><p>v3.0-epsilon inference result. surprisingly, epsilon model sometimes works well, here 23/25 success.</p></blockquote><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/4.png" alt="v3.0-vpred&#39;s inference result" tabindex="0" loading="lazy"><figcaption>v3.0-vpred&#39;s inference result</figcaption></figure><blockquote><p>v3.0-vpred’s inference result. The model shows oversaturated colors at default.</p></blockquote><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/5.png" alt="v3.5-vpred inference result" tabindex="0" loading="lazy"><figcaption>v3.5-vpred inference result</figcaption></figure><blockquote><p>v3.5-vpred’s inference result. The model shows mid-range colors with separation.</p></blockquote><h2 id="complex-prompt-inference-result" tabindex="-1"><a class="header-anchor" href="#complex-prompt-inference-result"><span>Complex Prompt Inference Result</span></a></h2><p>Here is the prompt used for inference:</p><blockquote><p>A breathtaking scene unfolds with two girls standing together, a striking contrast in both appearance and demeanor. The girl on the left has flowing, deep red hair, cascading in soft waves down her shoulders, with strands illuminated by the ambient light. Her eyes are a mesmerizing shade of sea-black, like the ocean at midnight, deep and endless, reflecting a quiet mystery. She has a gentle, thoughtful expression, her lips slightly parted as if about to speak, her gaze filled with an unspoken story. She wears an elegant, dark-blue dress with silver embroidery resembling delicate ocean waves, flowing like liquid silk around her slender frame. The gentle breeze lifts the fabric slightly, giving her an ethereal presence.</p><p>Beside her, the girl on the right is a vivid contrast—her hair is a brilliant shade of blue, short yet tousled with an electric vibrancy, strands catching the light like crystalline ice. Her fiery ruby-red eyes burn with intensity, gleaming with an almost supernatural glow. Her sharp, confident smirk reveals her bold personality, a spirit unafraid of the world. She is clad in a striking ensemble—black with intricate red patterns resembling flickering flames, an outfit that exudes strength and energy. The contrast of her fiery eyes against her cool-toned hair creates an almost otherworldly effect, as if she embodies the elements themselves.</p><p>The setting complements their striking presence—perhaps a twilight sky over a vast ocean, where the last remnants of the sun cast a golden glow across the waves, mixing with the incoming darkness. The wind rustles through their hair as they stand close, an unspoken connection between them. Or maybe they are in a futuristic cityscape, neon lights reflecting in their eyes, the bustling energy of the world around them barely touching the silent understanding they share. Their postures mirror their personalities—one calm, introspective, a quiet storm; the other bold, fiery, a force of nature.</p><p>The image captures the balance between them—opposites yet intertwined, like fire and water, night and day. Their expressions, their outfits, and the atmosphere all tell a story waiting to be unraveled. masterpiece</p></blockquote><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/6.png" alt="inference result from v3.0-vpred" tabindex="0" loading="lazy"><figcaption>inference result from v3.0-vpred</figcaption></figure><blockquote><p>The inference result from v3.0-vpred. The model was not able to generate stable outputs, and exposed oversaturation.</p></blockquote><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/7.png" alt="inference result from community-driven v-parameterization model" tabindex="0" loading="lazy"><figcaption>inference result from community-driven v-parameterization model</figcaption></figure><blockquote><p>The inference result from community-driven v-parameterization model. The model also exposed oversaturation, and was unable to follow the prompts.</p></blockquote><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/8.png" alt="results from other popular epsilon prediction models" tabindex="0" loading="lazy"><figcaption>results from other popular epsilon prediction models</figcaption></figure><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/9.png" alt="results from other popular epsilon prediction models" tabindex="0" loading="lazy"><figcaption>results from other popular epsilon prediction models</figcaption></figure><blockquote><p>Generation results from other popular epsilon prediction models. The prompt exposed certain problem on the models.</p></blockquote><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/10.png" alt="result from v3.0-epsilon" tabindex="0" loading="lazy"><figcaption>result from v3.0-epsilon</figcaption></figure><blockquote><p>Generation result from v3.0-epsilon. The model was able to generate somehow, the stylish result among all the results with more stability.</p></blockquote><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/11.png" alt="result from Flux.1-Schnell" tabindex="0" loading="lazy"><figcaption>result from Flux.1-Schnell</figcaption></figure><blockquote><p>Generation result from Flux.1-Schnell. The model has followed the prompt distinctly well with lack of stylization.</p></blockquote><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/12.png" alt="result from Illustrious-v3.5-vpred" tabindex="0" loading="lazy"><figcaption>result from Illustrious-v3.5-vpred</figcaption></figure><blockquote><p>Generation result from Illustrious-v3.5-vpred. The model has shown significantly stable generation results similar to Flux somehow, <strong>however lost stylish generation</strong> unlike v3.0-epsilon. I note here that Flux’s prompt following is “distinctly good and robust” whilst v3.5-vpred’s prompt following “can be limited” - it highly depends on the given prompt format.</p></blockquote><h2 id="denoising-pattern-analysis" tabindex="-1"><a class="header-anchor" href="#denoising-pattern-analysis"><span>Denoising Pattern Analysis</span></a></h2><p>All inference used <code>empty black background</code> prompt with DDIM 10 steps.</p><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/13.png" alt="v3.0-epsilon" tabindex="0" loading="lazy"><figcaption>v3.0-epsilon</figcaption></figure><blockquote><p>v3.0-epsilon</p></blockquote><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/14.png" alt="v3.0-vpred" tabindex="0" loading="lazy"><figcaption>v3.0-vpred</figcaption></figure><blockquote><p>v3.0-vpred</p></blockquote><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/15.png" alt="v3.5-vpred" tabindex="0" loading="lazy"><figcaption>v3.5-vpred</figcaption></figure><blockquote><p>v3.5-vpred</p></blockquote><h2 id="mathematical-insights-and-adjustments" tabindex="-1"><a class="header-anchor" href="#mathematical-insights-and-adjustments"><span>Mathematical Insights and Adjustments</span></a></h2><p>Post-analysis, mathematical adjustments to variance calculations stabilized the vpred training process, significantly enhancing prompt comprehension. The epsilon model, conversely, retained issues with accurately following complex conditioning due to an overreliance on initial noise sampling, despite the ability to generate the stylized outputs.</p><p>The &quot;golden noise&quot; phenomenon referenced from recent research underscored the importance of good initial noise conditions for optimal model performance, reinforcing the superiority of vpred methods in complex prompt scenarios.</p><p>In easy words, epsilon model follows the low frequency features, and cannot overcome the color domain, which causes anatomy / bad initial conditioning afterwards.</p><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/13.png" alt="epsilon model" tabindex="0" loading="lazy"><figcaption>epsilon model</figcaption></figure><p>Whileas, v-parameterization models can overcome this problem with high noise removal in initial steps.</p><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/16.png" alt="v-parameterization model" tabindex="0" loading="lazy"><figcaption>v-parameterization model</figcaption></figure><p>In fact, the zero terminal SNR / or v-parameterization makes &quot;first initial step very strong&quot; but not for other steps. After the first step - the colors are mostly &#39;fixed&#39;, - low frequency features are &#39;fixed&#39;, so model does not expect to change main colors after that.</p><p>However, the &quot;snr estimation&quot; -or variances are &#39;errored&#39; - the colors are reverted one step.</p><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/17.png" alt="without CFG" tabindex="0" loading="lazy"><figcaption>without CFG</figcaption></figure><p>The phenomenon is complicated - this appears regardless of samplers, schedulers, or even with / without zero terminal SNR in inference.</p><p>Without CFG, the model shows some better visualization - the &quot;mid range steps&quot; - they are actually trying to generate artificial high frequency features!</p><p>Since model is NOT extensively trained with &quot;empty images&quot; - as a result, the model natively assumes that &quot;some reasonable high frequency feature should be placed&quot;, same to epsilon objective. This is the bias.</p><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/18.png" alt="img2img process" tabindex="0" loading="lazy"><figcaption>img2img process</figcaption></figure><p>The img2img process, also shows notable behavior : the overshooting latents shows significantly &#39;inverted&#39; latent colors after initial step. This means that &quot;somehow model learnt to fix overshooting&quot; - I might have some idea to validate &amp; improve this phenomenon even further, currently planning some more setups. The initial thought, is to test samplers &amp; schedulers, which would make sense to use the initial steps behavior and exponentially decrease- which would make sense.</p><p>I will introduce Illustrious v4 - when I am prepared mathematically, to solve the proposed overshooting problem.</p><h2 id="dataset-insights-and-color-control" tabindex="-1"><a class="header-anchor" href="#dataset-insights-and-color-control"><span>Dataset Insights and Color Control</span></a></h2><p>The correct control of colors in v-parameterization variants, however exposes other problem-</p><p>The model would follow the average color of dataset. It is wrong to expect model to generate vibrant colors naturally without dataset being dominantly vibrant.</p><p>But, in fact, the &quot;variant colored&quot; images are less than 10% - the model naturally does NOT show the vibrant colors.</p><p>Here is simple color analysis:</p><p>We can call the image is &quot;colorful&quot; when its saturation is above 120, but with saturation stdev &lt; 50 (variable colors saturation). In 4.4M image sample analysis, the result is not surprising - most of the images, are not &quot;colorful&quot; or &#39;saturated&#39; - then how would model learn the &quot;vibrant colors&quot;?</p><p><img src="/assets/images/reprints/illustrious/v3.0-3.5/18.png" alt="colorfulness metric" loading="lazy"><em>Very small portion of the dataset have &#39;vibrant or colorful&#39; images.</em></p><p>How would we control colors if most of the image are not vibrant, without sacrificing muted color - not making model biased toward certain colors?</p><p>The answer was, doing some more analysis on the datasets &amp; introducing controlling tokens.</p><p>Here, we introduce the &quot;control tokens&quot; - explicitly tagged by image analysis:</p><p><strong>Contrast</strong>: &#39;low contrast&#39;, &#39;medium contrast&#39;, &#39;high contrast&#39;, &#39;very high contrast&#39;<br><strong>Brightness</strong>: &#39;dark&#39;, &#39;normal brightness&#39;, &#39;bright&#39;, &#39;very bright&#39;<br><strong>Sharpness</strong>: &#39;blurry&#39;, &#39;slightly sharp&#39;, &#39;sharp&#39;, &#39;very sharp&#39;<br><strong>Dynamic Colors</strong>: &#39;static colors&#39;, &#39;medium dynamic colors&#39;, &#39;high dynamic colors&#39;, &#39;very dynamic colors&#39;<br><strong>Colorfulness</strong>: &#39;monotonic color&#39;, &#39;medium colorfulness&#39;, &#39;high colorfulness&#39;, &#39;very high colorfulness&#39;<br><strong>Saturation</strong>: &#39;muted colors&#39;, &#39;average colors&#39;, &#39;vibrant colors&#39;, &#39;very vibrant colors&#39;</p><p>The control token, works specifically in v3.0-epsilon and v3.5-vpred model, however, v3.0-vpred model may not work well the token. However, the visual result may be &quot;appealing&quot; due to vibrant colors - you might like the result when proper prompt was used with the model.</p><p>The idea is quite simple, however some cases are not well-handled, there are extremely small amount of &quot;very dark&quot; or &quot;very white&quot; images, this is near opposite of statements which claims &quot;model requires #000000 or #FFFFFF&quot;.</p><p>Also, common phrase, &quot;dark&quot; is contaminated - &quot;black theme&quot; might be better for usecases.</p><p>Unfortunately, I didn&#39;t have time to spend on finding which region contains those HSV-exact colors - will do some analysis on it.</p><h2 id="the-academical-naming-convention" tabindex="-1"><a class="header-anchor" href="#the-academical-naming-convention"><span>The Academical Naming Convention</span></a></h2><p>Here is the result from v3.0-epsilon, and v3.5-vpred:</p><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/19.png" alt="result from v3.0-epsilon" tabindex="0" loading="lazy"><figcaption>result from v3.0-epsilon</figcaption></figure><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/20.png" alt="result from v3.5-vpred" tabindex="0" loading="lazy"><figcaption>result from v3.5-vpred</figcaption></figure><p>With this 2 results, you might think &quot;Oh the first one is v3.5-vpred and second one is v3.0-epsilon&quot; — however its not! the first one is v3.0-epsilon, the second one is v3.5-vpred.</p><p>The namings, are purely academical, especially the &quot;3&quot; means it is &quot;3rd academical breakthrough&quot;.</p><p>Some prompts - may obviously work better on certain versions.</p><p>The demo will be offered as soon as possible, so you would be able to compare &amp; experiment soon.</p><h2 id="lora-training-is-suffering" tabindex="-1"><a class="header-anchor" href="#lora-training-is-suffering"><span>LoRA Training Is Suffering</span></a></h2><p>The LoRA training on v-parameterization models did not go well - this is also well known on v0.1-vpred model, and noobai vpred models, and flow based models as well. One hypothesis is - the low frequency features were not supposed to be &#39;trained&#39; with limited dataset - it is far more fragile than high frequency features. Since v-parameterization models upsample the importance of low snr steps, they naturally also make model exposed to low frequency features - however, this causes drastic collapse of denoising kernels. Since LoRA training dataset is uncomparably small, it is fragile and nearly impossible to prevent.</p><p>One ongoing idea is to prevent this with sampling timesteps with weighting instead - since model seems to be more affected by low snr steps. and adding some robustness - I might have to check this as well. I hope I can showcase the good results very soon.</p><h2 id="ongoing-research-and-future-directions" tabindex="-1"><a class="header-anchor" href="#ongoing-research-and-future-directions"><span>Ongoing Research and Future Directions</span></a></h2><p>The Lumina-2.0-Illustrious (name can be changed), is currently being trained by OnomaAI&#39;s support, despite within limited budget.</p><p>Here are some samples, I&#39;d say the model is currently &#39;20% toward v0.1 level&#39; - We spent several thousand dollars again on the training with various trial and errors. The model is currently being carefully tested for estimating total compute budget, but after we cover the certain budget, the training can be accelerated. We promise the model to be open sourced right after being prepared, which would foster the new ecosystem.</p><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/21.png" alt="example image 1" tabindex="0" loading="lazy"><figcaption>example image 1</figcaption></figure><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/22.png" alt="example image 2" tabindex="0" loading="lazy"><figcaption>example image 2</figcaption></figure><figure><img src="/assets/images/reprints/illustrious/v3.0-3.5/23.png" alt="example image 3" tabindex="0" loading="lazy"><figcaption>example image 3</figcaption></figure><p>In theory, when the training is successfully done, it will be similar-sized natural language understanding DiT model.</p><p>Unlike flux, this would cost far less budget for LoRA training - and in worst case, it can work as smallest illustration &quot;draft&quot; generator, which robustly understands the natural language prompting for various setups &amp; poses. In best case scenario, we would be able to generate text-in-image one shot!</p><p>Flux is good - but obviously training compute is big, and model itself is big. This was also why we were supporting who tries to train / reduce the Flux model&#39;s size within budget- this was secret, but it was disclosed now. We will definitely continue to contribute to open source, maybe secretly or publicly. Well, yes, we tried and opened about SD3.5 model finetuning too. You might be already watching some results.</p><p>I&#39;m preparing a lot of stuff to be disclosed soon. See you later!</p>',102)]))}]]),o=JSON.parse('{"path":"/posts/reprints/illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.html","title":"Illustrious XL 3.0-3.5-vpred: 2048 Resolution and Natural Language","lang":"en-US","frontmatter":{"title":"Illustrious XL 3.0-3.5-vpred: 2048 Resolution and Natural Language","icon":"mdi:paint-outline","cover":"/assets/images/reprints/illustrious/v3.0-3.5/thumbnail.webp","date":"2025-03-22T00:00:00.000Z","category":["Model Development","Model Training","Image Generation","Anime Style"],"tag":["SDXL","2048 Resolution","Illustrious","vpred","epsilon prediction"],"author":"Angelbottomless","description":"Illustrious XL 3.0-3.5-vpred represents a major advancement in Stable Diffusion XL modeling, supporting resolutions up to 2048 with enhanced natural language understanding.","head":[["meta",{"name":"keywords","content":"SDXL, high resolution generation, vpred, epsilon, natural language understanding"}],["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://neverbiasu.github.io/zh/posts/reprints/illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.html"}],["meta",{"property":"og:url","content":"https://neverbiasu.github.io/posts/reprints/illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"Illustrious XL 3.0-3.5-vpred: 2048 Resolution and Natural Language"}],["meta",{"property":"og:description","content":"Illustrious XL 3.0-3.5-vpred represents a major advancement in Stable Diffusion XL modeling, supporting resolutions up to 2048 with enhanced natural language understanding."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/thumbnail.webp"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/thumbnail.webp"}],["meta",{"name":"twitter:image:alt","content":"Illustrious XL 3.0-3.5-vpred: 2048 Resolution and Natural Language"}],["meta",{"property":"article:author","content":"Angelbottomless"}],["meta",{"property":"article:tag","content":"SDXL"}],["meta",{"property":"article:tag","content":"2048 Resolution"}],["meta",{"property":"article:tag","content":"Illustrious"}],["meta",{"property":"article:tag","content":"vpred"}],["meta",{"property":"article:tag","content":"epsilon prediction"}],["meta",{"property":"article:published_time","content":"2025-03-22T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Illustrious XL 3.0-3.5-vpred: 2048 Resolution and Natural Language\\",\\"image\\":[\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/0.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/1.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/2.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/3.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/4.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/5.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/6.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/7.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/8.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/9.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/10.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/11.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/12.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/13.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/14.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/15.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/13.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/16.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/17.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/18.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/18.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/19.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/20.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/21.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/22.png\\",\\"https://neverbiasu.github.io/assets/images/reprints/illustrious/v3.0-3.5/23.png\\"],\\"datePublished\\":\\"2025-03-22T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Angelbottomless\\"}]}"]],"gitInclude":[]},"headers":[{"level":2,"title":"Training Objectives and Summary: eps vs. vpred","slug":"training-objectives-and-summary-eps-vs-vpred","link":"#training-objectives-and-summary-eps-vs-vpred","children":[]},{"level":2,"title":"Technical Challenges in V-Parameterization","slug":"technical-challenges-in-v-parameterization","link":"#technical-challenges-in-v-parameterization","children":[]},{"level":2,"title":"The Comparative Tests—Color Separation, Really Complex Prompts","slug":"the-comparative-tests—color-separation-really-complex-prompts","link":"#the-comparative-tests—color-separation-really-complex-prompts","children":[]},{"level":2,"title":"Complex Prompt Inference Result","slug":"complex-prompt-inference-result","link":"#complex-prompt-inference-result","children":[]},{"level":2,"title":"Denoising Pattern Analysis","slug":"denoising-pattern-analysis","link":"#denoising-pattern-analysis","children":[]},{"level":2,"title":"Mathematical Insights and Adjustments","slug":"mathematical-insights-and-adjustments","link":"#mathematical-insights-and-adjustments","children":[]},{"level":2,"title":"Dataset Insights and Color Control","slug":"dataset-insights-and-color-control","link":"#dataset-insights-and-color-control","children":[]},{"level":2,"title":"The Academical Naming Convention","slug":"the-academical-naming-convention","link":"#the-academical-naming-convention","children":[]},{"level":2,"title":"LoRA Training Is Suffering","slug":"lora-training-is-suffering","link":"#lora-training-is-suffering","children":[]},{"level":2,"title":"Ongoing Research and Future Directions","slug":"ongoing-research-and-future-directions","link":"#ongoing-research-and-future-directions","children":[]}],"readingTime":{"minutes":8.56,"words":2568},"filePathRelative":"posts/reprints/illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.md","localizedDate":"March 22, 2025","excerpt":"\\n<p>Illustrious XL 3.0–3.5-vpred represents a major advancement in Stable Diffusion XL (SD XL) modeling, notably supporting resolutions ranging seamlessly from 256 up to 2048. The v3.5-vpred variant particularly emphasizes robust natural language understanding capabilities, comparable in sophistication to miniaturized large language models (LLMs), achieved through extensive simultaneous training of both CLIP and UNet components.</p>"}')}}]);