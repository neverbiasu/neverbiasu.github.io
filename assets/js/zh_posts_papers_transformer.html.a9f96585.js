"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[5822],{66262:(t,e)=>{e.A=(t,e)=>{const r=t.__vccOpts||t;for(const[t,a]of e)r[t]=a;return r}},53637:(t,e,r)=>{r.r(e),r.d(e,{comp:()=>i,data:()=>l});var a=r(20641);const n={},i=(0,r(66262).A)(n,[["render",function(t,e){return(0,a.uX)(),(0,a.CE)("div",null,e[0]||(e[0]=[(0,a.Fv)('<h1 id="【论文精读】transformer-attention-is-all-you-need" tabindex="-1"><a class="header-anchor" href="#【论文精读】transformer-attention-is-all-you-need"><span>【论文精读】Transformer：Attention Is All You Need</span></a></h1><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>Transformer 由 Vaswani 等人于 2017 年提出，首次完全基于自注意力机制（Self-Attention）实现序列建模，摒弃了 RNN/CNN 结构。该模型极大提升了并行效率和长距离依赖建模能力，成为 NLP、CV 等多模态领域的基础架构。论文提出的多头注意力、位置编码等机制，推动了大模型和生成式 AI 的快速发展。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li><li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li><li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li><li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li></ol><hr><h2 id="背景与研究目标" tabindex="-1"><a class="header-anchor" href="#背景与研究目标"><span>背景与研究目标</span></a></h2><ul><li><strong>领域背景</strong>：序列建模任务（如机器翻译、文本生成）传统依赖 RNN/CNN，存在并行效率低、长距离依赖难捕捉等问题。</li><li><strong>核心问题</strong>：如何提升序列建模的效率与表达能力，突破 RNN 的时序瓶颈。</li><li><strong>研究目标</strong>：提出一种完全基于注意力机制的模型，实现高效、可扩展的序列建模。</li><li><strong>论文特点</strong>：作者排序随机，贡献明细透明，代码开源。标题&quot;Attention is all you need&quot;成为学术梗，对后续研究影响深远。</li></ul><hr><h2 id="方法与创新点" tabindex="-1"><a class="header-anchor" href="#方法与创新点"><span>方法与创新点</span></a></h2><figure><img src="https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-21.png" alt="Transformer 整体架构" tabindex="0" loading="lazy"><figcaption>Transformer 整体架构</figcaption></figure><h3 id="整体流程与核心思想" tabindex="-1"><a class="header-anchor" href="#整体流程与核心思想"><span>整体流程与核心思想</span></a></h3><p>Transformer 采用编码器（Encoder）和解码器（Decoder）结构，每个模块由 6 个相同层堆叠而成。每层包含自注意力机制和前馈网络。<strong>核心优势：每个 token 能直接与序列中所有 token 建立联系，实现全局依赖建模。</strong></p><figure><img src="https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png" alt="Encoder-Decoder 架构" tabindex="0" loading="lazy"><figcaption>Encoder-Decoder 架构</figcaption></figure><h4 id="_1-输入嵌入与位置编码" tabindex="-1"><a class="header-anchor" href="#_1-输入嵌入与位置编码"><span>1. 输入嵌入与位置编码</span></a></h4><ul><li><strong>词嵌入</strong>：将输入 token 映射为 512 维向量，编码器和解码器共享权重。</li><li><strong>位置编码</strong>：用正弦/余弦函数编码位置信息，与词嵌入相加： $$ \\text{PE}<em>{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d</em>{model}}}\\right),\\quad \\text{PE}<em>{(pos,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d</em>{model}}}\\right) $$</li></ul><h4 id="_2-自注意力机制-self-attention" tabindex="-1"><a class="header-anchor" href="#_2-自注意力机制-self-attention"><span>2. 自注意力机制（Self-Attention）</span></a></h4><ul><li><strong>核心原理</strong>：每个 token 通过 Query、Key、Value 三组向量与所有 token 计算相关性并加权聚合信息。</li><li><strong>计算流程</strong>： <ol><li>生成 Q、K、V：输入向量分别乘以权重矩阵 $W<sup>Q$、$W</sup>K$、$W^V$</li><li>计算注意力分数：Q 与 K 的点积，再除以 $\\sqrt{d_k}$ 缩放</li><li>应用 Softmax 获得权重</li><li>加权求和：权重与 V 相乘得到输出</li></ol></li></ul><figure><img src="https://jalammar.github.io/images/t/self-attention-output.png" alt="Self-Attention 计算过程" tabindex="0" loading="lazy"><figcaption>Self-Attention 计算过程</figcaption></figure><ul><li><p><strong>公式表示</strong>： $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$</p></li><li><p><strong>三种应用场景</strong>：</p><ul><li>Encoder Self-Attention：编码器内部，Q、K、V 均来自上一层输出</li><li>Decoder Masked Self-Attention：解码器内部，添加掩码防止看到未来信息</li><li>Encoder-Decoder Attention：解码器中，Q 来自解码器，K、V 来自编码器输出</li></ul></li></ul><h4 id="_3-多头注意力-multi-head-attention" tabindex="-1"><a class="header-anchor" href="#_3-多头注意力-multi-head-attention"><span>3. 多头注意力（Multi-Head Attention）</span></a></h4><ul><li><strong>机制</strong>：将注意力机制并行计算 8 次，每次使用不同的线性投影。</li><li><strong>优势</strong>：不同头可以关注不同的特征模式，类似 CNN 多通道。</li></ul><figure><img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt="多头注意力机制" tabindex="0" loading="lazy"><figcaption>多头注意力机制</figcaption></figure><ul><li><strong>公式</strong>： $$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O $$ 其中 $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$</li></ul><h4 id="_4-前馈网络与残差连接" tabindex="-1"><a class="header-anchor" href="#_4-前馈网络与残差连接"><span>4. 前馈网络与残差连接</span></a></h4><ul><li><p><strong>前馈网络</strong>：每个位置独立的两层全连接网络，中间使用 ReLU 激活函数： $$ \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2 $$</p></li><li><p><strong>残差连接与层归一化</strong>：每个子层输出加上输入，再应用 LayerNorm，表示为 <code>LayerNorm(x + Sublayer(x))</code>。</p></li></ul><h4 id="_5-解码过程" tabindex="-1"><a class="header-anchor" href="#_5-解码过程"><span>5. 解码过程</span></a></h4><ul><li><strong>自回归生成</strong>：解码器每次生成一个 token，将已生成序列作为输入。</li><li><strong>Mask 机制</strong>：在解码器的自注意力层使用掩码，防止看到未来信息。</li></ul><figure><img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif" alt="解码过程" tabindex="0" loading="lazy"><figcaption>解码过程</figcaption></figure><h3 id="关键创新点总结" tabindex="-1"><a class="header-anchor" href="#关键创新点总结"><span>关键创新点总结</span></a></h3><table><thead><tr><th>创新点</th><th>原理/机制</th><th>优势与影响</th></tr></thead><tbody><tr><td>自注意力机制</td><td>计算全局 token 间相关性</td><td>并行高效，捕捉长距离依赖</td></tr><tr><td>多头注意力</td><td>多组注意力并行计算</td><td>丰富特征表达，提升建模能力</td></tr><tr><td>位置编码</td><td>正弦/余弦函数编码位置信息</td><td>无需递归即可建模序列顺序</td></tr><tr><td>全注意力替代RNN</td><td>摒弃递归与卷积，纯注意力架构</td><td>极大提升并行效率，易于扩展</td></tr><tr><td>残差+层归一化</td><td>每层后加残差连接与 LayerNorm</td><td>稳定训练，缓解梯度消失</td></tr></tbody></table><hr><h2 id="实验与结果分析" tabindex="-1"><a class="header-anchor" href="#实验与结果分析"><span>实验与结果分析</span></a></h2><h3 id="_1-训练设置与超参数" tabindex="-1"><a class="header-anchor" href="#_1-训练设置与超参数"><span>1. 训练设置与超参数</span></a></h3><ul><li><strong>数据集</strong>：WMT 2014 英德（450万句对）、英法（3600万句对）</li><li><strong>模型规模</strong>： <ul><li><strong>Base</strong>：6层，$d_{model}=512$，$d_{ff}=2048$，8头，$d_k=d_v=64$</li><li><strong>Big</strong>：6层，$d_{model}=1024$，$d_{ff}=4096$，16头</li></ul></li><li><strong>训练细节</strong>： <ul><li>硬件：8块 P100 GPU</li><li>Base模型：12小时（100k步）</li><li>Big模型：3.5天（300k步）</li><li>优化器：Adam + 自适应学习率调度</li><li>正则化：Dropout (0.1~0.3)、Label Smoothing (0.1)</li></ul></li></ul><h3 id="_2-主要实验结果" tabindex="-1"><a class="header-anchor" href="#_2-主要实验结果"><span>2. 主要实验结果</span></a></h3><table><thead><tr><th>Model</th><th>EN-DE BLEU</th><th>EN-FR BLEU</th><th>训练成本 (FLOPs)</th></tr></thead><tbody><tr><td>GNMT + RL</td><td>24.6</td><td>39.9</td><td>$2.3 \\times 10^{19}$</td></tr><tr><td>ConvS2S</td><td>25.2</td><td>40.5</td><td>$9.6 \\times 10^{18}$</td></tr><tr><td>Transformer (Base)</td><td>27.3</td><td>38.1</td><td>$3.3 \\times 10^{18}$</td></tr><tr><td>Transformer (Big)</td><td><strong>28.4</strong></td><td><strong>41.8</strong></td><td>$2.3 \\times 10^{19}$</td></tr></tbody></table><p><strong>结论</strong>：Transformer 在翻译任务上超越当时所有模型，且训练成本更低。</p><h3 id="_3-消融实验-表-3" tabindex="-1"><a class="header-anchor" href="#_3-消融实验-表-3"><span>3. 消融实验（表 3）</span></a></h3><table><thead><tr><th>变体/超参</th><th>BLEU 变化</th><th>说明</th></tr></thead><tbody><tr><td>单头 attention</td><td>-0.9</td><td>多头更优，头数过多无益</td></tr><tr><td>前馈层宽度减小</td><td>-</td><td>表现下降</td></tr><tr><td>去除位置编码</td><td>大幅下降</td><td>位置编码不可或缺</td></tr><tr><td>去除残差/LayerNorm</td><td>不收敛</td><td>训练极不稳定</td></tr><tr><td>Dropout 调整</td><td>-</td><td>适当 dropout 防止过拟合</td></tr><tr><td>learned pos embed</td><td>≈</td><td>与 sin/cos 编码效果相近</td></tr></tbody></table><h3 id="_4-长序列建模效率分析" tabindex="-1"><a class="header-anchor" href="#_4-长序列建模效率分析"><span>4. 长序列建模效率分析</span></a></h3><table><thead><tr><th>层类型</th><th>复杂度/层</th><th>顺序操作数</th><th>最大路径长度</th></tr></thead><tbody><tr><td>Self-Attention</td><td>$O(n^2 d)$</td><td>$O(1)$</td><td>$O(1)$</td></tr><tr><td>RNN</td><td>$O(n d^2)$</td><td>$O(n)$</td><td>$O(n)$</td></tr><tr><td>CNN</td><td>$O(k n d^2)$</td><td>$O(1)$</td><td>$O(\\log_k n)$</td></tr><tr><td>限制Self-Attn</td><td>$O(r n d)$</td><td>$O(1)$</td><td>$O(n/r)$</td></tr></tbody></table><p><strong>分析</strong>：Self-Attention 并行度高（$O(1)$ 顺序操作），路径最短（$O(1)$），适合长距离依赖建模，但复杂度随序列长度二次增长。</p><h3 id="_5-注意力可视化分析" tabindex="-1"><a class="header-anchor" href="#_5-注意力可视化分析"><span>5. 注意力可视化分析</span></a></h3><p>不同注意力头学习不同的语言特征模式，如句法结构、指代关系等。</p><figure><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization.png" alt="注意力头模式可视化" tabindex="0" loading="lazy"><figcaption>注意力头模式可视化</figcaption></figure><hr><h2 id="模型启发与方法延伸" tabindex="-1"><a class="header-anchor" href="#模型启发与方法延伸"><span>模型启发与方法延伸</span></a></h2><ul><li><strong>通用性</strong>：已成为 NLP、CV、音频等多模态领域的基础架构</li><li><strong>衍生模型</strong>：BERT、GPT、ViT 等都基于 Transformer 发展而来</li><li><strong>工程优势</strong>：结构简单，参数少，易扩展，调参简单</li><li><strong>局限性</strong>：归纳偏置弱，需要大数据支撑；计算复杂度随序列长度二次增长</li></ul><hr><h2 id="结论与未来展望" tabindex="-1"><a class="header-anchor" href="#结论与未来展望"><span>结论与未来展望</span></a></h2><p>Transformer 架构彻底变革了序列建模范式，其高效并行性和强大表达能力推动了大模型和生成式 AI 的发展。该架构简洁而强大，设计理念影响深远。未来 Transformer 将在多模态、长序列处理等方向持续演进，也为后续模型设计提供了范式参考。</p><hr><h2 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h2><ol><li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">论文原文</a></li><li><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">The Illustrated Transformer</a></li><li><a href="https://www.bilibili.com/video/BV1vJ411n7oT" target="_blank" rel="noopener noreferrer">李沐 Transformer 精读视频</a></li><li><a href="https://www.alphaxiv.org/overview/1706.03762" target="_blank" rel="noopener noreferrer">alphaXiv 博客解读</a></li></ol><hr><blockquote><p><strong>阅读提示</strong></p><ul><li>结合 <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">The Illustrated Transformer</a> 的可视化理解注意力机制</li><li>思考问题：为什么 Transformer 能取代 RNN？位置编码如何影响模型性能？多头注意力的直观意义是什么？</li><li>尝试从头实现简化版 Transformer，加深对核心机制的理解</li></ul></blockquote>',58)]))}]]),l=JSON.parse('{"path":"/zh/posts/papers/transformer.html","title":"【论文精读】Transformer：Attention Is All You Need","lang":"zh-CN","frontmatter":{"description":"【论文精读】Transformer：Attention Is All You Need 摘要 Transformer 由 Vaswani 等人于 2017 年提出，首次完全基于自注意力机制（Self-Attention）实现序列建模，摒弃了 RNN/CNN 结构。该模型极大提升了并行效率和长距离依赖建模能力，成为 NLP、CV 等多模态领域的基础架构。...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/papers/transformer.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"【论文精读】Transformer：Attention Is All You Need"}],["meta",{"property":"og:description","content":"【论文精读】Transformer：Attention Is All You Need 摘要 Transformer 由 Vaswani 等人于 2017 年提出，首次完全基于自注意力机制（Self-Attention）实现序列建模，摒弃了 RNN/CNN 结构。该模型极大提升了并行效率和长距离依赖建模能力，成为 NLP、CV 等多模态领域的基础架构。..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-21.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"【论文精读】Transformer：Attention Is All You Need\\",\\"image\\":[\\"https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-21.png\\",\\"https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png\\",\\"https://jalammar.github.io/images/t/self-attention-output.png\\",\\"https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\\",\\"https://jalammar.github.io/images/t/transformer_decoding_1.gif\\",\\"https://jalammar.github.io/images/t/transformer_self-attention_visualization.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"背景与研究目标","slug":"背景与研究目标","link":"#背景与研究目标","children":[]},{"level":2,"title":"方法与创新点","slug":"方法与创新点","link":"#方法与创新点","children":[{"level":3,"title":"整体流程与核心思想","slug":"整体流程与核心思想","link":"#整体流程与核心思想","children":[]},{"level":3,"title":"关键创新点总结","slug":"关键创新点总结","link":"#关键创新点总结","children":[]}]},{"level":2,"title":"实验与结果分析","slug":"实验与结果分析","link":"#实验与结果分析","children":[{"level":3,"title":"1. 训练设置与超参数","slug":"_1-训练设置与超参数","link":"#_1-训练设置与超参数","children":[]},{"level":3,"title":"2. 主要实验结果","slug":"_2-主要实验结果","link":"#_2-主要实验结果","children":[]},{"level":3,"title":"3. 消融实验（表 3）","slug":"_3-消融实验-表-3","link":"#_3-消融实验-表-3","children":[]},{"level":3,"title":"4. 长序列建模效率分析","slug":"_4-长序列建模效率分析","link":"#_4-长序列建模效率分析","children":[]},{"level":3,"title":"5. 注意力可视化分析","slug":"_5-注意力可视化分析","link":"#_5-注意力可视化分析","children":[]}]},{"level":2,"title":"模型启发与方法延伸","slug":"模型启发与方法延伸","link":"#模型启发与方法延伸","children":[]},{"level":2,"title":"结论与未来展望","slug":"结论与未来展望","link":"#结论与未来展望","children":[]},{"level":2,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}],"readingTime":{"minutes":6.29,"words":1887},"filePathRelative":"zh/posts/papers/transformer.md","excerpt":"\\n<h2>摘要</h2>\\n<p>Transformer 由 Vaswani 等人于 2017 年提出，首次完全基于自注意力机制（Self-Attention）实现序列建模，摒弃了 RNN/CNN 结构。该模型极大提升了并行效率和长距离依赖建模能力，成为 NLP、CV 等多模态领域的基础架构。论文提出的多头注意力、位置编码等机制，推动了大模型和生成式 AI 的快速发展。</p>\\n<hr>\\n<h2>目录</h2>\\n<ol>\\n<li><a href=\\"#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87\\">背景与研究目标</a></li>\\n<li><a href=\\"#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9\\">方法与创新点</a></li>\\n<li><a href=\\"#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90\\">实验与结果分析</a></li>\\n<li><a href=\\"#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8\\">模型启发与方法延伸</a></li>\\n<li><a href=\\"#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B\\">结论与未来展望</a></li>\\n</ol>","autoDesc":true}')}}]);