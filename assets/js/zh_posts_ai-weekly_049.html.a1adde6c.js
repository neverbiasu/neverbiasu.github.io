"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[2507],{66262:(e,r)=>{r.A=(e,r)=>{const n=e.__vccOpts||e;for(const[e,a]of r)n[e]=a;return n}},3915:(e,r,n)=>{n.r(r),n.d(r,{comp:()=>i,data:()=>o});var a=n(20641);const t={},i=(0,n(66262).A)(t,[["render",function(e,r){return(0,a.uX)(),(0,a.CE)("div",null,r[0]||(r[0]=[(0,a.Fv)('<h1 id="x‐omni融合生成与理解-hunyuanworld‐1-0构建可交互3d世界-screencoder自动图像转代码【ai周报】" tabindex="-1"><a class="header-anchor" href="#x‐omni融合生成与理解-hunyuanworld‐1-0构建可交互3d世界-screencoder自动图像转代码【ai周报】"><span>X‑Omni融合生成与理解|HunyuanWorld‑1.0构建可交互3D世界|ScreenCoder自动图像转代码【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg" alt="封面图源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面图源自C站作者Koal2</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：X‑Omni利用强化学习驱动多模态理解与高质量图像生成统一；HunyuanWorld‑1.0从图像或文本构建可交互的3D世界；ScreenCoder构建多智能体UI编码系统，实现图像到代码自动转换。详见正文，相关参考链接请见文末。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#x%E2%80%91omni%E4%BD%BF%E7%94%A8%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%8F%90%E5%8D%87%E7%A6%BB%E6%95%A3%E8%87%AA%E5%9B%9E%E5%BD%92%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E8%B4%A8%E9%87%8F%E7%9A%84%E7%BB%9F%E4%B8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B">X‑Omni：强化学习驱动的统一多模态自回归图像生成</a></li><li><a href="#hunyuanworld-10%E4%BB%8E%E6%96%87%E6%9C%AC%E6%88%96%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E5%8F%AF%E6%8E%A2%E7%B4%A2%E5%8F%AF%E4%BA%A4%E4%BA%92%E7%9A%84%E6%B2%89%E6%B5%B8%E5%BC%8F-3d-%E4%B8%96%E7%95%8C">HunyuanWorld 1.0：文本/图像生成可交互沉浸式 3D 世界</a></li><li><a href="#arc-hunyuan%E2%80%91video%E2%80%917b7b-%E6%A8%A1%E5%9E%8B%E9%A9%B1%E5%8A%A8%E7%9A%84%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E7%BB%9F%E4%B8%80%E6%A1%86%E6%9E%B6">ARC-Hunyuan‑Video‑7B：视频理解与生成统一框架</a></li><li><a href="#gpt%E2%80%91image%E2%80%91edit%E2%80%9115m%E7%99%BE%E4%B8%87%E7%BA%A7-gpt-%E7%94%9F%E6%88%90%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91%E4%B8%89%E5%85%83%E7%BB%84%E6%95%B0%E6%8D%AE%E9%9B%86">GPT‑Image‑Edit‑1.5M：百万级 GPT 生成图像编辑三元组数据集</a></li><li><a href="#screencoder%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E9%A9%B1%E5%8A%A8%E7%9A%84%E8%A7%86%E8%A7%89%E5%88%B0%E5%89%8D%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90%E7%B3%BB%E7%BB%9F">ScreenCoder：多智能体驱动的视觉到前端代码生成系统</a></li><li><a href="#alphavae%E7%AB%AF%E5%88%B0%E7%AB%AF-rgba-%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E4%B8%8E%E9%87%8D%E6%9E%84%E7%BB%9F%E4%B8%80%E6%A8%A1%E5%9E%8B">AlphaVAE：端到端 RGBA 图像生成与重构统一模型</a></li></ol><hr><h2 id="x‐omni-使用强化学习提升离散自回归图像生成质量的统一多模态模型" tabindex="-1"><a class="header-anchor" href="#x‐omni-使用强化学习提升离散自回归图像生成质量的统一多模态模型"><span>X‑Omni：使用强化学习提升离散自回归图像生成质量的统一多模态模型</span></a></h2><p><img src="https://x-omni-team.github.io/static/images/fig1-1.png" alt="X‑Omni Teaser 图" loading="lazy"><strong>概要</strong>：<strong>X‑Omni</strong> 是腾讯混元团队提出的首个基于强化学习的统一多模态模型，通过引入语义图像 tokenizer 与离散自回归机制，实现图像与文本的一体化生成与理解。团队使用强化学习优化 SFT 模型，使得生成图像质量、美学评分与长文本渲染能力均超过其他开源统一模型，同时准确遵循复杂指令并在任意分辨率下生成高保真图像。此模型已在 GitHub 和 Hugging Face 上发布，并附带 LongText‑Bench 基准与推理代码供社区使用。</p><p><strong>标签</strong>：#自回归图像生成 #强化学习 #多模态统一 #长文本渲染 #复杂指令控制</p><hr><h2 id="hunyuanworld-1-0-从文本或图像生成可探索、可交互的沉浸式-3d-世界" tabindex="-1"><a class="header-anchor" href="#hunyuanworld-1-0-从文本或图像生成可探索、可交互的沉浸式-3d-世界"><span>HunyuanWorld 1.0：从文本或图像生成可探索、可交互的沉浸式 3D 世界</span></a></h2><p><img src="https://3d-models.hunyuan.tencent.com/world/assets/webp/archi-CCIB2cwa.webp" alt="HunyuanWorld 1.0 Architecture 图" loading="lazy"><strong>概要</strong>：<strong>HunyuanWorld 1.0</strong> 是<strong>腾讯混元团队</strong>发布的首个支持模拟交互的 3D 世界生成系统，能够通过文本或单张图像构建具有语义分层与可导出的 3D mesh 世界。它使用全景图像作为生成代理，将场景拆解为语义层、再通过分层结构生成可探索的模型，兼顾内容一致性与渲染效率。系统支持虚拟现实、游戏开发与物理模拟应用，在多个基准上优于现有模型，并具备真实世界可应用能力。</p><p><strong>标签</strong>：#3D世界生成 #全景代理 #语义分层 #互动模拟 #游戏与VR支持</p><hr><h2 id="arc-hunyuan‐video‐7b-7b-模型驱动的视频理解与生成统一框架" tabindex="-1"><a class="header-anchor" href="#arc-hunyuan‐video‐7b-7b-模型驱动的视频理解与生成统一框架"><span>ARC-Hunyuan‑Video‑7B：7B 模型驱动的视频理解与生成统一框架</span></a></h2><p><img src="https://tencentarc.github.io/images/arc_video_method.jpg" alt="ARC‑Hunyuan‑Video‑7B Architecture 图" loading="lazy"><strong>概要</strong>：<strong>ARC‑Hunyuan‑Video‑7B</strong> 是<strong>腾讯ARC团队</strong>开发的一款多模态语言模型，专注将视频理解、文本生成与视频生成任务整合至统一框架。该模型采用大规模视频文本对齐数据训练，并通过 ARC-VL 编码器和视频生成 decoder 模块协同工作，实现从视频帧描述、问答推理到视频生成的多任务支持。内部评测表明，相较于同参数规模的开源模型，该系统在视频问答、生成准确性与时序连贯性上均具有显著优势。</p><p><strong>标签</strong>：#多模态语言模型 #视频理解 #文本生成 #统一框架 #7B模型</p><hr><h2 id="gpt‐image‐edit‐1-5m-百万级-gpt-生成图像编辑三元组数据集" tabindex="-1"><a class="header-anchor" href="#gpt‐image‐edit‐1-5m-百万级-gpt-生成图像编辑三元组数据集"><span>GPT‑Image‑Edit‑1.5M：百万级 GPT 生成图像编辑三元组数据集</span></a></h2><figure><img src="https://ucsc-vlaa.github.io/GPT-Image-Edit/static/images/GPT-Edit_teaser_image-cropped.png" alt="GPT‑Image‑Edit‑1.5M Teaser 图" tabindex="0" loading="lazy"><figcaption>GPT‑Image‑Edit‑1.5M Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>GPT‑Image‑Edit‑1.5M</strong> 是 UCSC‑VLAA 等团队基于 GPT‑4o 自动生成的图像编辑集合，整合并增强了 OmniEdit、HQ‑Edit 与 UltraEdit 三大数据源，构建了包含 <strong>150 万条指令-原图-编辑图</strong>的统一训练集。输出图像经过重新渲染，指令语义被重写优化，提高质量与对齐度。经 fine‑tune 后的 FluxKontext 模型在 GEdit‑EN、ImgEdit‑Full 及 Complex‑Edit 基准上表现优异，明显超越此前开源方案并逼近 GPT‑4o 等闭源模型表现。</p><p><strong>标签</strong>：#图像编辑数据 #GPT-4o生成 #大规模三元组 #模型微调 #高质量对齐</p><hr><h2 id="screencoder-多智能体驱动的视觉到前端代码生成系统" tabindex="-1"><a class="header-anchor" href="#screencoder-多智能体驱动的视觉到前端代码生成系统"><span>ScreenCoder：多智能体驱动的视觉到前端代码生成系统</span></a></h2><p><img src="https://github.com/leigest519/ScreenCoder/raw/main/teaser.jpg" alt="ScreenCoder Teaser 图" loading="lazy"><strong>概要</strong>：<strong>ScreenCoder</strong> 是由<strong>香港中文大学等团队</strong>提出的一款创新型系统，能够将界面设计（如手绘图或 mockup）自动转换为对应的 HTML/CSS 前端代码。系统设计为三阶段模块化流程：首先由 <strong>Grounding agent</strong> 基于视觉语言模型对界面组件进行识别与标注；接着 <strong>Planning agent</strong> 构建层次布局树；最后 <strong>Generation agent</strong> 通过 prompt 驱动生成高结构精度的代码。该框架还作为图像—代码对训练引擎，可实时生产大量数据并用于微调视觉语言模型，显著提升布局理解与生成准确度。大量实验显示其在结构一致性、代码正确率和语义对齐方面优于现有端到端方法。</p><p><strong>标签</strong>：#视觉语言模型 #前端自动化 #多智能体架构 #布局理解 #UI生成</p><hr><h2 id="alphavae-端到端-rgba-图像生成与重构统一模型" tabindex="-1"><a class="header-anchor" href="#alphavae-端到端-rgba-图像生成与重构统一模型"><span>AlphaVAE：端到端 RGBA 图像生成与重构统一模型</span></a></h2><figure><img src="https://github.com/o0o0o00o0/AlphaVAE/raw/main/qualitative_t2i.png" alt="AlphaVAE Example 图" tabindex="0" loading="lazy"><figcaption>AlphaVAE Example 图</figcaption></figure><p><strong>概要</strong>：<strong>AlphaVAE</strong> 是<strong>清华大学</strong>最新提出的 RGBA (红绿蓝 + alpha 通道) 图像生成与重构 VAE 模型。创新地在变分自编码器中引入 alpha 通道感知机制，使模型能处理透明背景与遮罩信息，在仅使用 8K 图像训练的情况下，其 PSNR 提高约 <strong>4.9 dB</strong>，SSIM 提升约 <strong>3.2%</strong>，超出此前基于百万图像训练的 LayerDiffuse 方法。该方法采用复合训练目标，包括 alpha-blended 像素重构、patch 级保真损失、感知一致性与双 KL 发散约束，保证 RGB 与 alpha 表示间的潜变量一致性。AlphaVAE 提供了一种高效泛化能力强的生成方案，为 RGBA 图像建模提供了新的可能性。</p><p><strong>标签</strong>：#RGBA 图像 #变分自编码器 #alpha 感知 #高保真重构 #小样本训练</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://x-omni-team.github.io/" target="_blank" rel="noopener noreferrer">X‑Omni 项目主页</a></li><li><a href="https://github.com/X-Omni-Team/X-Omni" target="_blank" rel="noopener noreferrer">X‑Omni Github 仓库</a></li><li><a href="https://arxiv.org/html/2507.22058" target="_blank" rel="noopener noreferrer">X‑Omni 论文</a></li><li><a href="https://3d-models.hunyuan.tencent.com/world/" target="_blank" rel="noopener noreferrer">HunyuanWorld 1.0 项目主页</a></li><li><a href="https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0" target="_blank" rel="noopener noreferrer">HunyuanWorld 1.0 Github 仓库</a></li><li><a href="https://arxiv.org/html/2507.21809v1" target="_blank" rel="noopener noreferrer">HunyuanWorld 1.0 论文</a></li><li><a href="https://tencentarc.github.io/posts/arc-video-announcement/" target="_blank" rel="noopener noreferrer">ARC‑Hunyuan‑Video‑7B 项目公告</a></li><li><a href="https://github.com/TencentARC/ARC-Hunyuan-Video-7B" target="_blank" rel="noopener noreferrer">ARC‑Hunyuan‑Video‑7B Github 仓库</a></li><li><a href="https://arxiv.org/html/2507.20939" target="_blank" rel="noopener noreferrer">ARC‑Hunyuan‑Video‑7B 论文</a></li><li><a href="https://ucsc-vlaa.github.io/GPT-Image-Edit/" target="_blank" rel="noopener noreferrer">GPT‑Image‑Edit 项目主页</a></li><li><a href="https://github.com/wyhlovecpp/GPT-Image-Edit" target="_blank" rel="noopener noreferrer">GPT‑Image‑Edit Github 仓库</a></li><li><a href="https://arxiv.org/html/2507.21033" target="_blank" rel="noopener noreferrer">GPT‑Image‑Edit 论文</a></li><li><a href="https://github.com/leigest519/ScreenCoder" target="_blank" rel="noopener noreferrer">ScreenCoder Github 仓库</a></li><li><a href="https://arxiv.org/html/2507.22827" target="_blank" rel="noopener noreferrer">ScreenCoder 论文</a></li><li><a href="https://github.com/o0o0o00o0/AlphaVAE" target="_blank" rel="noopener noreferrer">AlphaVAE Github 仓库</a></li><li><a href="https://arxiv.org/pdf/2507.09308" target="_blank" rel="noopener noreferrer">AlphaVAE 论文</a></li></ol>',36)]))}]]),o=JSON.parse('{"path":"/zh/posts/ai-weekly/049.html","title":"X‑Omni融合生成与理解|HunyuanWorld‑1.0构建可交互3D世界|ScreenCoder自动图像转代码【AI周报】","lang":"zh-CN","frontmatter":{"description":"X‑Omni融合生成与理解|HunyuanWorld‑1.0构建可交互3D世界|ScreenCoder自动图像转代码【AI周报】 封面图源自C站作者Koal2封面图源自C站作者Koal2 摘要 本周亮点：X‑Omni利用强化学习驱动多模态理解与高质量图像生成统一；HunyuanWorld‑1.0从图像或文本构建可交互的3D世界；ScreenCoder构...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/049.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"X‑Omni融合生成与理解|HunyuanWorld‑1.0构建可交互3D世界|ScreenCoder自动图像转代码【AI周报】"}],["meta",{"property":"og:description","content":"X‑Omni融合生成与理解|HunyuanWorld‑1.0构建可交互3D世界|ScreenCoder自动图像转代码【AI周报】 封面图源自C站作者Koal2封面图源自C站作者Koal2 摘要 本周亮点：X‑Omni利用强化学习驱动多模态理解与高质量图像生成统一；HunyuanWorld‑1.0从图像或文本构建可交互的3D世界；ScreenCoder构..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"X‑Omni融合生成与理解|HunyuanWorld‑1.0构建可交互3D世界|ScreenCoder自动图像转代码【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg\\",\\"https://x-omni-team.github.io/static/images/fig1-1.png\\",\\"https://3d-models.hunyuan.tencent.com/world/assets/webp/archi-CCIB2cwa.webp\\",\\"https://tencentarc.github.io/images/arc_video_method.jpg\\",\\"https://ucsc-vlaa.github.io/GPT-Image-Edit/static/images/GPT-Edit_teaser_image-cropped.png\\",\\"https://github.com/leigest519/ScreenCoder/raw/main/teaser.jpg\\",\\"https://github.com/o0o0o00o0/AlphaVAE/raw/main/qualitative_t2i.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"X‑Omni：使用强化学习提升离散自回归图像生成质量的统一多模态模型","slug":"x‐omni-使用强化学习提升离散自回归图像生成质量的统一多模态模型","link":"#x‐omni-使用强化学习提升离散自回归图像生成质量的统一多模态模型","children":[]},{"level":2,"title":"HunyuanWorld 1.0：从文本或图像生成可探索、可交互的沉浸式 3D 世界","slug":"hunyuanworld-1-0-从文本或图像生成可探索、可交互的沉浸式-3d-世界","link":"#hunyuanworld-1-0-从文本或图像生成可探索、可交互的沉浸式-3d-世界","children":[]},{"level":2,"title":"ARC-Hunyuan‑Video‑7B：7B 模型驱动的视频理解与生成统一框架","slug":"arc-hunyuan‐video‐7b-7b-模型驱动的视频理解与生成统一框架","link":"#arc-hunyuan‐video‐7b-7b-模型驱动的视频理解与生成统一框架","children":[]},{"level":2,"title":"GPT‑Image‑Edit‑1.5M：百万级 GPT 生成图像编辑三元组数据集","slug":"gpt‐image‐edit‐1-5m-百万级-gpt-生成图像编辑三元组数据集","link":"#gpt‐image‐edit‐1-5m-百万级-gpt-生成图像编辑三元组数据集","children":[]},{"level":2,"title":"ScreenCoder：多智能体驱动的视觉到前端代码生成系统","slug":"screencoder-多智能体驱动的视觉到前端代码生成系统","link":"#screencoder-多智能体驱动的视觉到前端代码生成系统","children":[]},{"level":2,"title":"AlphaVAE：端到端 RGBA 图像生成与重构统一模型","slug":"alphavae-端到端-rgba-图像生成与重构统一模型","link":"#alphavae-端到端-rgba-图像生成与重构统一模型","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":6.19,"words":1857},"filePathRelative":"zh/posts/ai-weekly/049.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg\\" alt=\\"封面图源自C站作者Koal2\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面图源自C站作者Koal2</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：X‑Omni利用强化学习驱动多模态理解与高质量图像生成统一；HunyuanWorld‑1.0从图像或文本构建可交互的3D世界；ScreenCoder构建多智能体UI编码系统，实现图像到代码自动转换。详见正文，相关参考链接请见文末。</p>","autoDesc":true}')}}]);