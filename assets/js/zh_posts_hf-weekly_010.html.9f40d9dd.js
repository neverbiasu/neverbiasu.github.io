"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[4281],{66262:(n,e)=>{e.A=(n,e)=>{const a=n.__vccOpts||n;for(const[n,i]of e)a[n]=i;return a}},71010:(n,e,a)=>{a.r(e),a.d(e,{comp:()=>o,data:()=>r});var i=a(20641);const t={},o=(0,a(66262).A)(t,[["render",function(n,e){return(0,i.uX)(),(0,i.CE)("div",null,e[0]||(e[0]=[(0,i.Fv)('<h1 id="vibevoice长音频生成-longcat-flash高效moe-internvl3-5多模态新sota【hf周报】" tabindex="-1"><a class="header-anchor" href="#vibevoice长音频生成-longcat-flash高效moe-internvl3-5多模态新sota【hf周报】"><span>VibeVoice长音频生成 | LongCat-Flash高效MoE | InternVL3.5多模态新SOTA【HF周报】</span></a></h1><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：微软发布专为长篇对话设计的TTS模型VibeVoice；美团推出5600亿参数的创新MoE语言模型LongCat-Flash；OpenGVLab的InternVL3.5在多模态基准上再创SOTA。此外，还有来自面壁智能、Wan-AI、StepFun和腾讯混元等机构的最新模型。详见正文，相关参考链接请见文末。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#vibevoice-15b%E5%BE%AE%E8%BD%AF%E5%BC%80%E6%BA%90%E7%9A%84%E9%95%BF%E7%AF%87%E5%A4%9A%E8%AF%B4%E8%AF%9D%E4%BA%BA%E5%AF%B9%E8%AF%9D%E9%9F%B3%E9%A2%91%E7%94%9F%E6%88%90%E6%A1%86%E6%9E%B6">VibeVoice-1.5B：微软开源的长篇多说话人对话音频生成框架</a></li><li><a href="#longcat-flash-chat%E7%BE%8E%E5%9B%A2%E6%8E%A8%E5%87%BA%E7%9A%845600%E4%BA%BF%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88moe%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">LongCat-Flash-Chat：美团推出的5600亿参数高效MoE语言模型</a></li><li><a href="#internvl3_5-241b-a28bopengvlab%E5%8F%91%E5%B8%83%E7%9A%84%E6%96%B0%E4%B8%80%E4%BB%A3%E5%BC%80%E6%BA%90%E5%A4%9A%E6%A8%A1%E6%80%81sota%E6%A8%A1%E5%9E%8B">InternVL3_5-241B-A28B：OpenGVLab发布的新一代开源多模态SOTA模型</a></li><li><a href="#minicpm-v-4_5%E9%9D%A2%E5%A3%81%E6%99%BA%E8%83%BD%E6%8E%A8%E5%87%BA%E7%9A%84%E5%8F%AF%E5%9C%A8%E6%89%8B%E6%9C%BA%E7%AB%AF%E8%BF%90%E8%A1%8C%E7%9A%84gpt-4o%E7%BA%A7%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B">MiniCPM-V-4_5：面壁智能推出的可在手机端运行的GPT-4o级多模态模型</a></li><li><a href="#wan22-s2v-14bwan-ai%E5%8F%91%E5%B8%83%E7%9A%84%E9%9F%B3%E9%A2%91%E9%A9%B1%E5%8A%A8%E7%94%B5%E5%BD%B1%E7%BA%A7%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">Wan2.2-S2V-14B：Wan-AI发布的音频驱动电影级视频生成模型</a></li><li><a href="#step-audio-2-ministepfun%E6%8E%A8%E5%87%BA%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E9%9F%B3%E9%A2%91%E7%90%86%E8%A7%A3%E4%B8%8E%E8%AF%AD%E9%9F%B3%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B">Step-Audio-2-mini：StepFun推出的端到端音频理解与语音对话模型</a></li><li><a href="#hunyuanvideo-foley%E8%85%BE%E8%AE%AF%E6%B7%B7%E5%85%83%E5%BC%80%E6%BA%90%E7%9A%84%E4%B8%93%E4%B8%9A%E7%BA%A7%E8%A7%86%E9%A2%91%E9%9F%B3%E6%95%88%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">HunyuanVideo-Foley：腾讯混元开源的专业级视频音效生成模型</a></li></ol><hr><h2 id="vibevoice-1-5b-微软开源的长篇多说话人对话音频生成框架" tabindex="-1"><a class="header-anchor" href="#vibevoice-1-5b-微软开源的长篇多说话人对话音频生成框架"><span>VibeVoice-1.5B：微软开源的长篇多说话人对话音频生成框架</span></a></h2><figure><img src="https://huggingface.co/microsoft/VibeVoice-1.5B/resolve/main/figures/Fig1.png" alt="VibeVoice-1.5 Evaluation 图" tabindex="0" loading="lazy"><figcaption>VibeVoice-1.5 Evaluation 图</figcaption></figure><p><strong>概要</strong>：<strong>微软</strong> 发布了 <strong>VibeVoice-1.5B</strong>，一个专为生成富有表现力的长篇、多说话人对话音频（如播客）而设计的开源模型。它利用7.5Hz的超低帧率连续语音分词器，在保持音频保真度的同时大幅提升了处理长序列的计算效率，可合成长达90分钟、支持多达4个不同说话人的语音。</p><p><strong>标签</strong>：#Microsoft #VibeVoice #文本转语音 #长音频 #多说话人</p><hr><h2 id="longcat-flash-chat-美团推出的5600亿参数高效moe语言模型" tabindex="-1"><a class="header-anchor" href="#longcat-flash-chat-美团推出的5600亿参数高效moe语言模型"><span>LongCat-Flash-Chat：美团推出的5600亿参数高效MoE语言模型</span></a></h2><figure><img src="https://raw.githubusercontent.com/meituan-longcat/LongCat-Flash-Chat/main/figures/longcat_logo.svg" alt="LongCat-Flash-Chat Logo 图" tabindex="0" loading="lazy"><figcaption>LongCat-Flash-Chat Logo 图</figcaption></figure><p><strong>概要</strong>：<strong>美团-LongCat</strong>发布了<strong>LongCat-Flash-Chat</strong>，一个拥有5600亿总参数的MoE语言模型。该模型采用动态计算机制，根据上下文需求激活186亿至313亿参数，平均激活约270亿，优化了计算效率和性能。其架构设计实现了高吞吐量和低延迟的推理。</p><p><strong>标签</strong>：#美团 #LongCat #MoE #大语言模型 #动态计算</p><hr><h2 id="internvl3-5-241b-a28b-opengvlab发布的新一代开源多模态sota模型" tabindex="-1"><a class="header-anchor" href="#internvl3-5-241b-a28b-opengvlab发布的新一代开源多模态sota模型"><span>InternVL3_5-241B-A28B：OpenGVLab发布的新一代开源多模态SOTA模型</span></a></h2><figure><img src="https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance.jpg" alt="InternVL3_5-241B-A28B Performance 图" tabindex="0" loading="lazy"><figcaption>InternVL3_5-241B-A28B Performance 图</figcaption></figure><p><strong>概要</strong>：<strong>OpenGVLab</strong> 推出了 <strong>InternVL3.5</strong>系列，这是一个在多功能性、推理能力和推理效率方面取得显著进步的开源多模态模型家族。其关键创新是级联强化学习（Cascade RL）框架和视觉分辨率路由器（ViR），在提升推理性能的同时实现了4.05倍的推理加速，并在多个基准测试中取得了SOTA结果。</p><p><strong>标签</strong>：#OpenGVLab #InternVL #多模态 #级联强化学习 #SOTA</p><hr><h2 id="minicpm-v-4-5-面壁智能推出的可在手机端运行的gpt-4o级多模态模型" tabindex="-1"><a class="header-anchor" href="#minicpm-v-4-5-面壁智能推出的可在手机端运行的gpt-4o级多模态模型"><span>MiniCPM-V-4_5：面壁智能推出的可在手机端运行的GPT-4o级多模态模型</span></a></h2><figure><img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/radar_minicpm_v45.png" alt="MiniCPM-V-4_5 Evaluation 图" tabindex="0" loading="lazy"><figcaption>MiniCPM-V-4_5 Evaluation 图</figcaption></figure><p><strong>概要</strong>：<strong>面壁智能（OpenBNB）<strong>发布了</strong>MiniCPM-V 4.5</strong>，这是MiniCPM-V系列的最新模型。该模型基于Qwen3-8B和SigLIP2-400M构建，总参数为8B，在OpenCompass上取得了优异成绩，支持高效的高帧率和长视频理解，并提供可控的快速/深度思维混合模式。</p><p><strong>标签</strong>：#面壁智能 #MiniCPM #端侧模型 #视频理解 #混合思维</p><hr><h2 id="wan2-2-s2v-14b-wan-ai发布的音频驱动电影级视频生成模型" tabindex="-1"><a class="header-anchor" href="#wan2-2-s2v-14b-wan-ai发布的音频驱动电影级视频生成模型"><span>Wan2.2-S2V-14B：Wan-AI发布的音频驱动电影级视频生成模型</span></a></h2><figure><img src="https://humanaigc.github.io/wan-s2v-webpage/content/v3/pipeline.png" alt="Wan2.2-S2V-14B Pipeline 图" tabindex="0" loading="lazy"><figcaption>Wan2.2-S2V-14B Pipeline 图</figcaption></figure><p><strong>概要</strong>：<strong>Wan-AI</strong> 发布了 <strong>Wan2.2-S2V-14B</strong>，一个音频驱动的电影级视频生成模型。它引入了混合专家（MoE）架构，并支持通过音频输入结合参考图像和可选的文本提示来生成视频，甚至可以由姿态视频驱动，实现与音频输入同步的特定姿态序列。</p><p><strong>标签</strong>：#Wan-AI #WanS2V #音频驱动视频 #视频生成 #MoE架构</p><hr><h2 id="step-audio-2-mini-stepfun推出的端到端音频理解与语音对话模型" tabindex="-1"><a class="header-anchor" href="#step-audio-2-mini-stepfun推出的端到端音频理解与语音对话模型"><span>Step-Audio-2-mini：StepFun推出的端到端音频理解与语音对话模型</span></a></h2><figure><img src="https://huggingface.co/stepfun-ai/Step-Audio-2-mini/resolve/main/assets/radar.png" alt="Step-Audio-2-mini Radar 图" tabindex="0" loading="lazy"><figcaption>Step-Audio-2-mini Radar 图</figcaption></figure><p><strong>概要</strong>**：StepFun** 发布了<strong>Step-Audio 2-mini</strong>，一个专为工业级音频理解和语音对话设计的端到端多模态大语言模型。它在ASR和音频理解方面表现出色，能实现自然智能的交互，并利用工具调用和多模态RAG访问真实世界知识，减少幻觉。</p><p><strong>标签</strong>：#StepFun #StepAudio #音频理解 #语音对话 #多模态RAG</p><hr><h2 id="hunyuanvideo-foley-腾讯混元开源的专业级视频音效生成模型" tabindex="-1"><a class="header-anchor" href="#hunyuanvideo-foley-腾讯混元开源的专业级视频音效生成模型"><span>HunyuanVideo-Foley：腾讯混元开源的专业级视频音效生成模型</span></a></h2><figure><img src="https://huggingface.co/tencent/HunyuanVideo-Foley/resolve/main/assets/pan_chart.png" alt="HunyuanVideo-Foley Evaluation 图" tabindex="0" loading="lazy"><figcaption>HunyuanVideo-Foley Evaluation 图</figcaption></figure><p><strong>概要</strong>：<strong>腾讯混元</strong> 开源了<strong>HunyuanVideo-Foley</strong>，一个专为视频内容创作者设计的端到端视频音效生成模型。它能生成与复杂视频场景同步且语义对齐的高质量音频，智能平衡视觉和文本信息，并输出48kHz的高保真音频，在多个评测基准上达到SOTA水平。</p><p><strong>标签</strong>：#腾讯混元 #HunyuanVideoFoley #视频音效 #Foley #多场景同步</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://huggingface.co/microsoft/VibeVoice-1.5B" target="_blank" rel="noopener noreferrer">VibeVoice-1.5B</a></li><li><a href="https://huggingface.co/openbmb/MiniCPM-V-4_5" target="_blank" rel="noopener noreferrer">MiniCPM-V-4_5</a></li><li><a href="https://huggingface.co/Wan-AI/Wan2.2-S2V-14B" target="_blank" rel="noopener noreferrer">Wan2.2-S2V-14B</a></li><li><a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Chat" target="_blank" rel="noopener noreferrer">LongCat-Flash-Chat</a></li><li><a href="https://huggingface.co/stepfun-ai/Step-Audio-2-mini" target="_blank" rel="noopener noreferrer">Step-Audio-2-mini</a></li><li><a href="https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B" target="_blank" rel="noopener noreferrer">InternVL3_5-241B-A28B</a></li><li><a href="https://huggingface.co/tencent/HunyuanVideo-Foley" target="_blank" rel="noopener noreferrer">HunyuanVideo-Foley</a></li></ol>',44)]))}]]),r=JSON.parse('{"path":"/zh/posts/hf-weekly/010.html","title":"VibeVoice长音频生成 | LongCat-Flash高效MoE | InternVL3.5多模态新SOTA【HF周报】","lang":"zh-CN","frontmatter":{"description":"VibeVoice长音频生成 | LongCat-Flash高效MoE | InternVL3.5多模态新SOTA【HF周报】 摘要 本周亮点：微软发布专为长篇对话设计的TTS模型VibeVoice；美团推出5600亿参数的创新MoE语言模型LongCat-Flash；OpenGVLab的InternVL3.5在多模态基准上再创SOTA。此外，还有来自...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/hf-weekly/010.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"VibeVoice长音频生成 | LongCat-Flash高效MoE | InternVL3.5多模态新SOTA【HF周报】"}],["meta",{"property":"og:description","content":"VibeVoice长音频生成 | LongCat-Flash高效MoE | InternVL3.5多模态新SOTA【HF周报】 摘要 本周亮点：微软发布专为长篇对话设计的TTS模型VibeVoice；美团推出5600亿参数的创新MoE语言模型LongCat-Flash；OpenGVLab的InternVL3.5在多模态基准上再创SOTA。此外，还有来自..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://huggingface.co/microsoft/VibeVoice-1.5B/resolve/main/figures/Fig1.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"VibeVoice长音频生成 | LongCat-Flash高效MoE | InternVL3.5多模态新SOTA【HF周报】\\",\\"image\\":[\\"https://huggingface.co/microsoft/VibeVoice-1.5B/resolve/main/figures/Fig1.png\\",\\"https://raw.githubusercontent.com/meituan-longcat/LongCat-Flash-Chat/main/figures/longcat_logo.svg\\",\\"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance.jpg\\",\\"https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/radar_minicpm_v45.png\\",\\"https://humanaigc.github.io/wan-s2v-webpage/content/v3/pipeline.png\\",\\"https://huggingface.co/stepfun-ai/Step-Audio-2-mini/resolve/main/assets/radar.png\\",\\"https://huggingface.co/tencent/HunyuanVideo-Foley/resolve/main/assets/pan_chart.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"VibeVoice-1.5B：微软开源的长篇多说话人对话音频生成框架","slug":"vibevoice-1-5b-微软开源的长篇多说话人对话音频生成框架","link":"#vibevoice-1-5b-微软开源的长篇多说话人对话音频生成框架","children":[]},{"level":2,"title":"LongCat-Flash-Chat：美团推出的5600亿参数高效MoE语言模型","slug":"longcat-flash-chat-美团推出的5600亿参数高效moe语言模型","link":"#longcat-flash-chat-美团推出的5600亿参数高效moe语言模型","children":[]},{"level":2,"title":"InternVL3_5-241B-A28B：OpenGVLab发布的新一代开源多模态SOTA模型","slug":"internvl3-5-241b-a28b-opengvlab发布的新一代开源多模态sota模型","link":"#internvl3-5-241b-a28b-opengvlab发布的新一代开源多模态sota模型","children":[]},{"level":2,"title":"MiniCPM-V-4_5：面壁智能推出的可在手机端运行的GPT-4o级多模态模型","slug":"minicpm-v-4-5-面壁智能推出的可在手机端运行的gpt-4o级多模态模型","link":"#minicpm-v-4-5-面壁智能推出的可在手机端运行的gpt-4o级多模态模型","children":[]},{"level":2,"title":"Wan2.2-S2V-14B：Wan-AI发布的音频驱动电影级视频生成模型","slug":"wan2-2-s2v-14b-wan-ai发布的音频驱动电影级视频生成模型","link":"#wan2-2-s2v-14b-wan-ai发布的音频驱动电影级视频生成模型","children":[]},{"level":2,"title":"Step-Audio-2-mini：StepFun推出的端到端音频理解与语音对话模型","slug":"step-audio-2-mini-stepfun推出的端到端音频理解与语音对话模型","link":"#step-audio-2-mini-stepfun推出的端到端音频理解与语音对话模型","children":[]},{"level":2,"title":"HunyuanVideo-Foley：腾讯混元开源的专业级视频音效生成模型","slug":"hunyuanvideo-foley-腾讯混元开源的专业级视频音效生成模型","link":"#hunyuanvideo-foley-腾讯混元开源的专业级视频音效生成模型","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":4.85,"words":1456},"filePathRelative":"zh/posts/hf-weekly/010.md","excerpt":"\\n<h2>摘要</h2>\\n<p>本周亮点：微软发布专为长篇对话设计的TTS模型VibeVoice；美团推出5600亿参数的创新MoE语言模型LongCat-Flash；OpenGVLab的InternVL3.5在多模态基准上再创SOTA。此外，还有来自面壁智能、Wan-AI、StepFun和腾讯混元等机构的最新模型。详见正文，相关参考链接请见文末。</p>\\n<hr>\\n<h2>目录</h2>\\n<ol>\\n<li><a href=\\"#vibevoice-15b%E5%BE%AE%E8%BD%AF%E5%BC%80%E6%BA%90%E7%9A%84%E9%95%BF%E7%AF%87%E5%A4%9A%E8%AF%B4%E8%AF%9D%E4%BA%BA%E5%AF%B9%E8%AF%9D%E9%9F%B3%E9%A2%91%E7%94%9F%E6%88%90%E6%A1%86%E6%9E%B6\\">VibeVoice-1.5B：微软开源的长篇多说话人对话音频生成框架</a></li>\\n<li><a href=\\"#longcat-flash-chat%E7%BE%8E%E5%9B%A2%E6%8E%A8%E5%87%BA%E7%9A%845600%E4%BA%BF%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88moe%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B\\">LongCat-Flash-Chat：美团推出的5600亿参数高效MoE语言模型</a></li>\\n<li><a href=\\"#internvl3_5-241b-a28bopengvlab%E5%8F%91%E5%B8%83%E7%9A%84%E6%96%B0%E4%B8%80%E4%BB%A3%E5%BC%80%E6%BA%90%E5%A4%9A%E6%A8%A1%E6%80%81sota%E6%A8%A1%E5%9E%8B\\">InternVL3_5-241B-A28B：OpenGVLab发布的新一代开源多模态SOTA模型</a></li>\\n<li><a href=\\"#minicpm-v-4_5%E9%9D%A2%E5%A3%81%E6%99%BA%E8%83%BD%E6%8E%A8%E5%87%BA%E7%9A%84%E5%8F%AF%E5%9C%A8%E6%89%8B%E6%9C%BA%E7%AB%AF%E8%BF%90%E8%A1%8C%E7%9A%84gpt-4o%E7%BA%A7%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B\\">MiniCPM-V-4_5：面壁智能推出的可在手机端运行的GPT-4o级多模态模型</a></li>\\n<li><a href=\\"#wan22-s2v-14bwan-ai%E5%8F%91%E5%B8%83%E7%9A%84%E9%9F%B3%E9%A2%91%E9%A9%B1%E5%8A%A8%E7%94%B5%E5%BD%B1%E7%BA%A7%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B\\">Wan2.2-S2V-14B：Wan-AI发布的音频驱动电影级视频生成模型</a></li>\\n<li><a href=\\"#step-audio-2-ministepfun%E6%8E%A8%E5%87%BA%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E9%9F%B3%E9%A2%91%E7%90%86%E8%A7%A3%E4%B8%8E%E8%AF%AD%E9%9F%B3%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B\\">Step-Audio-2-mini：StepFun推出的端到端音频理解与语音对话模型</a></li>\\n<li><a href=\\"#hunyuanvideo-foley%E8%85%BE%E8%AE%AF%E6%B7%B7%E5%85%83%E5%BC%80%E6%BA%90%E7%9A%84%E4%B8%93%E4%B8%9A%E7%BA%A7%E8%A7%86%E9%A2%91%E9%9F%B3%E6%95%88%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B\\">HunyuanVideo-Foley：腾讯混元开源的专业级视频音效生成模型</a></li>\\n</ol>","autoDesc":true}')}}]);