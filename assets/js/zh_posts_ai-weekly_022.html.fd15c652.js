"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[4864],{6262:(e,t)=>{t.A=(e,t)=>{const a=e.__vccOpts||e;for(const[e,r]of t)a[e]=r;return a}},6799:(e,t,a)=>{a.r(t),a.d(t,{comp:()=>n,data:()=>o});var r=a(641);const i={},n=(0,a(6262).A)(i,[["render",function(e,t){return(0,r.uX)(),(0,r.CE)("div",null,t[0]||(t[0]=[(0,r.Fv)('<h1 id="deepseek-r1定义强化学习推理-emo2高效语音驱动数字-textoon文本生成live2d模型【ai周报】" tabindex="-1"><a class="header-anchor" href="#deepseek-r1定义强化学习推理-emo2高效语音驱动数字-textoon文本生成live2d模型【ai周报】"><span>DeepSeek-R1定义强化学习推理|EMO2高效语音驱动数字|Textoon文本生成Live2D模型【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/5ea5d962-dcc2-4afe-983e-9b4eedb1f20d/original=true,quality=90/52768659.jpeg" alt="封面源自C站作者AIdaFONDA" tabindex="0" loading="lazy"><figcaption>封面源自C站作者AIdaFONDA</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：FilmAgent实现多智能体协作虚拟电影制作；DeepSeek-R1通过强化学习提升推理性能；EMO-2单图驱动数字人；PASA增强学术搜索效率；Textoon文本生成Live2D模型。详情见正文。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#filmagent%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%8F%E4%BD%9C%E7%9A%84%E8%99%9A%E6%8B%9F%E7%94%B5%E5%BD%B1%E5%88%B6%E4%BD%9C%E6%A1%86%E6%9E%B6">FilmAgent：多智能体协作的虚拟电影制作框架</a></li><li><a href="#deepseek-r1%E9%80%9A%E8%BF%87%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A2%9E%E5%BC%BA%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B">DeepSeek-R1：通过强化学习增强大型语言模型的推理能力</a></li><li><a href="#kimi-k15%E6%94%AF%E6%8C%81%E5%A4%8D%E6%9D%82%E6%8E%A8%E7%90%86%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81ai%E5%8A%A9%E6%89%8B">Kimi-k1.5：支持复杂推理的多模态AI助手</a></li><li><a href="#emo2%E4%BB%8E%E9%9D%99%E6%80%81%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E5%8A%A8%E6%80%81%E8%A1%A8%E6%83%85">EMO2：从静态图像生成动态表情</a></li><li><a href="#gamefactory%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B8%B8%E6%88%8F%E5%86%85%E5%AE%B9%E7%94%9F%E6%88%90%E5%B9%B3%E5%8F%B0">GameFactory：自动化游戏内容生成平台</a></li><li><a href="#pasa%E9%9D%A2%E5%90%91%E5%AD%A6%E6%9C%AF%E6%90%9C%E7%B4%A2%E7%9A%84%E6%99%BA%E8%83%BD%E4%BB%A3%E7%90%86">PASA：面向学术搜索的智能代理</a></li><li><a href="#textoon%E5%9F%BA%E4%BA%8E%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E7%94%9F%E5%8A%A8%E7%9A%842d%E5%8D%A1%E9%80%9A%E8%A7%92%E8%89%B2">Textoon：基于文本生成生动的2D卡通角色</a></li></ol><hr><h2 id="filmagent-多智能体协作的虚拟电影制作框架" tabindex="-1"><a class="header-anchor" href="#filmagent-多智能体协作的虚拟电影制作框架"><span>FilmAgent：多智能体协作的虚拟电影制作框架</span></a></h2><figure><img src="https://github.com/HITsz-TMG/FilmAgent/raw/main/pics/framework.png" alt="FilmAgent Framework 图" tabindex="0" loading="lazy"><figcaption>FilmAgent Framework 图</figcaption></figure><p><strong>概要</strong>：<strong>哈尔滨工业大学（深圳）<strong>的研究团队提出了</strong>FilmAgent</strong>，这是一个基于大型语言模型（LLM）的多智能体协作框架，旨在实现虚拟3D空间中的端到端电影自动化制作。FilmAgent模拟了导演、编剧、演员和摄影师等关键角色，涵盖了电影制作的主要阶段：1. <strong>创意开发</strong>：将初步的想法转化为结构化的故事大纲。2. <strong>剧本写作</strong>：为每个场景详细编写对话和角色动作。3. <strong>电影摄影</strong>：为每个镜头确定摄像机设置。各智能体通过迭代反馈和修订进行协作，从而验证中间剧本并减少幻觉现象。人类评估显示，FilmAgent在所有方面均优于基线模型，平均得分为3.98（满分5分），展示了多智能体协作在电影制作中的可行性。进一步分析表明，尽管FilmAgent使用了相对较简单的GPT-4o模型，但其表现优于单智能体模型，体现了协调良好的多智能体系统的优势。</p><p><strong>标签</strong>：#FilmAgent #多智能体协作 #虚拟电影制作 #LLM</p><hr><h2 id="deepseek-r1-通过强化学习增强大型语言模型的推理能力" tabindex="-1"><a class="header-anchor" href="#deepseek-r1-通过强化学习增强大型语言模型的推理能力"><span>DeepSeek-R1：通过强化学习增强大型语言模型的推理能力</span></a></h2><figure><img src="https://github.com/deepseek-ai/DeepSeek-R1/raw/main/figures/benchmark.jpg" alt="DeepSeek-R1 Benchmark 图" tabindex="0" loading="lazy"><figcaption>DeepSeek-R1 Benchmark 图</figcaption></figure><p><strong>概要</strong>：<strong>DeepSeek-R1</strong> 是由中国 AI 初创公司 <strong>DeepSeek</strong> 开发的先进大型语言模型，旨在通过强化学习（Reinforcement Learning, RL）提升模型的推理能力。初始版本 <strong>DeepSeek-R1-Zero</strong> 采用大规模 RL 训练，未经过监督微调（Supervised Fine-Tuning, SFT），展现了卓越的推理性能，但存在可读性差和语言混杂等问题。为此，研究团队引入了多阶段训练和冷启动数据，开发了 <strong>DeepSeek-R1</strong>，在数学、代码和推理任务上表现出与 <strong>OpenAI-o1</strong> 相当的性能。值得注意的是，<strong>DeepSeek</strong> 将 <strong>DeepSeek-R1</strong> 完全开源，采用 MIT 许可证，允许免费商业和学术使用。此外，团队还发布了从 <strong>DeepSeek-R1</strong> 蒸馏而来的六个稠密模型（1.5B、7B、8B、14B、32B、70B），以支持研究社区。</p><p><strong>标签</strong>：#DeepSeek-R1 #大型语言模型 #强化学习 #推理能力 #开源</p><hr><h2 id="kimi-k1-5-支持复杂推理的多模态ai助手" tabindex="-1"><a class="header-anchor" href="#kimi-k1-5-支持复杂推理的多模态ai助手"><span>Kimi-k1.5：支持复杂推理的多模态AI助手</span></a></h2><figure><img src="https://github.com/MoonshotAI/Kimi-k1.5/raw/main/images/system.png" alt="Kimi k1.5 System 图" tabindex="0" loading="lazy"><figcaption>Kimi k1.5 System 图</figcaption></figure><p><strong>概要</strong>：<strong>Kimi k1.5</strong> 是由中国初创公司 <strong>Moonshot AI</strong> 开发的多模态大型语言模型，旨在通过强化学习（RL）扩展大型语言模型（LLM）的能力。该模型在复杂推理任务中表现出色，在AIME、MATH-500和LiveCodeBench等基准测试中，短链式思维（short-CoT）版本的表现优于GPT-4o和Claude Sonnet 3.5，提升幅度高达550%。此外，长链式思维（long-CoT）版本在MathVista、AIME和Codeforces等多种任务中表现与o1相当。Kimi k1.5支持最长128k的上下文窗口，采用高效的训练方法，通过部分回合（partial rollouts）实现高效训练。</p><p><strong>标签</strong>：#Kimi_k1.5 #多模态 #LLM #强化学习</p><hr><h2 id="emo2-端效器引导的音频驱动虚拟形象视频生成" tabindex="-1"><a class="header-anchor" href="#emo2-端效器引导的音频驱动虚拟形象视频生成"><span>EMO2：端效器引导的音频驱动虚拟形象视频生成</span></a></h2><figure><img src="https://humanaigc.github.io/emote-portrait-alive-2/content/v2/motivation.png" alt="EMO2 Motivation 图" tabindex="0" loading="lazy"><figcaption>EMO2 Motivation 图</figcaption></figure><p><strong>概要</strong>：<strong>EMO2</strong> 是由 <strong>阿里巴巴集团智能计算研究院</strong> 提出的音频驱动虚拟形象生成方法。该方法通过输入单张角色图像和音频（如歌唱），生成包含丰富面部表情和手势的虚拟形象视频。与以往专注于全身或半身姿态生成的方法不同，EMO2 重新定义了任务流程，分为两个阶段：首先从音频生成手部姿态，然后采用扩散模型合成视频帧，将生成的手部姿态融入其中，以实现逼真的面部表情和身体动作。实验结果表明，EMO2 在视觉质量和同步精度方面优于现有方法，如 CyberHost 和 Vlogger。</p><p><strong>标签</strong>：#EMO2 #音频驱动 #数字人 #视频生成 #手势生成</p><hr><h2 id="gamefactory-生成交互式视频创建新游戏" tabindex="-1"><a class="header-anchor" href="#gamefactory-生成交互式视频创建新游戏"><span>GameFactory：生成交互式视频创建新游戏</span></a></h2><figure><img src="https://vvictoryuki.github.io/gamefactory/static/assets/1.jpg" alt="GameFactory Overview 图" tabindex="0" loading="lazy"><figcaption>GameFactory Overview 图</figcaption></figure><p><strong>概要</strong>：<strong>GameFactory</strong> 是 <strong>KwaiVGI</strong>（快手视觉生成与交互中心）提出的一个新框架，旨在解决游戏视频生成中场景泛化的挑战。通过结合预训练大型视频生成模型的开放域生成能力和从小规模高质量数据集 GF-Minecraft 学习的动作控制模块，GameFactory 实现了无需依赖固定风格和环境即可创建多样化和新颖游戏的能力。</p><p><strong>标签</strong>：#GameFactory #KwaiVGI #游戏生成 #视频生成</p><hr><h2 id="pasa-面向学术搜索的智能代理" tabindex="-1"><a class="header-anchor" href="#pasa-面向学术搜索的智能代理"><span>PASA：面向学术搜索的智能代理</span></a></h2><figure><img src="https://github.com/bytedance/pasa/raw/main/src/architecture.png" alt="PaSa Architecture 图" tabindex="0" loading="lazy"><figcaption>PaSa Architecture 图</figcaption></figure><p><strong>概要</strong>：<strong>PaSa</strong>（Pa per Sa erch Agent）是由 <strong>字节跳动研究院</strong> 和 <strong>北京大学</strong> 的研究人员联合开发的学术论文搜索代理。该代理利用大型语言模型（LLM），能够自主决策，包括调用搜索工具、阅读论文以及选择相关参考文献，以提供复杂学术查询的全面准确结果。PaSa通过强化学习进行优化，使用了包含35,000个细粒度学术查询及对应论文的合成数据集AutoScholarQuery。此外，研究团队还开发了RealScholarQuery，这是一个收集真实世界学术查询的基准，用于评估PaSa在实际场景中的性能。尽管仅在合成数据上进行了训练，PaSa在RealScholarQuery上的表现显著优于现有基线方法，包括Google、Google Scholar、结合GPT-4的Google、ChatGPT（支持搜索的GPT-4）、GPT-4，以及基于GPT-4实现的PaSa。</p><p><strong>标签</strong>：#PaSa #学术搜索 #LLM #强化学习 #字节跳动</p><hr><h2 id="textoon-基于文本生成生动的2d卡通角色" tabindex="-1"><a class="header-anchor" href="#textoon-基于文本生成生动的2d卡通角色"><span>Textoon：基于文本生成生动的2D卡通角色</span></a></h2><figure><img src="https://arxiv.org/html/2501.10020v1/x2.png" alt="Textoon Pipeline 图" tabindex="0" loading="lazy"><figcaption>Textoon Pipeline 图</figcaption></figure><p><strong>概要</strong>：<strong>Textoon</strong> 是由 <strong>Tongyi Lab, Alibaba Group</strong> 的研究团队提出的一种创新方法，旨在根据文本描述生成多样化的 2D 卡通角色，采用 Live2D 格式。该方法利用先进的语言和视觉模型，理解文本意图并生成相应的 2D 外观，使用户能够在不到一分钟的时间内创建出各种生动且可交互的 2D 角色。与传统的 3D 模型相比，Textoon 提供了一种更高效的替代方案，无需构建完整的 3D 模型即可模拟 3D 动作，同时采用轻量级的 HTML5 渲染，提高了可访问性和效率。</p><p><strong>标签</strong>：#Textoon #Live2D #数字人 #2D卡通角色 #文生图</p><hr><h3 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span><strong>参考文献</strong></span></a></h3><ol><li><a href="https://github.com/HITsz-TMG/FilmAgent" target="_blank" rel="noopener noreferrer">FilmAgent GitHub</a></li><li><a href="https://filmagent.github.io/" target="_blank" rel="noopener noreferrer">FilmAgent 项目主页</a></li><li><a href="https://arxiv.org/html/2501.12909v1" target="_blank" rel="noopener noreferrer">FilmAgent 论文</a></li><li><a href="https://github.com/deepseek-ai/DeepSeek-R1" target="_blank" rel="noopener noreferrer">DeepSeek-R1 GitHub</a></li><li><a href="https://chat.deepseek.com/" target="_blank" rel="noopener noreferrer">DeepSeek-R1 项目主页</a></li><li><a href="https://arxiv.org/html/2501.12948v1" target="_blank" rel="noopener noreferrer">DeepSeek-R1 论文</a></li><li><a href="https://github.com/MoonshotAI/Kimi-k1.5" target="_blank" rel="noopener noreferrer">Kimi-k1.5 GitHub</a></li><li><a href="https://arxiv.org/html/2501.12599v1" target="_blank" rel="noopener noreferrer">Kimi-k1.5 论文</a></li><li><a href="https://humanaigc.github.io/emote-portrait-alive-2/" target="_blank" rel="noopener noreferrer">Emote-Portrait-Alive-2 项目主页</a></li><li><a href="https://arxiv.org/html/2501.10687v1" target="_blank" rel="noopener noreferrer">Emote-Portrait-Alive-2 论文</a></li><li><a href="https://vvictoryuki.github.io/gamefactory/" target="_blank" rel="noopener noreferrer">GameFactory 项目主页</a></li><li><a href="https://github.com/KwaiVGI/GameFactory" target="_blank" rel="noopener noreferrer">GameFactory GitHub</a></li><li><a href="https://arxiv.org/html/2501.08325v1" target="_blank" rel="noopener noreferrer">GameFactory 论文</a></li><li><a href="https://github.com/bytedance/pasa" target="_blank" rel="noopener noreferrer">PASA GitHub</a></li><li><a href="https://arxiv.org/html/2501.10120v1" target="_blank" rel="noopener noreferrer">PASA 论文</a></li><li><a href="https://human3daigc.github.io/Textoon_webpage/" target="_blank" rel="noopener noreferrer">Textoon 项目主页</a></li><li><a href="https://arxiv.org/html/2501.10020v1" target="_blank" rel="noopener noreferrer">Textoon 论文</a></li></ol>',45)]))}]]),o=JSON.parse('{"path":"/zh/posts/ai-weekly/022.html","title":"DeepSeek-R1定义强化学习推理|EMO2高效语音驱动数字|Textoon文本生成Live2D模型【AI周报】","lang":"zh-CN","frontmatter":{"description":"DeepSeek-R1定义强化学习推理|EMO2高效语音驱动数字|Textoon文本生成Live2D模型【AI周报】 封面源自C站作者AIdaFONDA封面源自C站作者AIdaFONDA 摘要 本周亮点：FilmAgent实现多智能体协作虚拟电影制作；DeepSeek-R1通过强化学习提升推理性能；EMO-2单图驱动数字人；PASA增强学术搜索效率；T...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/022.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"DeepSeek-R1定义强化学习推理|EMO2高效语音驱动数字|Textoon文本生成Live2D模型【AI周报】"}],["meta",{"property":"og:description","content":"DeepSeek-R1定义强化学习推理|EMO2高效语音驱动数字|Textoon文本生成Live2D模型【AI周报】 封面源自C站作者AIdaFONDA封面源自C站作者AIdaFONDA 摘要 本周亮点：FilmAgent实现多智能体协作虚拟电影制作；DeepSeek-R1通过强化学习提升推理性能；EMO-2单图驱动数字人；PASA增强学术搜索效率；T..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/5ea5d962-dcc2-4afe-983e-9b4eedb1f20d/original=true,quality=90/52768659.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"DeepSeek-R1定义强化学习推理|EMO2高效语音驱动数字|Textoon文本生成Live2D模型【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/5ea5d962-dcc2-4afe-983e-9b4eedb1f20d/original=true,quality=90/52768659.jpeg\\",\\"https://github.com/HITsz-TMG/FilmAgent/raw/main/pics/framework.png\\",\\"https://github.com/deepseek-ai/DeepSeek-R1/raw/main/figures/benchmark.jpg\\",\\"https://github.com/MoonshotAI/Kimi-k1.5/raw/main/images/system.png\\",\\"https://humanaigc.github.io/emote-portrait-alive-2/content/v2/motivation.png\\",\\"https://vvictoryuki.github.io/gamefactory/static/assets/1.jpg\\",\\"https://github.com/bytedance/pasa/raw/main/src/architecture.png\\",\\"https://arxiv.org/html/2501.10020v1/x2.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"FilmAgent：多智能体协作的虚拟电影制作框架","slug":"filmagent-多智能体协作的虚拟电影制作框架","link":"#filmagent-多智能体协作的虚拟电影制作框架","children":[]},{"level":2,"title":"DeepSeek-R1：通过强化学习增强大型语言模型的推理能力","slug":"deepseek-r1-通过强化学习增强大型语言模型的推理能力","link":"#deepseek-r1-通过强化学习增强大型语言模型的推理能力","children":[]},{"level":2,"title":"Kimi-k1.5：支持复杂推理的多模态AI助手","slug":"kimi-k1-5-支持复杂推理的多模态ai助手","link":"#kimi-k1-5-支持复杂推理的多模态ai助手","children":[]},{"level":2,"title":"EMO2：端效器引导的音频驱动虚拟形象视频生成","slug":"emo2-端效器引导的音频驱动虚拟形象视频生成","link":"#emo2-端效器引导的音频驱动虚拟形象视频生成","children":[]},{"level":2,"title":"GameFactory：生成交互式视频创建新游戏","slug":"gamefactory-生成交互式视频创建新游戏","link":"#gamefactory-生成交互式视频创建新游戏","children":[]},{"level":2,"title":"PASA：面向学术搜索的智能代理","slug":"pasa-面向学术搜索的智能代理","link":"#pasa-面向学术搜索的智能代理","children":[]},{"level":2,"title":"Textoon：基于文本生成生动的2D卡通角色","slug":"textoon-基于文本生成生动的2d卡通角色","link":"#textoon-基于文本生成生动的2d卡通角色","children":[{"level":3,"title":"参考文献","slug":"参考文献","link":"#参考文献","children":[]}]}],"readingTime":{"minutes":6.97,"words":2092},"filePathRelative":"zh/posts/ai-weekly/022.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/5ea5d962-dcc2-4afe-983e-9b4eedb1f20d/original=true,quality=90/52768659.jpeg\\" alt=\\"封面源自C站作者AIdaFONDA\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面源自C站作者AIdaFONDA</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：FilmAgent实现多智能体协作虚拟电影制作；DeepSeek-R1通过强化学习提升推理性能；EMO-2单图驱动数字人；PASA增强学术搜索效率；Textoon文本生成Live2D模型。详情见正文。</p>","autoDesc":true}')}}]);