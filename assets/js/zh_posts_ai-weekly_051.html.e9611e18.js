"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[6730],{66262:(t,e)=>{e.A=(t,e)=>{const r=t.__vccOpts||t;for(const[t,o]of e)r[t]=o;return r}},65189:(t,e,r)=>{r.r(e),r.d(e,{comp:()=>i,data:()=>n});var o=r(20641);const a={},i=(0,r(66262).A)(a,[["render",function(t,e){return(0,o.uX)(),(0,o.CE)("div",null,e[0]||(e[0]=[(0,o.Fv)('<h1 id="nextstep-1-发布-unipic-2-升级多模态生成-stableavatar-推出长时人像视频【ai周报】" tabindex="-1"><a class="header-anchor" href="#nextstep-1-发布-unipic-2-升级多模态生成-stableavatar-推出长时人像视频【ai周报】"><span>NextStep-1 发布 | UniPic-2 升级多模态生成 | StableAvatar 推出长时人像视频【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg" alt="封面图源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面图源自C站作者Koal2</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：StepFun 推出 NextStep-1 优化大模型训练与推理；Skywork 发布 UniPic-2 强化跨模态生成与编辑；复旦/微软等发布 StableAvatar，实现高保真无限时长音频驱动人像视频。详见正文，相关参考链接请见文末。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#unipic-20-%E4%B8%80%E4%BD%93%E5%8C%96%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3%E4%B8%8E%E7%BC%96%E8%BE%91%E5%8D%87%E7%BA%A7%E6%A8%A1%E5%9E%8B">UniPic 2.0：一体化视觉理解与编辑升级模型</a></li><li><a href="#nextstep-1-%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%BF%9E%E7%BB%AD-token-%E8%87%AA%E5%9B%9E%E5%BD%92%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">NextStep-1：大规模连续-token-自回归图像生成模型</a></li><li><a href="#story2board-%E8%AE%AD%E7%BB%83%E5%85%8D%E7%96%AB%E7%94%B5%E5%BD%B1%E7%BA%A7%E6%95%85%E4%BA%8B%E6%9D%BF%E7%94%9F%E6%88%90%E7%B3%BB%E7%BB%9F">Story2Board：训练免疫电影级故事板生成系统</a></li><li><a href="#voost-%E6%8E%A8%E5%87%BA%E7%BB%9F%E4%B8%80%E8%99%9A%E6%8B%9F%E8%AF%95%E7%A9%BF%E4%B8%8E%E8%84%B1%E4%B8%8B%E6%A1%86%E6%9E%B6">Voost：推出统一虚拟试穿与脱下框架</a></li><li><a href="#tooncomposer-%E5%8D%A1%E9%80%9A%E5%88%B6%E4%BD%9C%E4%B8%80%E4%BD%93%E5%8C%96%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">ToonComposer：卡通制作一体化生成模型</a></li><li><a href="#echo-4o-%E5%A4%9A%E6%A0%A1%E8%81%94%E5%90%88%E6%89%93%E9%80%A0%E7%9A%84-gpt-4o-%E5%90%88%E6%88%90%E5%9B%BE%E5%83%8F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%A2%9E%E5%BC%BA%E6%A1%86%E6%9E%B6">Echo-4o：多校联合打造的 GPT-4o 合成图像大数据与模型增强框架</a></li><li><a href="#stableavatar-%E6%97%A0%E9%99%90%E9%95%BF%E5%BA%A6%E9%9F%B3%E9%A2%91%E9%A9%B1%E5%8A%A8%E5%8C%96%E8%BA%AB%E8%A7%86%E9%A2%91%E7%B3%BB%E7%BB%9F">StableAvatar：无限长度音频驱动化身视频系统</a></li></ol><hr><h2 id="unipic-2-0-一体化视觉理解与编辑升级模型" tabindex="-1"><a class="header-anchor" href="#unipic-2-0-一体化视觉理解与编辑升级模型"><span>UniPic 2.0：一体化视觉理解与编辑升级模型</span></a></h2><figure><img src="https://unipic-v2.github.io/static/imgs/unipicv2-pipeline.png" alt="UniPic 2.0 Pipeline 图" tabindex="0" loading="lazy"><figcaption>UniPic 2.0 Pipeline 图</figcaption></figure><p><strong>概要</strong>：<strong>UniPic 2.0</strong> 是由 <strong>天工AI多模态团队</strong>推出的第二代统一视觉模型，基于 Qwen2.5-VL-Instruct 与 SD3.5-Medium 架构，内置“理解—生成—编辑”三合一能力框架。该版本通过 <strong>Flow-GRPO 强化学习</strong> 同步优化生成与编辑任务，显著提升文本渲染与编辑一致性，展现出比 Bagel、UniWorld-V1 等更优的多任务表现。尽管模型参数仅为 2B，仍在多个基准测试中取得 SOTA 级成绩，展示了极佳的效率与扩展性。</p><p><strong>标签</strong>：#统一视觉模型 #生成与编辑融合 #强化学习优化 #高效低参 #天工AI</p><hr><h2 id="nextstep-1-大规模连续-token-自回归图像生成模型" tabindex="-1"><a class="header-anchor" href="#nextstep-1-大规模连续-token-自回归图像生成模型"><span>NextStep-1：大规模连续 token 自回归图像生成模型</span></a></h2><figure><img src="https://github.com/stepfun-ai/NextStep-1/blob/main/assets/t2i_demo.gif?raw=true" alt="NextStep-1 Demo 图" tabindex="0" loading="lazy"><figcaption>NextStep-1 Demo 图</figcaption></figure><p><strong>概要</strong>：<strong>NextStep-1</strong> 是由**NextStep Team（StepFun 团队）**提出的一款具有 14B 参数的自回归图像生成模型，配备一个 157M 参数的 Flow Matching 头。它首次将“离散文本 token 与连续图像 token”进行统一的 next-token 预测学习，显著提高了图像合成质量和编辑性能，在文本到图像任务中达成 SOTA 水平，并展示了强大的图像编辑能力。同时，官方计划开放代码与模型，推动社区研究发展。</p><p><strong>标签</strong>：#自回归图像生成 #连续图像Token #FlowMatching头 #高保真生成 #阶跃星辰</p><hr><h2 id="story2board-训练免疫电影级故事板生成系统" tabindex="-1"><a class="header-anchor" href="#story2board-训练免疫电影级故事板生成系统"><span>Story2Board：训练免疫电影级故事板生成系统</span></a></h2><figure><img src="https://daviddinkevich.github.io/Story2Board/static/images/teaser.webp" alt="Story2Board Teaser 图" tabindex="0" loading="lazy"><figcaption>Story2Board Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>Story2Board</strong> 提出“训练免疫”故事板流程：由 <strong>LLM Director</strong> 将长叙事分解为一个共享参考面板提示与逐帧场景提示以统一角色外观；在生成批次内用 <strong>LPA（Latent Panel Anchoring）</strong> 配合并行去噪，用共享参考面板区域替换对应潜变量区域以强制跨面板的低频与结构一致性；并通过 <strong>RAVM（Reciprocal Attention Value Mixing）</strong> 在自注意力的 value 空间按 token 级别混合参考与目标表征，强化细粒度外观（如表情、手型）而不改动生成布局。整体流程为：LLM 分解提示 → 批量同步 LPA+RAVM 去噪 → 解码与裁切，在不改动基础扩散模型或额外微调的前提下提升角色一致性与细节保真。</p><p><strong>标签</strong>：#故事板生成 #训练免疫 #角色一致性 #视觉叙事 #DiT</p><hr><h2 id="voost-推出统一虚拟试穿与脱下框架" tabindex="-1"><a class="header-anchor" href="#voost-推出统一虚拟试穿与脱下框架"><span>Voost：推出统一虚拟试穿与脱下框架</span></a></h2><figure><img src="https://nxnai.github.io/Voost/static/images/teaser.jpg" alt="Voost Teaser 图" tabindex="0" loading="lazy"><figcaption>Voost Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>Voost</strong> 是由 <strong>NXN Labs</strong> 提出的一种创新的扩散 Transformer 模型，能够同时处理虚拟“试穿”（try-on）与“试下”（try-off）任务。该模型通过将服饰图与人体图像在空间上水平拼接，实现统一条件输入，从而增强服装与人体间的对应关系，无需额外网络结构或标签辅导。模型还引入两个推理阶段策略：<strong>attention 温度调整</strong>以应对分辨率或遮挡变化，以及<strong>自我纠错采样</strong>增强两向一致性，显著提升细节对齐与视觉逼真度。大量基准测试显示，Voost 在两项任务上均超越现有方法，质量、通用性与鲁棒性全面领先。</p><p><strong>标签</strong>：#虚拟试穿 #DiffusionTransformer #统一生成 #任务互监督 #视觉一致性</p><hr><h2 id="tooncomposer-卡通制作一体化生成模型" tabindex="-1"><a class="header-anchor" href="#tooncomposer-卡通制作一体化生成模型"><span>ToonComposer：卡通制作一体化生成模型</span></a></h2><figure><img src="https://lg-li.github.io/pub-images/tooncomposer/workflow-cmp.jpg" alt="ToonComposer Motivation 图" tabindex="0" loading="lazy"><figcaption>ToonComposer Motivation 图</figcaption></figure><p><strong>概要</strong>：<strong>ToonComposer</strong> 由 <strong>Tencent ARC 实验室（腾讯 PCG）<strong>与</strong>香港中文大学</strong>联合提出，是一个开创性的生成模型，融合了动画中的“补间”（inbetweening）与“上色”（colorization）两个关键阶段于一个“后关键帧生成”步骤中。模型采用<strong>稀疏素描注入</strong>技术，仅需极少关键帧素描与一帧彩色参考即可生成连贯、风格一致的卡通动画；同时引入 **空间低秩适配器（SLRA）**针对动画领域进行模型微调，在保留 DiT 视频模型的时间一致性前提下，实现视觉样式定制。此外，支持区域控制、灵活输入等增强功能，通过新构建的 <strong>PKBench 基准数据集</strong>进行评估，结果表明在视觉质量、运动连贯性及制作效率方面全面领先。</p><p><strong>标签</strong>：#卡通动画生成 #补间与上色整合 #稀疏素描控制 #空间低秩适配 #腾讯 ARC</p><hr><h2 id="echo-4o-多校联合打造的-gpt-4o-合成图像大数据与模型增强框架" tabindex="-1"><a class="header-anchor" href="#echo-4o-多校联合打造的-gpt-4o-合成图像大数据与模型增强框架"><span>Echo-4o：多校联合打造的 GPT-4o 合成图像大数据与模型增强框架</span></a></h2><figure><img src="https://github.com/yejy53/Echo-4o/raw/master/assets/radar.jpg" alt="Echo-4o Rader 图" tabindex="0" loading="lazy"><figcaption>Echo-4o Rader 图</figcaption></figure><p><strong>概要</strong>：<strong>Echo-4o</strong> 由<strong>中科大</strong>、<strong>香港中文大学</strong>和<strong>中山大学</strong>等机构合作提出的统一模型，是对 Bagel 进行微调后得到的模型，在 Imag-Guided、GenEval、DPG-Bench 及新提出的 “GenEval++” 和 “Imagine-Bench” 等基准上表现均达最新水平；同一合成数据集用于其他模型（如 OmniGen2、BLIP3-o）也能带来稳定的性能提升，证明了其强大的泛化能力与迁移价值。<strong>Echo-4o-Image</strong> 是一个由 GPT-4o 生成的合成图像数据集，包含 180K 条覆盖稀有场景的样本，分为“奇幻风（38K）”、“多参考图像（73K）”与“复杂指令执行（68K）”三类。</p><p><strong>标签</strong>：#GPT-4o合成数据 #幻想图像生成 #多参考融合 #复杂指令理解 #模型微调提升</p><hr><h2 id="stableavatar-无限长度音频驱动化身视频系统" tabindex="-1"><a class="header-anchor" href="#stableavatar-无限长度音频驱动化身视频系统"><span>StableAvatar：无限长度音频驱动化身视频系统</span></a></h2><figure><img src="https://francis-rings.github.io/StableAvatar/static/images/framework.jpg" alt="StableAvatar Framework 图" tabindex="0" loading="lazy"><figcaption>StableAvatar Framework 图</figcaption></figure><p><strong>概要</strong>：<strong>StableAvatar</strong> 由 <strong>复旦大学、微软亚洲研究院、西安交通大学和腾讯联合团队</strong>共同研发，是首套端到端视频扩散 Transformer 模型，支持从单张参考图像与音频输入生成 <strong>无限长度、高保富同步度与身份一致性</strong>的人像视频。模型创新引入了 <strong>Time-step-aware Audio Adapter</strong> 以防止音频信号误差累积以及 <strong>Audio Native Guidance</strong> 机制增强音画对齐，还采用动态加权滑动窗口策略确保视频平滑性。实验结果表明其在无后处理情况下、持续数分钟的视频生成中始终保持质量稳定与人物身份准确。</p><p><strong>标签</strong>：#无缝音频同步 #音频适配器 #无限时长 #扩散Transformer #高身份一致性</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://unipic-v2.github.io/" target="_blank" rel="noopener noreferrer">UniPic 2.0 项目主页</a></li><li><a href="https://github.com/SkyworkAI/UniPic/tree/main/UniPic-2" target="_blank" rel="noopener noreferrer">UniPic 2.0 Github 仓库</a></li><li><a href="https://github.com/SkyworkAI/UniPic/blob/main/UniPic-2/assets/pdf/UNIPIC2.pdf" target="_blank" rel="noopener noreferrer">UniPic 2.0 论文 PDF</a></li><li><a href="https://huggingface.co/Skywork/UniPic2-Metaquery-9B" target="_blank" rel="noopener noreferrer">UniPic 2.0 Hugging Face Demo</a></li><li><a href="https://stepfun.ai/research/en/nextstep1" target="_blank" rel="noopener noreferrer">NextStep-1 项目主页</a></li><li><a href="https://github.com/stepfun-ai/NextStep-1" target="_blank" rel="noopener noreferrer">NextStep-1 Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.10711" target="_blank" rel="noopener noreferrer">NextStep-1 论文</a></li><li><a href="https://daviddinkevich.github.io/Story2Board/" target="_blank" rel="noopener noreferrer">Story2Board 项目主页</a></li><li><a href="https://github.com/daviddinkevich/Story2Board" target="_blank" rel="noopener noreferrer">Story2Board Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.09983v1" target="_blank" rel="noopener noreferrer">Story2Board 论文</a></li><li><a href="https://nxnai.github.io/Voost/" target="_blank" rel="noopener noreferrer">Voost 项目主页</a></li><li><a href="https://github.com/nxnai/Voost" target="_blank" rel="noopener noreferrer">Voost Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.04825v1" target="_blank" rel="noopener noreferrer">Voost 论文</a></li><li><a href="https://lg-li.github.io/project/tooncomposer/" target="_blank" rel="noopener noreferrer">ToonComposer 项目主页</a></li><li><a href="https://github.com/TencentARC/ToonComposer" target="_blank" rel="noopener noreferrer">ToonComposer Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.10881v1" target="_blank" rel="noopener noreferrer">ToonComposer 论文</a></li><li><a href="https://yejy53.github.io/Echo-4o/" target="_blank" rel="noopener noreferrer">Echo-4o 项目主页</a></li><li><a href="https://github.com/yejy53/Echo-4o" target="_blank" rel="noopener noreferrer">Echo-4o Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.09987v1" target="_blank" rel="noopener noreferrer">Echo-4o 论文</a></li><li><a href="https://francis-rings.github.io/StableAvatar/" target="_blank" rel="noopener noreferrer">StableAvatar 项目主页</a></li><li><a href="https://github.com/Francis-Rings/StableAvatar" target="_blank" rel="noopener noreferrer">StableAvatar Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.08248" target="_blank" rel="noopener noreferrer">StableAvatar 论文</a></li></ol>',45)]))}]]),n=JSON.parse('{"path":"/zh/posts/ai-weekly/051.html","title":"NextStep-1 发布 | UniPic-2 升级多模态生成 | StableAvatar 推出长时人像视频【AI周报】","lang":"zh-CN","frontmatter":{"description":"NextStep-1 发布 | UniPic-2 升级多模态生成 | StableAvatar 推出长时人像视频【AI周报】 封面图源自C站作者Koal2封面图源自C站作者Koal2 摘要 本周亮点：StepFun 推出 NextStep-1 优化大模型训练与推理；Skywork 发布 UniPic-2 强化跨模态生成与编辑；复旦/微软等发布 Stab...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/051.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"NextStep-1 发布 | UniPic-2 升级多模态生成 | StableAvatar 推出长时人像视频【AI周报】"}],["meta",{"property":"og:description","content":"NextStep-1 发布 | UniPic-2 升级多模态生成 | StableAvatar 推出长时人像视频【AI周报】 封面图源自C站作者Koal2封面图源自C站作者Koal2 摘要 本周亮点：StepFun 推出 NextStep-1 优化大模型训练与推理；Skywork 发布 UniPic-2 强化跨模态生成与编辑；复旦/微软等发布 Stab..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"NextStep-1 发布 | UniPic-2 升级多模态生成 | StableAvatar 推出长时人像视频【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg\\",\\"https://unipic-v2.github.io/static/imgs/unipicv2-pipeline.png\\",\\"https://github.com/stepfun-ai/NextStep-1/blob/main/assets/t2i_demo.gif?raw=true\\",\\"https://daviddinkevich.github.io/Story2Board/static/images/teaser.webp\\",\\"https://nxnai.github.io/Voost/static/images/teaser.jpg\\",\\"https://lg-li.github.io/pub-images/tooncomposer/workflow-cmp.jpg\\",\\"https://github.com/yejy53/Echo-4o/raw/master/assets/radar.jpg\\",\\"https://francis-rings.github.io/StableAvatar/static/images/framework.jpg\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"UniPic 2.0：一体化视觉理解与编辑升级模型","slug":"unipic-2-0-一体化视觉理解与编辑升级模型","link":"#unipic-2-0-一体化视觉理解与编辑升级模型","children":[]},{"level":2,"title":"NextStep-1：大规模连续 token 自回归图像生成模型","slug":"nextstep-1-大规模连续-token-自回归图像生成模型","link":"#nextstep-1-大规模连续-token-自回归图像生成模型","children":[]},{"level":2,"title":"Story2Board：训练免疫电影级故事板生成系统","slug":"story2board-训练免疫电影级故事板生成系统","link":"#story2board-训练免疫电影级故事板生成系统","children":[]},{"level":2,"title":"Voost：推出统一虚拟试穿与脱下框架","slug":"voost-推出统一虚拟试穿与脱下框架","link":"#voost-推出统一虚拟试穿与脱下框架","children":[]},{"level":2,"title":"ToonComposer：卡通制作一体化生成模型","slug":"tooncomposer-卡通制作一体化生成模型","link":"#tooncomposer-卡通制作一体化生成模型","children":[]},{"level":2,"title":"Echo-4o：多校联合打造的 GPT-4o 合成图像大数据与模型增强框架","slug":"echo-4o-多校联合打造的-gpt-4o-合成图像大数据与模型增强框架","link":"#echo-4o-多校联合打造的-gpt-4o-合成图像大数据与模型增强框架","children":[]},{"level":2,"title":"StableAvatar：无限长度音频驱动化身视频系统","slug":"stableavatar-无限长度音频驱动化身视频系统","link":"#stableavatar-无限长度音频驱动化身视频系统","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":7.04,"words":2111},"filePathRelative":"zh/posts/ai-weekly/051.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg\\" alt=\\"封面图源自C站作者Koal2\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面图源自C站作者Koal2</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：StepFun 推出 NextStep-1 优化大模型训练与推理；Skywork 发布 UniPic-2 强化跨模态生成与编辑；复旦/微软等发布 StableAvatar，实现高保真无限时长音频驱动人像视频。详见正文，相关参考链接请见文末。</p>","autoDesc":true}')}}]);