"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[277],{6262:(e,n)=>{n.A=(e,n)=>{const a=e.__vccOpts||e;for(const[e,i]of n)a[e]=i;return a}},3749:(e,n,a)=>{a.r(n),a.d(n,{comp:()=>t,data:()=>s});var i=a(641);const r={},t=(0,a(6262).A)(r,[["render",function(e,n){return(0,i.uX)(),(0,i.CE)("div",null,n[0]||(n[0]=[(0,i.Fv)('<h1 id="minimax-01扩展长文本处理-seaweed-apt一步视频生成-anydressing个性化虚拟试衣【ai周报】" tabindex="-1"><a class="header-anchor" href="#minimax-01扩展长文本处理-seaweed-apt一步视频生成-anydressing个性化虚拟试衣【ai周报】"><span>MiniMax-01扩展长文本处理|Seaweed-APT一步视频生成|AnyDressing个性化虚拟试衣【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/c6f924f2-97b3-45ab-94f6-0c29066798aa/anim=false,width=450/51311073.jpeg" alt="封面源自C站作者karlanan" tabindex="0" loading="lazy"><figcaption>封面源自C站作者karlanan</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：MiniMax-01采用Lightning Attention支持超长文本处理，提升多模态理解；Seaweed-APT通过对抗性后训练实现高分辨率视频的即时生成；AnyDressing利用定制化网络提供多服装虚拟试衣。详情见正文。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#minimax-01-%E6%89%A9%E5%B1%95%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E5%A4%84%E7%90%86%E8%83%BD%E5%8A%9B">MiniMax-01：扩展基础模型的长上下文处理能力</a></li><li><a href="#manganinja%E7%B2%BE%E7%A1%AE%E5%8F%82%E8%80%83%E5%BC%95%E5%AF%BC%E7%9A%84%E7%BA%BF%E7%A8%BF%E4%B8%8A%E8%89%B2">MangaNinja：精确参考引导的线稿上色</a></li><li><a href="#seaweed-apt%E4%B8%80%E6%AD%A5%E5%BC%8F%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E7%9A%84%E6%89%A9%E6%95%A3%E5%AF%B9%E6%8A%97%E5%90%8E%E8%AE%AD%E7%BB%83">Seaweed-APT：一步式视频生成的扩散对抗后训练</a></li><li><a href="#layeranimate%E5%8A%A8%E7%94%BB%E7%9A%84%E5%B1%82%E7%BA%A7%E6%8E%A7%E5%88%B6">LayerAnimate：动画的层级控制</a></li><li><a href="#anydressing%E5%8F%AF%E5%AE%9A%E5%88%B6%E7%9A%84%E5%A4%9A%E6%9C%8D%E8%A3%85%E8%99%9A%E6%8B%9F%E8%AF%95%E8%A1%A3%E6%A8%A1%E5%9E%8B">AnyDressing：可定制的多服装虚拟试衣模型</a></li><li><a href="#microdiffusion%E5%9F%BA%E4%BA%8E%E7%A8%80%E7%96%8Ftransformer%E7%9A%84%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95">MicroDiffusion：基于稀疏Transformer的扩散模型优化训练方法</a></li></ol><hr><h2 id="minimax-01-扩展基础模型的长上下文处理能力" tabindex="-1"><a class="header-anchor" href="#minimax-01-扩展基础模型的长上下文处理能力"><span>MiniMax-01：扩展基础模型的长上下文处理能力</span></a></h2><figure><img src="https://github.com/MiniMax-AI/MiniMax-01/raw/main/figures/TextBench.png" alt="MiniMax-01 Benchmark 图" tabindex="0" loading="lazy"><figcaption>MiniMax-01 Benchmark 图</figcaption></figure><p><strong>概要</strong>：<strong>MiniMax-01</strong> 系列模型由 <strong>MiniMax</strong> 公司推出，包括 <strong>MiniMax-Text-01</strong> 和 <strong>MiniMax-VL-01</strong>，旨在提供更长上下文处理能力。通过引入高效的 Lightning Attention 机制，这些模型在处理长达数百万标记的上下文时，仍能保持高效的训练和推理性能。其中，MiniMax-Text-01 在标准基准测试中表现出色，可在推理阶段处理多达 400 万标记的上下文。MiniMax-VL-01 则通过持续训练 5120 亿视觉语言标记，展示了卓越的多模态理解能力。实验结果表明，这些模型在性能上可与 GPT-4o 和 Claude-3.5-Sonnet 等顶级模型媲美，同时提供了长达 20-32 倍的上下文窗口。</p><p><strong>标签</strong>：#MiniMax-01 #MiniMax-Text-01 #MiniMax-VL-01 #LightningAttention #长上下文 #多模态理解</p><hr><h2 id="manganinja-精确参考引导的线稿上色" tabindex="-1"><a class="header-anchor" href="#manganinja-精确参考引导的线稿上色"><span>MangaNinja：精确参考引导的线稿上色</span></a></h2><figure><img src="https://github.com/ali-vilab/MangaNinjia/raw/main/docs/teaser.png" alt="MangaNinja Teaser 图" tabindex="0" loading="lazy"><figcaption>MangaNinja Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>MangaNinja</strong> 是由 <strong>香港大学</strong>、<strong>香港科技大学</strong>、<strong>同济实验室</strong> 和 <strong>蚂蚁集团</strong> 的研究者联合提出的一种基于参考的线稿上色方法，旨在实现精确的角色细节转移。该方法引入了两个关键设计：一个是<strong>Patch Shuffling 模块</strong>，用于在参考彩色图像和目标线稿之间建立对应关系；另一个是<strong>点驱动控制方案</strong>，允许用户进行细粒度的颜色匹配。实验结果表明，MangaNinja 在处理具有显著差异的参考图像和线稿时，能够保持细节的一致性，超越了现有的解决方案。此外，用户可以通过点控制实现更复杂的任务，如处理极端姿势和阴影、跨角色上色以及多参考图像的融合等。</p><p><strong>标签</strong>：#MangaNinja #线稿上色 #参考引导 #深度学习</p><hr><h2 id="seaweed-apt-一步式视频生成的扩散对抗后训练" tabindex="-1"><a class="header-anchor" href="#seaweed-apt-一步式视频生成的扩散对抗后训练"><span>Seaweed-APT：一步式视频生成的扩散对抗后训练</span></a></h2><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img9@main/2025/01/19/1737267344937-98f0e4dc-4c5d-4e9d-9c17-89aff8d84953.gif" alt="Seaweed-APT Demo 图" tabindex="0" loading="lazy"><figcaption>Seaweed-APT Demo 图</figcaption></figure><p><strong>概要</strong>：<strong>Seaweed-APT</strong> 是由 <strong>字节跳动</strong> 的研究团队提出的一种新颖的视频生成模型，旨在通过对抗性后训练（APT）技术，实现一步式高分辨率视频生成。传统的扩散模型在视频生成任务中需要多次神经网络评估，导致生成速度缓慢。Seaweed-APT 通过在扩散预训练后进行<strong>对抗性后训练</strong>，仅需一步推理即可生成高质量的视频。实验结果表明，该模型能够实时生成 2 秒、1280×720 分辨率、24 帧/秒的视频，同时在一步式图像生成任务中也表现出与当前最先进方法相当的质量。</p><p><strong>标签</strong>：#SeaweedAPT #一步式视频生成 #扩散模型 #对抗性后训练 #字节跳动</p><hr><h2 id="layeranimate-动画的层级控制" tabindex="-1"><a class="header-anchor" href="#layeranimate-动画的层级控制"><span>LayerAnimate：动画的层级控制</span></a></h2><figure><img src="https://layeranimate.github.io/resources/architecture.png" alt="LayerAnimate Architecture 图" tabindex="0" loading="lazy"><figcaption>LayerAnimate Architecture 图</figcaption></figure><p><strong>概要</strong>：<strong>LayerAnimate</strong> 是由 <strong>中国科学院自动化研究所</strong> 和 <strong>中国科学院大学</strong> 的研究人员提出的一种新颖的动画控制生成框架，旨在通过视频扩散模型对动画的各个层级进行精细控制。该方法允许用户独立操控前景和背景元素，提供冻结特定元素、使用部分草图动画角色以及切换静态或动态背景等功能。为了解决层级数据有限的问题，研究者设计了一个数据处理流程，包括自动元素分割、运动状态的层次合并以及运动一致性优化。实验结果表明，LayerAnimate 在动画质量、控制精度和可用性方面均优于现有方法，为专业动画师和业余爱好者提供了新的创作工具。</p><p><strong>标签</strong>：#LayerAnimate #动画生成 #层级控制 #视频扩散模型</p><hr><h2 id="anydressing-可定制的多服装虚拟试衣模型" tabindex="-1"><a class="header-anchor" href="#anydressing-可定制的多服装虚拟试衣模型"><span>AnyDressing：可定制的多服装虚拟试衣模型</span></a></h2><figure><img src="https://crayon-shinchan.github.io/AnyDressing/static/images/teaser.png" alt="AnyDressing Teaser 图" tabindex="0" loading="lazy"><figcaption>AnyDressing Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>AnyDressing</strong> 是由 <strong>字节跳动</strong> 和 <strong>清华大学</strong> 提出的虚拟试衣模型，可根据多服装和文本提示生成个性化人物图像。模型包括 <strong>GarmentsNet</strong> 和 <strong>DressingNet</strong> 两个核心模块，分别负责服装特征提取和多服装纹理融合。通过 <strong>Dressing Attention (DA)</strong> 和 <strong>Garment-Enhanced Texture Learning (GTL)</strong>，实现了高质量服装纹理表达和精准区域控制。实验结果表明，AnyDressing 在复杂服装试衣任务中表现优异实验结果表明，AnyDressing 在多种场景和复杂服装的虚拟试衣任务中表现出色，并且兼容 LoRA、ControlNet 和 FaceID 等插件，提升了合成图像的多样性和可控性。</p><p><strong>标签</strong>：#AnyDressing #虚拟试衣 #多服装 #生成模型 #字节跳动 #清华大学</p><hr><h2 id="microdiffusion-微预算下的大规模扩散模型训练" tabindex="-1"><a class="header-anchor" href="#microdiffusion-微预算下的大规模扩散模型训练"><span>MicroDiffusion：微预算下的大规模扩散模型训练</span></a></h2><figure><img src="https://github.com/SonyResearch/micro_diffusion/raw/main/assets/demo.jpg" alt="MicroDiffusion Demo 图" tabindex="0" loading="lazy"><figcaption>MicroDiffusion Demo 图</figcaption></figure><p><strong>概要</strong>：<strong>MicroDiffusion</strong> 是 <strong>索尼研究团队</strong> 提出的一种高效低成本的扩散模型训练方法。与传统直接训练扩散模型的方式不同，MicroDiffusion 通过训练一个<strong>稀疏 Transformer</strong> 来取代标准的扩散模型，显著降低了计算需求。模型设计上，采用<strong>稀疏注意力机制</strong>，在保留生成能力的同时减少参数数量，提高了内存利用效率。通过仅使用 3700 万张公开的真实与合成图像，该方法以极低的成本（约 1890 美元）完成了从头训练。实验结果显示，该模型在 COCO 数据集上的零样本生成任务中，达到了 12.7 的 FID 分数，尽管计算资源极其有限，却实现了与传统扩散模型媲美的生成质量。</p><p><strong>标签</strong>：#MicroDiffusion #扩散模型 #低预算 #COCO #图像生成</p><hr><h3 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span><strong>参考文献</strong></span></a></h3><ol><li><a href="https://github.com/MiniMax-AI/MiniMax-01" target="_blank" rel="noopener noreferrer">MiniMax-01 GitHub</a></li><li><a href="https://arxiv.org/pdf/2501.08313" target="_blank" rel="noopener noreferrer">MiniMax-01 论文</a></li><li><a href="https://huggingface.co/MiniMaxAI" target="_blank" rel="noopener noreferrer">MiniMax-01 Hugging Face</a></li><li><a href="https://johanan528.github.io/MangaNinjia/" target="_blank" rel="noopener noreferrer">MangaNinja 项目主页</a></li><li><a href="https://github.com/ali-vilab/MangaNinjia" target="_blank" rel="noopener noreferrer">MangaNinja GitHub</a></li><li><a href="https://arxiv.org/html/2501.08332v1" target="_blank" rel="noopener noreferrer">MangaNinja 论文</a></li><li><a href="https://seaweed-apt.com/" target="_blank" rel="noopener noreferrer">Seaweed-APT 官方页面</a></li><li><a href="https://arxiv.org/html/2501.08316" target="_blank" rel="noopener noreferrer">Seaweed-APT 论文</a></li><li><a href="https://layeranimate.github.io/" target="_blank" rel="noopener noreferrer">LayerAnimate 项目主页</a></li><li><a href="https://github.com/IamCreateAI/LayerAnimate" target="_blank" rel="noopener noreferrer">LayerAnimate GitHub</a></li><li><a href="https://arxiv.org/html/2501.08295v1" target="_blank" rel="noopener noreferrer">LayerAnimate 论文</a></li><li><a href="https://crayon-shinchan.github.io/AnyDressing/" target="_blank" rel="noopener noreferrer">AnyDressing 项目主页</a></li><li><a href="https://github.com/Crayon-Shinchan/AnyDressing" target="_blank" rel="noopener noreferrer">AnyDressing GitHub</a></li><li><a href="https://arxiv.org/html/2412.04146" target="_blank" rel="noopener noreferrer">AnyDressing 论文</a></li><li><a href="https://github.com/sonyresearch/micro_diffusion" target="_blank" rel="noopener noreferrer">MicroDiffusion GitHub</a></li><li><a href="https://arxiv.org/pdf/2407.15811" target="_blank" rel="noopener noreferrer">MicroDiffusion 论文</a></li></ol>',40)]))}]]),s=JSON.parse('{"path":"/zh/posts/ai-weekly/021.html","title":"MiniMax-01扩展长文本处理|Seaweed-APT一步视频生成|AnyDressing个性化虚拟试衣【AI周报】","lang":"zh-CN","frontmatter":{"description":"MiniMax-01扩展长文本处理|Seaweed-APT一步视频生成|AnyDressing个性化虚拟试衣【AI周报】 封面源自C站作者karlanan封面源自C站作者karlanan 摘要 本周亮点：MiniMax-01采用Lightning Attention支持超长文本处理，提升多模态理解；Seaweed-APT通过对抗性后训练实现高分辨率视频...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/021.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"MiniMax-01扩展长文本处理|Seaweed-APT一步视频生成|AnyDressing个性化虚拟试衣【AI周报】"}],["meta",{"property":"og:description","content":"MiniMax-01扩展长文本处理|Seaweed-APT一步视频生成|AnyDressing个性化虚拟试衣【AI周报】 封面源自C站作者karlanan封面源自C站作者karlanan 摘要 本周亮点：MiniMax-01采用Lightning Attention支持超长文本处理，提升多模态理解；Seaweed-APT通过对抗性后训练实现高分辨率视频..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/c6f924f2-97b3-45ab-94f6-0c29066798aa/anim=false,width=450/51311073.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"MiniMax-01扩展长文本处理|Seaweed-APT一步视频生成|AnyDressing个性化虚拟试衣【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/c6f924f2-97b3-45ab-94f6-0c29066798aa/anim=false,width=450/51311073.jpeg\\",\\"https://github.com/MiniMax-AI/MiniMax-01/raw/main/figures/TextBench.png\\",\\"https://github.com/ali-vilab/MangaNinjia/raw/main/docs/teaser.png\\",\\"https://fastly.jsdelivr.net/gh/bucketio/img9@main/2025/01/19/1737267344937-98f0e4dc-4c5d-4e9d-9c17-89aff8d84953.gif\\",\\"https://layeranimate.github.io/resources/architecture.png\\",\\"https://crayon-shinchan.github.io/AnyDressing/static/images/teaser.png\\",\\"https://github.com/SonyResearch/micro_diffusion/raw/main/assets/demo.jpg\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"MiniMax-01：扩展基础模型的长上下文处理能力","slug":"minimax-01-扩展基础模型的长上下文处理能力","link":"#minimax-01-扩展基础模型的长上下文处理能力","children":[]},{"level":2,"title":"MangaNinja：精确参考引导的线稿上色","slug":"manganinja-精确参考引导的线稿上色","link":"#manganinja-精确参考引导的线稿上色","children":[]},{"level":2,"title":"Seaweed-APT：一步式视频生成的扩散对抗后训练","slug":"seaweed-apt-一步式视频生成的扩散对抗后训练","link":"#seaweed-apt-一步式视频生成的扩散对抗后训练","children":[]},{"level":2,"title":"LayerAnimate：动画的层级控制","slug":"layeranimate-动画的层级控制","link":"#layeranimate-动画的层级控制","children":[]},{"level":2,"title":"AnyDressing：可定制的多服装虚拟试衣模型","slug":"anydressing-可定制的多服装虚拟试衣模型","link":"#anydressing-可定制的多服装虚拟试衣模型","children":[]},{"level":2,"title":"MicroDiffusion：微预算下的大规模扩散模型训练","slug":"microdiffusion-微预算下的大规模扩散模型训练","link":"#microdiffusion-微预算下的大规模扩散模型训练","children":[{"level":3,"title":"参考文献","slug":"参考文献","link":"#参考文献","children":[]}]}],"readingTime":{"minutes":6.16,"words":1848},"filePathRelative":"zh/posts/ai-weekly/021.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/c6f924f2-97b3-45ab-94f6-0c29066798aa/anim=false,width=450/51311073.jpeg\\" alt=\\"封面源自C站作者karlanan\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面源自C站作者karlanan</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：MiniMax-01采用Lightning Attention支持超长文本处理，提升多模态理解；Seaweed-APT通过对抗性后训练实现高分辨率视频的即时生成；AnyDressing利用定制化网络提供多服装虚拟试衣。详情见正文。</p>","autoDesc":true}')}}]);