"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[6645],{66262:(a,e)=>{e.A=(a,e)=>{const t=a.__vccOpts||a;for(const[a,r]of e)t[a]=r;return t}},48700:(a,e,t)=>{t.r(e),t.d(e,{comp:()=>n,data:()=>l});var r=t(20641);const i={},n=(0,t(66262).A)(i,[["render",function(a,e){return(0,r.uX)(),(0,r.CE)("div",null,e[0]||(e[0]=[(0,r.Fv)('<h1 id="【论文精读】framepack-在下一帧预测视频生成模型中打包输入帧上下文" tabindex="-1"><a class="header-anchor" href="#【论文精读】framepack-在下一帧预测视频生成模型中打包输入帧上下文"><span>【论文精读】FramePack：在下一帧预测视频生成模型中打包输入帧上下文</span></a></h1><p align="center"><img src="https://lllyasviel.github.io/frame_pack_gitpage/img/logo.png" width="200"></p><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>FramePack由斯坦福大学张吕敏等提出，是一种输入预处理模块，可无缝集成到主流视频扩散模型（如混元视频模型），通过自适应帧压缩和反漂移采样，有效提升长时序一致性和生成质量，支持13B模型在6GB显存上流畅生成长视频，显著降低算力门槛。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li><li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li><li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li><li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li></ol><hr><h2 id="背景与研究目标" tabindex="-1"><a class="header-anchor" href="#背景与研究目标"><span>背景与研究目标</span></a></h2><p>视频生成领域，基于&quot;下一帧预测&quot;的扩散模型因其逐帧生成能力受到关注。但此类模型普遍面临两大核心挑战：</p><ul><li><strong>遗忘（Forgetting）</strong>：模型难以长期保持时序一致性，导致角色、场景等随时间漂移。</li><li><strong>漂移（Drifting）</strong>：误差逐帧累积，后续帧画质下降，出现模糊、失真等问题。</li></ul><p>传统方法往往顾此失彼：增加历史帧输入可缓解遗忘，却加剧漂移；减少依赖则反之。加之 Transformer 架构的计算复杂度，直接扩展上下文帧数并不可行。</p><hr><h2 id="方法与创新点" tabindex="-1"><a class="header-anchor" href="#方法与创新点"><span>方法与创新点</span></a></h2><p>FramePack 作为输入端的预处理模块，作用于历史帧输入阶段。在不改变主干网络结构的前提下，通过以下两大创新提升视频生成效果：</p><p>FramePack首先对所有历史帧进行压缩和重排，将处理后的帧拼接为统一长度的上下文序列，作为主干模型的输入。</p><h3 id="_1-自适应帧压缩-adaptive-frame-compression" tabindex="-1"><a class="header-anchor" href="#_1-自适应帧压缩-adaptive-frame-compression"><span>1. 自适应帧压缩（Adaptive Frame Compression）</span></a></h3><p>FramePack 利用视频帧间冗余，对不同时间距离的帧采用不同压缩率，远帧高压缩，近帧低压缩。通过调整 Transformer 输入层 patchify kernel size，实现帧级别分辨率动态分配，保证总上下文长度受控。</p><p align="center"><img src="https://lllyasviel.github.io/frame_pack_gitpage/img/nfp.png" width="600"></p><p align="center"><em>多帧输入的patchify压缩示意，近帧高分辨率，远帧低分辨率。</em></p><p>FramePack 通过对历史帧进行分级压缩，严格控制输入上下文长度。具体地，对第 $i$ 个历史帧，压缩后长度为： $\\phi(F_i) = \\frac{L_f}{\\lambda^i}$ 其中 $L_f$ 为单帧 patch 数，$\\lambda&gt;1$ 为压缩率。总上下文长度为：</p><p>$L = S \\cdot L_f + L_f \\cdot \\sum_{i=0}^{T-1} \\frac{1}{\\lambda^i} = S \\cdot L_f + L_f \\cdot \\frac{1-1/\\lambda^T}{1-1/\\lambda}$</p><p>当 $T\\to\\infty$ 时，上下文长度收敛到 $L = (S + \\frac{\\lambda}{\\lambda-1}) L_f$ 这保证了无论历史帧数多大，计算量都不会爆炸。</p><p>不同帧可采用不同的压缩调度策略（FramePack Scheduling），如几何级压缩、均匀压缩等，灵活适配不同场景和任务需求。</p><p align="center"><img src="https://lllyasviel.github.io/frame_pack_gitpage/img/ab.png" width="600"></p><p align="center"><em>多种帧压缩调度方式，支持灵活分配算力资源。</em></p><p>不同压缩率的 patchify kernel（如(2,4,4)、(4,8,8)等）对应独立的输入投影层参数，训练时通过插值初始化，保证不同压缩率下特征分布稳定。</p><p>对于极长视频，尾部帧可采用三种处理方式：直接删除、每帧增加一个像素、全局池化。不同压缩率下的 RoPE（Rotary Position Embedding）需通过平均池化对齐，确保时序信息一致。</p><h3 id="_2-反漂移采样-anti-drifting-sampling" tabindex="-1"><a class="header-anchor" href="#_2-反漂移采样-anti-drifting-sampling"><span>2. 反漂移采样（Anti-Drifting Sampling）</span></a></h3><p>FramePack 提出创新采样策略：</p><ul><li><strong>Vanilla</strong>：传统顺序采样，误差易累积。</li><li><strong>Anti-Drifting</strong>：先生成端点帧，再补中间帧，利用双向上下文提升一致性。</li><li><strong>Inverted Anti-Drifting</strong>：逆序采样，适合 image-to-video，能更好保持高质量起始帧特征。</li></ul><p align="center"><img src="https://lllyasviel.github.io/frame_pack_gitpage/img/sample.png" width="600"></p><p align="center"><em>不同采样方式对比，反漂移采样显著降低误差累积。</em></p><p>FramePack 支持 vanilla、anti-drifting、inverted anti-drifting 三种采样顺序。后两者通过双向上下文有效抑制漂移。为支持非连续帧采样，RoPE 位置编码需做随机访问调整。</p><hr><h2 id="实验与结果分析" tabindex="-1"><a class="header-anchor" href="#实验与结果分析"><span>实验与结果分析</span></a></h2><ul><li><strong>兼容性</strong>：FramePack 可直接微调 HunyuanVideo、Wan 等主流视频扩散模型。</li><li><strong>性能提升</strong>： <ul><li><strong>漂移抑制</strong>：Inverted Anti-Drifting 采样在多项指标和主观评价中表现最佳。</li><li><strong>画质提升</strong>：支持更大 batch size，训练更高效，生成视频更长且一致性更好。</li><li><strong>效率提升</strong>：6GB 显存即可驱动 13B 模型生成 1 分钟视频，极大降低硬件门槛。</li></ul></li></ul><h3 id="多维度评测指标" tabindex="-1"><a class="header-anchor" href="#多维度评测指标"><span>多维度评测指标</span></a></h3><p>评测指标包括清晰度（MUSIQ）、美学（LAION）、运动（VFI）、动态（RAFT）、语义一致性（ViCLIP）、解剖结构（ViT）、身份一致性（ArcFace+RetinaFace）等。</p><h3 id="漂移度量" tabindex="-1"><a class="header-anchor" href="#漂移度量"><span>漂移度量</span></a></h3><p>论文提出&quot;start-end contrast&quot;指标，度量视频首尾在各项指标上的差异，直接反映漂移严重程度。 $\\Delta_{\\text{drift}}^{M}(V) = |M(V_{\\text{start}}) - M(V_{\\text{end}})|$</p><h3 id="与主流方法对比" tabindex="-1"><a class="header-anchor" href="#与主流方法对比"><span>与主流方法对比</span></a></h3><p>FramePack 在与 anchor frame、causal attention、noisy history、history guidance 等主流方案对比中，在多项指标和人类评价中均表现最佳。</p><h3 id="消融实验" tabindex="-1"><a class="header-anchor" href="#消融实验"><span>消融实验</span></a></h3><p>消融实验采用统一命名规范（如 td_f16k4f4k2f1k1_g9），明确每种压缩调度与采样方式，便于复现和对比。不同压缩策略和采样方法对性能影响显著，最佳配置为几何级压缩+反漂移采样。</p><hr><h2 id="模型启发与方法延伸" tabindex="-1"><a class="header-anchor" href="#模型启发与方法延伸"><span>模型启发与方法延伸</span></a></h2><ul><li><strong>通用性</strong>：FramePack 作为输入预处理模块，可迁移至各类时序生成任务。</li><li><strong>与现有方法对比</strong>：相比单纯增加上下文或改进主干结构，FramePack以更低成本实现更优时序一致性。</li><li><strong>未来方向</strong>：可与更高效注意力机制、长时序建模方法结合，进一步提升超长视频生成能力。</li></ul><hr><h2 id="结论与未来展望" tabindex="-1"><a class="header-anchor" href="#结论与未来展望"><span>结论与未来展望</span></a></h2><p>FramePack 有效打破了“遗忘-漂移”二难困境，为视频生成模型带来更长时序、更高一致性和更低算力门槛。其输入端插件式设计，支持多种压缩调度和采样变体，适配不同场景和需求，便于集成到各类主流视频扩散模型。未来，FramePack 可与更高效注意力机制、长时序建模方法结合，进一步提升超长视频生成能力，并有望在内容创作、虚拟现实等实际应用中发挥更大作用。</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h3><ol><li><a href="https://arxiv.org/abs/2504.12626" target="_blank" rel="noopener noreferrer">FramePack 论文</a></li><li><a href="https://lllyasviel.github.io/frame_pack_gitpage/" target="_blank" rel="noopener noreferrer">FramePack 项目主页</a></li><li><a href="https://github.com/lllyasviel/FramePack" target="_blank" rel="noopener noreferrer">FramePack GitHub</a></li><li><a href="https://www.alphaxiv.org/overview/2504.12626" target="_blank" rel="noopener noreferrer">alphaXiv 博客原文</a></li></ol>',54)]))}]]),l=JSON.parse('{"path":"/zh/posts/papers/framepack.html","title":"【论文精读】FramePack：在下一帧预测视频生成模型中打包输入帧上下文","lang":"zh-CN","frontmatter":{"title":"【论文精读】FramePack：在下一帧预测视频生成模型中打包输入帧上下文","icon":"material-symbols:screenshot-frame-2","cover":"/assets/images/papers/framepack/teaser.png","date":"2025-04-26T00:00:00.000Z","category":["视频生成","论文精读","张吕敏"],"tag":["FramePack","视频生成","扩散模型","输入预处理"],"author":"Lvmin Zhang, Maneesh Agrawala","head":[["meta",{"name":"keywords","content":"FramePack, 视频生成, 输入预处理, 扩散模型, 论文精读, 张吕敏, Lvmin Zhang, Maneesh Agrawala"}],["meta",{"name":"description","content":"FramePack是一种创新的视频生成输入预处理模块，通过自适应帧压缩与反漂移采样，提升长时序一致性和生成质量，可无缝集成到主流视频扩散模型。"}],["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/papers/framepack.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"【论文精读】FramePack：在下一帧预测视频生成模型中打包输入帧上下文"}],["meta",{"property":"og:description","content":"【论文精读】FramePack：在下一帧预测视频生成模型中打包输入帧上下文 摘要 FramePack由斯坦福大学张吕敏等提出，是一种输入预处理模块，可无缝集成到主流视频扩散模型（如混元视频模型），通过自适应帧压缩和反漂移采样，有效提升长时序一致性和生成质量，支持13B模型在6GB显存上流畅生成长视频，显著降低算力门槛。 目录 背景与研究目标 方法与创新..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://neverbiasu.github.io/assets/images/papers/framepack/teaser.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://neverbiasu.github.io/assets/images/papers/framepack/teaser.png"}],["meta",{"name":"twitter:image:alt","content":"【论文精读】FramePack：在下一帧预测视频生成模型中打包输入帧上下文"}],["meta",{"property":"article:author","content":"Lvmin Zhang, Maneesh Agrawala"}],["meta",{"property":"article:tag","content":"FramePack"}],["meta",{"property":"article:tag","content":"视频生成"}],["meta",{"property":"article:tag","content":"扩散模型"}],["meta",{"property":"article:tag","content":"输入预处理"}],["meta",{"property":"article:published_time","content":"2025-04-26T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"【论文精读】FramePack：在下一帧预测视频生成模型中打包输入帧上下文\\",\\"image\\":[\\"https://neverbiasu.github.io/assets/images/papers/framepack/teaser.png\\"],\\"datePublished\\":\\"2025-04-26T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Lvmin Zhang, Maneesh Agrawala\\"}]}"]],"description":"【论文精读】FramePack：在下一帧预测视频生成模型中打包输入帧上下文 摘要 FramePack由斯坦福大学张吕敏等提出，是一种输入预处理模块，可无缝集成到主流视频扩散模型（如混元视频模型），通过自适应帧压缩和反漂移采样，有效提升长时序一致性和生成质量，支持13B模型在6GB显存上流畅生成长视频，显著降低算力门槛。 目录 背景与研究目标 方法与创新...","gitInclude":[]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"背景与研究目标","slug":"背景与研究目标","link":"#背景与研究目标","children":[]},{"level":2,"title":"方法与创新点","slug":"方法与创新点","link":"#方法与创新点","children":[{"level":3,"title":"1. 自适应帧压缩（Adaptive Frame Compression）","slug":"_1-自适应帧压缩-adaptive-frame-compression","link":"#_1-自适应帧压缩-adaptive-frame-compression","children":[]},{"level":3,"title":"2. 反漂移采样（Anti-Drifting Sampling）","slug":"_2-反漂移采样-anti-drifting-sampling","link":"#_2-反漂移采样-anti-drifting-sampling","children":[]}]},{"level":2,"title":"实验与结果分析","slug":"实验与结果分析","link":"#实验与结果分析","children":[{"level":3,"title":"多维度评测指标","slug":"多维度评测指标","link":"#多维度评测指标","children":[]},{"level":3,"title":"漂移度量","slug":"漂移度量","link":"#漂移度量","children":[]},{"level":3,"title":"与主流方法对比","slug":"与主流方法对比","link":"#与主流方法对比","children":[]},{"level":3,"title":"消融实验","slug":"消融实验","link":"#消融实验","children":[]}]},{"level":2,"title":"模型启发与方法延伸","slug":"模型启发与方法延伸","link":"#模型启发与方法延伸","children":[]},{"level":2,"title":"结论与未来展望","slug":"结论与未来展望","link":"#结论与未来展望","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":6.22,"words":1865},"filePathRelative":"zh/posts/papers/framepack.md","localizedDate":"2025年4月26日","excerpt":"\\n<p align=\\"center\\">\\n        <img src=\\"https://lllyasviel.github.io/frame_pack_gitpage/img/logo.png\\" width=\\"200\\">\\n</p>\\n<h2>摘要</h2>\\n<p>FramePack由斯坦福大学张吕敏等提出，是一种输入预处理模块，可无缝集成到主流视频扩散模型（如混元视频模型），通过自适应帧压缩和反漂移采样，有效提升长时序一致性和生成质量，支持13B模型在6GB显存上流畅生成长视频，显著降低算力门槛。</p>\\n<hr>\\n<h2>目录</h2>\\n<ol>\\n<li><a href=\\"#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87\\">背景与研究目标</a></li>\\n<li><a href=\\"#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9\\">方法与创新点</a></li>\\n<li><a href=\\"#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90\\">实验与结果分析</a></li>\\n<li><a href=\\"#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8\\">模型启发与方法延伸</a></li>\\n<li><a href=\\"#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B\\">结论与未来展望</a></li>\\n</ol>","autoDesc":true}')}}]);