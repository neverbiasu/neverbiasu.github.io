"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[4892],{66262:(e,n)=>{n.A=(e,n)=>{const r=e.__vccOpts||e;for(const[e,a]of n)r[e]=a;return r}},38291:(e,n,r)=>{r.r(n),r.d(n,{comp:()=>i,data:()=>o});var a=r(20641);const t={},i=(0,r(66262).A)(t,[["render",function(e,n){return(0,a.uX)(),(0,a.CE)("div",null,n[0]||(n[0]=[(0,a.Fv)('<h1 id="多模态sota-internvl-3-5-统一风格生成-uso-真实感角色动画-omnihuman-1-5【ai周报】" tabindex="-1"><a class="header-anchor" href="#多模态sota-internvl-3-5-统一风格生成-uso-真实感角色动画-omnihuman-1-5【ai周报】"><span>多模态SOTA InternVL 3.5 | 统一风格生成 USO | 真实感角色动画 OmniHuman-1.5【AI周报】</span></a></h1><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：OpenGVLab 开源了性能媲美商业模型的 InternVL 3.5 多模态模型系列；微软发布了 VibeVoice，一个能生成长达90分钟对话音频的开源TTS模型；字节跳动推出 OmniHuman-1.5，可从单张图片和音轨生成富有表现力的角色动画。详见正文，相关参考链接请见文末。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#InternVL-3.5">InternVL 3.5：性能媲美 GPT-4o 的开源多模态模型</a></li><li><a href="#VibeVoice">VibeVoice：微软开源的长篇对话式音频生成模型</a></li><li><a href="#USO">USO：字节跳动发布统一风格和主题的图像生成模型</a></li><li><a href="#OmniHuman-1.5">OmniHuman-1.5：字节跳动推出富有表现力的角色动画生成模型</a></li><li><a href="#MV-RAG">MV-RAG：检索增强的文本到3D生成，攻克域外概念难题</a></li><li><a href="#T2I-ReasonBench">T2I-ReasonBench：评估文本到图像模型推理能力的新基准</a></li></ol><hr><h2 id="internvl-3-5-性能媲美-gpt-4o-的开源多模态模型" tabindex="-1"><a class="header-anchor" href="#internvl-3-5-性能媲美-gpt-4o-的开源多模态模型"><span>InternVL 3.5：性能媲美 GPT-4o 的开源多模态模型</span></a></h2><figure><img src="https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance.jpg" alt="InternVL 3.5 Performance 图" tabindex="0" loading="lazy"><figcaption>InternVL 3.5 Performance 图</figcaption></figure><p><strong>概要</strong>：<strong>OpenGVLab</strong> 发布了 <strong>InternVL 3.5</strong>，一个全新的开源多模态模型系列。该系列模型在多功能性、推理能力和推理效率方面均有显著提升，其最强大的版本 InternVL3.5-241B-A28B 在通用多模态、推理、文本和智能体任务上达到了开源模型的顶尖水平，性能可与 GPT-4o 等商业模型相媲美。此外，团队还提供了更小、更高效的模型版本，以适应不同应用场景的需求。</p><p><strong>标签</strong>：#多模态 #大语言模型 #开源 #InternVL #OpenGVLab</p><hr><h2 id="vibevoice-微软开源的长篇对话式音频生成模型" tabindex="-1"><a class="header-anchor" href="#vibevoice-微软开源的长篇对话式音频生成模型"><span>VibeVoice：微软开源的长篇对话式音频生成模型</span></a></h2><figure><img src="https://huggingface.co/microsoft/VibeVoice-1.5B/resolve/main/figures/Fig1.png" alt="VibeVoice Evaluation 图" tabindex="0" loading="lazy"><figcaption>VibeVoice Evaluation 图</figcaption></figure><p><strong>概要</strong>：<strong>微软</strong>发布了 <strong>VibeVoice</strong>，一个前沿的开源文本转语音（TTS）模型，专为生成富有表现力的长篇多说话人对话音频（如播客）而设计。该模型能够生成长达90分钟、包含多达4个不同说话人的音频，解决了传统TTS系统在可扩展性、说话人一致性和自然轮换方面的挑战。</p><p><strong>标签</strong>：#文本转语音 #TTS #开源 #Microsoft #VibeVoice</p><hr><h2 id="uso-字节跳动发布统一风格和主题的图像生成模型" tabindex="-1"><a class="header-anchor" href="#uso-字节跳动发布统一风格和主题的图像生成模型"><span>USO：字节跳动发布统一风格和主题的图像生成模型</span></a></h2><figure><img src="https://github.com/bytedance/USO/raw/main/assets/teaser.webp" alt="USO Teaser 图" tabindex="0" loading="lazy"><figcaption>USO Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>字节跳动</strong> 的 <strong>UXO</strong> 团队发布了 <strong>USO</strong>（Unified Style-Subject Optimized），一个统一了风格驱动和主题驱动的图像生成模型。USO 能够将任意主题与任意风格自由结合，同时保持主题和风格的高度一致性与保真度。该模型采用解耦学习方案和风格奖励学习范式，实现了强大的生成效果。</p><p><strong>标签</strong>：#图像生成 #风格迁移 #主题保留 #ByteDance #USO</p><hr><h2 id="omnihuman-1-5-字节跳动推出富有表现力的角色动画生成模型" tabindex="-1"><a class="header-anchor" href="#omnihuman-1-5-字节跳动推出富有表现力的角色动画生成模型"><span>OmniHuman-1.5：字节跳动推出富有表现力的角色动画生成模型</span></a></h2><figure><img src="https://arxiv.org/html/2508.19209v1/figs/teaser.jpg" alt="OmniHuman-1.5 Teaser 图" tabindex="0" loading="lazy"><figcaption>OmniHuman-1.5 Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>字节跳动智能创作</strong>团队发布了 <strong>OmniHuman-1.5</strong>，该模型能仅通过一张图片和一段音轨，生成富有表现力的长篇角色动画。其独特的“双系统”架构，结合了多模态大语言模型（MLLM）的规划能力和扩散变换器的快速反应能力，可生成超过一分钟的包含动态运动和复杂多角色互动的视频。</p><p><strong>标签</strong>：#角色动画 #音频驱动 #多模态 #ByteDance #OmniHuman</p><hr><h2 id="mv-rag-检索增强的文本到3d生成-攻克域外概念难题" tabindex="-1"><a class="header-anchor" href="#mv-rag-检索增强的文本到3d生成-攻克域外概念难题"><span>MV-RAG：检索增强的文本到3D生成，攻克域外概念难题</span></a></h2><figure><img src="https://camo.githubusercontent.com/f0991ba8b1eb5b1bab2d8dcfcfcdc6c58edec774d58dd8851e4f6f7c48a8bcaf/68747470733a2f2f796f736566646179616e692e6769746875622e696f2f4d562d5241472f7374617469632f696d616765732f7465617365722e6a7067" alt="MV-RAG Teaser 图" tabindex="0" loading="lazy"><figcaption>MV-RAG Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>耶路撒冷希伯来大学</strong>（The Hebrew University of Jerusalem）的研究人员提出了 <strong>MV-RAG</strong>，一个新颖的文本到3D生成管线。该方法利用检索增强生成（RAG）技术，从大型2D图像数据库中检索相关图像，并将其作为条件输入到多视图扩散模型中，从而为域外（out-of-domain）或罕见的概念生成一致且准确的3D模型。</p><p><strong>标签</strong>：#文本到3D #检索增强生成 #RAG #3D建模 #OOD</p><hr><h2 id="t2i-reasonbench-评估文本到图像模型推理能力的新基准" tabindex="-1"><a class="header-anchor" href="#t2i-reasonbench-评估文本到图像模型推理能力的新基准"><span>T2I-ReasonBench：评估文本到图像模型推理能力的新基准</span></a></h2><figure><img src="https://github.com/KaiyueSun98/T2I-ReasonBench/raw/main/asset/teaser.png" alt="T2I-ReasonBench Teaser 图" tabindex="0" loading="lazy"><figcaption>T2I-ReasonBench Teaser 图</figcaption></figure><p><strong>概要</strong>：来自<strong>香港大学</strong>和<strong>香港中文大学</strong>的研究人员推出了 <strong>T2I-ReasonBench</strong>，这是一个旨在探索文本到图像（T2I）模型推理能力边界的新型基准。该基准包含800个精心设计的提示，分为成语解读、文本图像设计、实体推理和科学推理四个维度，对模型的潜在含义推断、领域知识整合和上下文歧义解决能力提出了挑战。</p><p><strong>标签</strong>：#基准测试 #文本到图像 #T2I #模型评测 #Reasoning</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://chat.intern-ai.org.cn/" target="_blank" rel="noopener noreferrer">InternVL 3.5 项目主页</a></li><li><a href="https://github.com/OpenGVLab/InternVL" target="_blank" rel="noopener noreferrer">InternVL 3.5 GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2508.18265v2" target="_blank" rel="noopener noreferrer">InternVL 3.5 论文</a></li><li><a href="https://microsoft.github.io/VibeVoice/" target="_blank" rel="noopener noreferrer">VibeVoice 项目主页</a></li><li><a href="https://github.com/microsoft/VibeVoice" target="_blank" rel="noopener noreferrer">VibeVoice GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2508.19205v1" target="_blank" rel="noopener noreferrer">VibeVoice 论文</a></li><li><a href="https://huggingface.co/microsoft/VibeVoice-1.5B" target="_blank" rel="noopener noreferrer">VibeVoice Hugging Face</a></li><li><a href="https://bytedance.github.io/USO/" target="_blank" rel="noopener noreferrer">USO 项目主页</a></li><li><a href="https://github.com/bytedance/USO" target="_blank" rel="noopener noreferrer">USO GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2508.18966v1" target="_blank" rel="noopener noreferrer">USO 论文</a></li><li><a href="https://omnihuman-lab.github.io/v1_5/" target="_blank" rel="noopener noreferrer">OmniHuman-1.5 项目主页</a></li><li><a href="https://arxiv.org/html/2508.19209v1" target="_blank" rel="noopener noreferrer">OmniHuman-1.5 论文</a></li><li><a href="https://yosefdayani.github.io/MV-RAG/" target="_blank" rel="noopener noreferrer">MV-RAG 项目主页</a></li><li><a href="https://github.com/yosefdayani/MV-RAG" target="_blank" rel="noopener noreferrer">MV-RAG GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2508.16577v1" target="_blank" rel="noopener noreferrer">MV-RAG 论文</a></li><li><a href="https://github.com/KaiyueSun98/T2I-ReasonBench" target="_blank" rel="noopener noreferrer">T2I-ReasonBench GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2508.17472v1" target="_blank" rel="noopener noreferrer">T2I-ReasonBench 论文</a></li></ol>',39)]))}]]),o=JSON.parse('{"path":"/zh/posts/ai-weekly/053.html","title":"多模态SOTA InternVL 3.5 | 统一风格生成 USO | 真实感角色动画 OmniHuman-1.5【AI周报】","lang":"zh-CN","frontmatter":{"description":"多模态SOTA InternVL 3.5 | 统一风格生成 USO | 真实感角色动画 OmniHuman-1.5【AI周报】 摘要 本周亮点：OpenGVLab 开源了性能媲美商业模型的 InternVL 3.5 多模态模型系列；微软发布了 VibeVoice，一个能生成长达90分钟对话音频的开源TTS模型；字节跳动推出 OmniHuman-1.5，...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/053.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"多模态SOTA InternVL 3.5 | 统一风格生成 USO | 真实感角色动画 OmniHuman-1.5【AI周报】"}],["meta",{"property":"og:description","content":"多模态SOTA InternVL 3.5 | 统一风格生成 USO | 真实感角色动画 OmniHuman-1.5【AI周报】 摘要 本周亮点：OpenGVLab 开源了性能媲美商业模型的 InternVL 3.5 多模态模型系列；微软发布了 VibeVoice，一个能生成长达90分钟对话音频的开源TTS模型；字节跳动推出 OmniHuman-1.5，..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance.jpg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"多模态SOTA InternVL 3.5 | 统一风格生成 USO | 真实感角色动画 OmniHuman-1.5【AI周报】\\",\\"image\\":[\\"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance.jpg\\",\\"https://huggingface.co/microsoft/VibeVoice-1.5B/resolve/main/figures/Fig1.png\\",\\"https://github.com/bytedance/USO/raw/main/assets/teaser.webp\\",\\"https://arxiv.org/html/2508.19209v1/figs/teaser.jpg\\",\\"https://camo.githubusercontent.com/f0991ba8b1eb5b1bab2d8dcfcfcdc6c58edec774d58dd8851e4f6f7c48a8bcaf/68747470733a2f2f796f736566646179616e692e6769746875622e696f2f4d562d5241472f7374617469632f696d616765732f7465617365722e6a7067\\",\\"https://github.com/KaiyueSun98/T2I-ReasonBench/raw/main/asset/teaser.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"InternVL 3.5：性能媲美 GPT-4o 的开源多模态模型","slug":"internvl-3-5-性能媲美-gpt-4o-的开源多模态模型","link":"#internvl-3-5-性能媲美-gpt-4o-的开源多模态模型","children":[]},{"level":2,"title":"VibeVoice：微软开源的长篇对话式音频生成模型","slug":"vibevoice-微软开源的长篇对话式音频生成模型","link":"#vibevoice-微软开源的长篇对话式音频生成模型","children":[]},{"level":2,"title":"USO：字节跳动发布统一风格和主题的图像生成模型","slug":"uso-字节跳动发布统一风格和主题的图像生成模型","link":"#uso-字节跳动发布统一风格和主题的图像生成模型","children":[]},{"level":2,"title":"OmniHuman-1.5：字节跳动推出富有表现力的角色动画生成模型","slug":"omnihuman-1-5-字节跳动推出富有表现力的角色动画生成模型","link":"#omnihuman-1-5-字节跳动推出富有表现力的角色动画生成模型","children":[]},{"level":2,"title":"MV-RAG：检索增强的文本到3D生成，攻克域外概念难题","slug":"mv-rag-检索增强的文本到3d生成-攻克域外概念难题","link":"#mv-rag-检索增强的文本到3d生成-攻克域外概念难题","children":[]},{"level":2,"title":"T2I-ReasonBench：评估文本到图像模型推理能力的新基准","slug":"t2i-reasonbench-评估文本到图像模型推理能力的新基准","link":"#t2i-reasonbench-评估文本到图像模型推理能力的新基准","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":4.42,"words":1325},"filePathRelative":"zh/posts/ai-weekly/053.md","excerpt":"\\n<h2>摘要</h2>\\n<p>本周亮点：OpenGVLab 开源了性能媲美商业模型的 InternVL 3.5 多模态模型系列；微软发布了 VibeVoice，一个能生成长达90分钟对话音频的开源TTS模型；字节跳动推出 OmniHuman-1.5，可从单张图片和音轨生成富有表现力的角色动画。详见正文，相关参考链接请见文末。</p>\\n<hr>\\n<h2>目录</h2>\\n<ol>\\n<li><a href=\\"#InternVL-3.5\\">InternVL 3.5：性能媲美 GPT-4o 的开源多模态模型</a></li>\\n<li><a href=\\"#VibeVoice\\">VibeVoice：微软开源的长篇对话式音频生成模型</a></li>\\n<li><a href=\\"#USO\\">USO：字节跳动发布统一风格和主题的图像生成模型</a></li>\\n<li><a href=\\"#OmniHuman-1.5\\">OmniHuman-1.5：字节跳动推出富有表现力的角色动画生成模型</a></li>\\n<li><a href=\\"#MV-RAG\\">MV-RAG：检索增强的文本到3D生成，攻克域外概念难题</a></li>\\n<li><a href=\\"#T2I-ReasonBench\\">T2I-ReasonBench：评估文本到图像模型推理能力的新基准</a></li>\\n</ol>","autoDesc":true}')}}]);