"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[9957],{66262:(e,r)=>{r.A=(e,r)=>{const t=e.__vccOpts||e;for(const[e,a]of r)t[e]=a;return t}},87242:(e,r,t)=>{t.r(r),t.d(r,{comp:()=>i,data:()=>l});var a=t(20641);const n={},i=(0,t(66262).A)(n,[["render",function(e,r){return(0,a.uX)(),(0,a.CE)("div",null,r[0]||(r[0]=[(0,a.Fv)('<h1 id="qwen2-5定义多模态模型新高度-deepseek-v3突破混合专家能力-mulberry强化推理反思【ai周报】" tabindex="-1"><a class="header-anchor" href="#qwen2-5定义多模态模型新高度-deepseek-v3突破混合专家能力-mulberry强化推理反思【ai周报】"><span>Qwen2.5定义多模态模型新高度|DeepSeek-V3突破混合专家能力|Mulberry强化推理反思【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/b338b22c-394c-4ae0-88d2-2bf15fa83809/original=true,quality=90/47422369.jpeg" alt="封面源自C站作者Meower2024" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Meower2024</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周聚焦大模型与多模态：Qwen2.5 优化预训练与后训练，定义多模态模型新标准；DeepSeek-V3 引入混合专家架构，提升训练效率；Mulberry 结合蒙特卡罗树搜索，增强推理与反思能力。详见正文。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#Qwen2.5%EF%BC%9A%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E8%BE%BE%E6%91%A9%E9%99%A2%E6%8E%A8%E5%87%BA%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B">Qwen2.5：阿里巴巴达摩院推出的多模态大模型</a></li><li><a href="#DepthLab%EF%BC%9A%E4%BB%8E%E9%83%A8%E5%88%86%E5%88%B0%E5%AE%8C%E6%95%B4%E7%9A%84%E6%B7%B1%E5%BA%A6%E8%A1%A5%E5%85%A8%E6%A8%A1%E5%9E%8B">DepthLab：从部分到完整的深度补全模型 </a></li><li><a href="#PAR%EF%BC%9A%E5%B9%B6%E8%A1%8C%E8%87%AA%E5%9B%9E%E5%BD%92%E8%A7%86%E8%A7%89%E7%94%9F%E6%88%90">PAR：并行自回归视觉生成</a></li><li><a href="#DeepSeek-V3%EF%BC%9A6710%E4%BA%BF%E5%8F%82%E6%95%B0%E7%9A%84%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">DeepSeek-V3：6710亿参数的混合专家语言模型</a></li><li><a href="#DiT-CTRL%EF%BC%9A%E5%A4%9A%E6%A8%A1%E6%80%81%E6%89%A9%E6%95%A3Transformer%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8E%A7%E5%88%B6%E6%8E%A2%E7%B4%A2">DiT-CTRL：多模态扩散Transformer中的注意力控制探索</a></li><li><a href="#Mulberry%EF%BC%9A%E9%80%9A%E8%BF%87%E9%9B%86%E4%BD%93%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%A0%91%E6%90%9C%E7%B4%A2%E5%A2%9E%E5%BC%BA%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86%E4%B8%8E%E5%8F%8D%E6%80%9D%E8%83%BD%E5%8A%9B">Mulberry：通过集体蒙特卡罗树搜索增强多模态大模型的推理与反思能力</a></li></ol><hr><h2 id="qwen2-5-阿里巴巴达摩院推出的多模态大模型" tabindex="-1"><a class="header-anchor" href="#qwen2-5-阿里巴巴达摩院推出的多模态大模型"><span>Qwen2.5：阿里巴巴达摩院推出的多模态大模型</span></a></h2><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-72B-Instruct-Score.jpg" alt="Qwen2.5 Performance 图" tabindex="0" loading="lazy"><figcaption>Qwen2.5 Performance 图</figcaption></figure><p><strong>概要</strong>：Qwen2.5 是阿里巴巴达摩院发布的多模态大模型系列，旨在满足多样化需求。相比之前的版本，Qwen2.5 在预训练和后训练阶段均有显著提升。预训练方面，模型的高质量预训练数据集从7万亿标记扩展至18万亿标记，增强了常识、专业知识和推理能力。后训练阶段，采用了超过100万样本的精细监督微调和多阶段强化学习，提升了模型的人类偏好对齐能力，特别是在长文本生成、结构化数据分析和指令遵循方面。Qwen2.5 系列提供多种规模的模型，包括基础模型和指令微调模型，并提供量化版本，以适应不同的使用场景。其中，Qwen2.5-72B-Instruct 在语言理解、推理、数学、编码和人类偏好对齐等基准测试中表现出色，超越了许多开源和专有模型。此外，Qwen2.5 还支持训练专用模型，如 Qwen2.5-Math、Qwen2.5-Coder 和多模态模型。</p><p><strong>标签</strong>：#Qwen2.5 #阿里巴巴达摩院 #多模态模型 #大规模预训练 #强化学习</p><hr><h2 id="depthlab-从部分到完整的深度补全模型" tabindex="-1"><a class="header-anchor" href="#depthlab-从部分到完整的深度补全模型"><span>DepthLab：从部分到完整的深度补全模型</span></a></h2><figure><img src="https://johanan528.github.io/depthlab_web/static/images/pipeline.jpeg" alt="DepthLab Pipeline 图" tabindex="0" loading="lazy"><figcaption>DepthLab Pipeline 图</figcaption></figure><p><strong>概要</strong>：DepthLab 是一款由香港大学、香港科技大学和蚂蚁集团的研究人员联合开发的深度补全基础模型，旨在解决深度数据中常见的缺失问题。该模型利用图像扩散先验，能够在已知深度的条件下，可靠地填补缺失区域，保持尺度一致性。DepthLab 在多个下游任务中表现出色，包括 3D 场景修复、文本到 3D 场景生成、稀疏视图重建和 LiDAR 深度补全，超越了现有方法的性能。</p><p><strong>标签</strong>：#DepthLab #深度补全 #图像扩散 #3D场景 #LiDAR</p><hr><h2 id="par-并行自回归视觉生成" tabindex="-1"><a class="header-anchor" href="#par-并行自回归视觉生成"><span>PAR：并行自回归视觉生成</span></a></h2><figure><img src="https://epiphqny.github.io/PAR-project/static/images/image_generation.jpg" alt="PAR Comparison 图" tabindex="0" loading="lazy"><figcaption>PAR Comparison 图</figcaption></figure><p><strong>概要</strong>：PAR（Parallelized Autoregressive Visual Generation）是一种旨在提升视觉生成效率的模型。传统的自回归模型逐个生成视觉数据的每个token，速度较慢。PAR通过并行生成弱依赖性的远距离token，同时对强依赖性的局部token保持顺序生成，从而加快生成过程。该方法无需修改模型架构或tokenizer，即可集成到标准自回归模型中。在ImageNet和UCF-101上的实验表明，PAR在图像和视频生成任务中实现了3.6倍的速度提升，且生成质量与传统方法相当。</p><p><strong>标签</strong>：#PAR #自回归模型 #视觉生成 #并行生成 #深度学习</p><hr><h2 id="deepseek-v3-6710亿参数的混合专家语言模型" tabindex="-1"><a class="header-anchor" href="#deepseek-v3-6710亿参数的混合专家语言模型"><span>DeepSeek-V3：6710亿参数的混合专家语言模型</span></a></h2><figure><img src="https://github.com/deepseek-ai/DeepSeek-V3/raw/main/figures/benchmark.png" alt="DeepSeek-V3 Benchmark 图" tabindex="0" loading="lazy"><figcaption>DeepSeek-V3 Benchmark 图</figcaption></figure><p><strong>概要</strong>：DeepSeek-V3 是由 deepseek-ai 开发的混合专家（MoE）语言模型，总参数量达 6710 亿，每个 token 激活 370 亿参数。该模型采用多头潜在注意力（MLA）和 DeepSeekMoE 架构，结合无辅助损失的负载均衡策略和多 token 预测训练目标，提升了推理效率和训练成本效益。在 14.8 万亿高质量多样化 token 上进行预训练，并经过监督微调和强化学习阶段，展现出卓越的性能，超越了其他开源模型，并与领先的闭源模型相媲美。完整训练耗时约 278.8 万 H800 GPU 小时，过程稳定，无需回滚。</p><p><strong>标签</strong>：#DeepSeek-V3 #混合专家模型 #语言模型 #深度学习 #人工智能</p><hr><h2 id="dit-ctrl-多模态扩散transformer中的注意力控制探索" tabindex="-1"><a class="header-anchor" href="#dit-ctrl-多模态扩散transformer中的注意力控制探索"><span>DiT-CTRL：多模态扩散Transformer中的注意力控制探索</span></a></h2><figure><img src="https://onevfall.github.io/project_page/ditctrl/static/images/framework.jpg" alt="DiT-CTRL Framework图" tabindex="0" loading="lazy"><figcaption>DiT-CTRL Framework图</figcaption></figure><p><strong>概要</strong>：DiT-CTRL是一种无需额外训练的多提示词视频生成方法，旨在解决现有模型在生成包含多个顺序提示词的视频时，缺乏连贯性和自然过渡的问题。通过分析多模态扩散Transformer（MM-DiT）的注意力机制，DiT-CTRL实现了基于掩码的精确语义控制，使生成的视频在不同提示词之间具有平滑过渡和一致的对象运动。实验结果表明，DiT-CTRL在无需额外训练的情况下，实现了多提示词视频生成的最新性能。</p><p><strong>标签</strong>：#DiT-CTRL #多提示词视频生成 #注意力控制 #多模态扩散Transformer #深度学习</p><hr><h2 id="mulberry-通过集体蒙特卡罗树搜索增强多模态大模型的推理与反思能力" tabindex="-1"><a class="header-anchor" href="#mulberry-通过集体蒙特卡罗树搜索增强多模态大模型的推理与反思能力"><span>Mulberry：通过集体蒙特卡罗树搜索增强多模态大模型的推理与反思能力</span></a></h2><p><img src="https://arxiv.org/html/2412.18319v1/x2.png" alt="Mulberry Overview 图" loading="lazy"><strong>概要</strong>：Mulberry 是一种多模态大语言模型（MLLM），旨在通过集体蒙特卡罗树搜索（CoMCTS）提升模型的逐步推理与反思能力。CoMCTS 方法引入集体学习的概念，通过扩展、模拟与错误定位、回传和选择等迭代操作，协作地推测、搜索并确定有效的推理路径，直至得出正确答案。利用 CoMCTS，研究者构建了包含 26 万个多模态数据集的 Mulberry-260k，每个问题都包含丰富、明确的推理节点树。在此基础上训练的 Mulberry 模型在多个基准测试中表现出色，展现了卓越的逐步推理与反思能力。</p><p><strong>标签</strong>：#Mulberry #多模态大语言模型 #推理 #反思 #蒙特卡罗树搜索</p><hr><h3 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span><strong>参考文献</strong></span></a></h3><ol><li><a href="https://github.com/QwenLM/Qwen2.5" target="_blank" rel="noopener noreferrer">Qwen2.5 GitHub</a></li><li><a href="https://arxiv.org/pdf/2412.15115" target="_blank" rel="noopener noreferrer">Qwen2.5 论文</a></li><li><a href="https://johanan528.github.io/depthlab_web/" target="_blank" rel="noopener noreferrer">DepthLab 项目主页</a></li><li><a href="https://github.com/Johanan528/DepthLab" target="_blank" rel="noopener noreferrer">DepthLab GitHub</a></li><li><a href="https://arxiv.org/html/2412.18153v1" target="_blank" rel="noopener noreferrer">DepthLab 论文</a></li><li><a href="https://epiphqny.github.io/PAR-project/" target="_blank" rel="noopener noreferrer">PAR 项目主页</a></li><li><a href="https://github.com/Epiphqny/PAR" target="_blank" rel="noopener noreferrer">PAR GitHub</a></li><li><a href="https://arxiv.org/html/2412.15119v1" target="_blank" rel="noopener noreferrer">PAR 论文</a></li><li><a href="https://www.deepseek.com/" target="_blank" rel="noopener noreferrer">DeepSeek-V3 官网</a></li><li><a href="https://github.com/deepseek-ai/DeepSeek-V3" target="_blank" rel="noopener noreferrer">DeepSeek-V3 GitHub</a></li><li><a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base" target="_blank" rel="noopener noreferrer">DeepSeek-V3 Hugging Face</a></li><li><a href="https://onevfall.github.io/project_page/ditctrl/" target="_blank" rel="noopener noreferrer">DiT-CTRL 项目主页</a></li><li><a href="https://github.com/tencentarc/ditctrl" target="_blank" rel="noopener noreferrer">DiT-CTRL GitHub</a></li><li><a href="https://arxiv.org/html/2412.18597v1" target="_blank" rel="noopener noreferrer">DiT-CTRL 论文</a></li><li><a href="https://github.com/hjyao00/mulberry" target="_blank" rel="noopener noreferrer">Mulberry GitHub</a></li><li><a href="https://arxiv.org/html/2412.18319v1" target="_blank" rel="noopener noreferrer">Mulberry 论文</a></li></ol>',39)]))}]]),l=JSON.parse('{"path":"/zh/posts/ai-weekly/018.html","title":"Qwen2.5定义多模态模型新高度|DeepSeek-V3突破混合专家能力|Mulberry强化推理反思【AI周报】","lang":"zh-CN","frontmatter":{"description":"Qwen2.5定义多模态模型新高度|DeepSeek-V3突破混合专家能力|Mulberry强化推理反思【AI周报】 封面源自C站作者Meower2024封面源自C站作者Meower2024 摘要 本周聚焦大模型与多模态：Qwen2.5 优化预训练与后训练，定义多模态模型新标准；DeepSeek-V3 引入混合专家架构，提升训练效率；Mulberry ...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/018.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"Qwen2.5定义多模态模型新高度|DeepSeek-V3突破混合专家能力|Mulberry强化推理反思【AI周报】"}],["meta",{"property":"og:description","content":"Qwen2.5定义多模态模型新高度|DeepSeek-V3突破混合专家能力|Mulberry强化推理反思【AI周报】 封面源自C站作者Meower2024封面源自C站作者Meower2024 摘要 本周聚焦大模型与多模态：Qwen2.5 优化预训练与后训练，定义多模态模型新标准；DeepSeek-V3 引入混合专家架构，提升训练效率；Mulberry ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/b338b22c-394c-4ae0-88d2-2bf15fa83809/original=true,quality=90/47422369.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Qwen2.5定义多模态模型新高度|DeepSeek-V3突破混合专家能力|Mulberry强化推理反思【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/b338b22c-394c-4ae0-88d2-2bf15fa83809/original=true,quality=90/47422369.jpeg\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-72B-Instruct-Score.jpg\\",\\"https://johanan528.github.io/depthlab_web/static/images/pipeline.jpeg\\",\\"https://epiphqny.github.io/PAR-project/static/images/image_generation.jpg\\",\\"https://github.com/deepseek-ai/DeepSeek-V3/raw/main/figures/benchmark.png\\",\\"https://onevfall.github.io/project_page/ditctrl/static/images/framework.jpg\\",\\"https://arxiv.org/html/2412.18319v1/x2.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"Qwen2.5：阿里巴巴达摩院推出的多模态大模型","slug":"qwen2-5-阿里巴巴达摩院推出的多模态大模型","link":"#qwen2-5-阿里巴巴达摩院推出的多模态大模型","children":[]},{"level":2,"title":"DepthLab：从部分到完整的深度补全模型","slug":"depthlab-从部分到完整的深度补全模型","link":"#depthlab-从部分到完整的深度补全模型","children":[]},{"level":2,"title":"PAR：并行自回归视觉生成","slug":"par-并行自回归视觉生成","link":"#par-并行自回归视觉生成","children":[]},{"level":2,"title":"DeepSeek-V3：6710亿参数的混合专家语言模型","slug":"deepseek-v3-6710亿参数的混合专家语言模型","link":"#deepseek-v3-6710亿参数的混合专家语言模型","children":[]},{"level":2,"title":"DiT-CTRL：多模态扩散Transformer中的注意力控制探索","slug":"dit-ctrl-多模态扩散transformer中的注意力控制探索","link":"#dit-ctrl-多模态扩散transformer中的注意力控制探索","children":[]},{"level":2,"title":"Mulberry：通过集体蒙特卡罗树搜索增强多模态大模型的推理与反思能力","slug":"mulberry-通过集体蒙特卡罗树搜索增强多模态大模型的推理与反思能力","link":"#mulberry-通过集体蒙特卡罗树搜索增强多模态大模型的推理与反思能力","children":[{"level":3,"title":"参考文献","slug":"参考文献","link":"#参考文献","children":[]}]}],"readingTime":{"minutes":6.01,"words":1803},"filePathRelative":"zh/posts/ai-weekly/018.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/b338b22c-394c-4ae0-88d2-2bf15fa83809/original=true,quality=90/47422369.jpeg\\" alt=\\"封面源自C站作者Meower2024\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面源自C站作者Meower2024</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周聚焦大模型与多模态：Qwen2.5 优化预训练与后训练，定义多模态模型新标准；DeepSeek-V3 引入混合专家架构，提升训练效率；Mulberry 结合蒙特卡罗树搜索，增强推理与反思能力。详见正文。</p>","autoDesc":true}')}}]);