"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[2084],{6262:(t,n)=>{n.A=(t,n)=>{const i=t.__vccOpts||t;for(const[t,r]of n)i[t]=r;return i}},3227:(t,n,i)=>{i.r(n),i.d(n,{comp:()=>e,data:()=>d});var r=i(641);const l={},e=(0,i(6262).A)(l,[["render",function(t,n){return(0,r.uX)(),(0,r.CE)("div",null,n[0]||(n[0]=[(0,r.Fv)('<h1 id="【论文精读】ming-omni-统一的多模态感知与生成模型" tabindex="-1"><a class="header-anchor" href="#【论文精读】ming-omni-统一的多模态感知与生成模型"><span>【论文精读】Ming-Omni: 统一的多模态感知与生成模型</span></a></h1><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>蚂蚁集团推出的<strong>Ming-Omni</strong>是首个开源统一多模态模型，支持文本、图像、视频和音频的感知与生成。创新MoE架构配合模态特定路由器提升跨模态协同。轻量版仅2.8B参数却达到SOTA性能，在模态覆盖上媲美GPT-4o，推动多模态AI普及。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li>背景与研究目标</li><li>方法与创新点</li><li>实验与结果分析</li><li>模型启发与方法延伸</li><li>结论与未来展望</li></ol><hr><h2 id="背景与研究目标" tabindex="-1"><a class="header-anchor" href="#背景与研究目标"><span>背景与研究目标</span></a></h2><figure><img src="https://arxiv.org/html/2506.09344v1/x1.png" alt="Ming-Omni多模态能力展示图" tabindex="0" loading="lazy"><figcaption>Ming-Omni多模态能力展示图</figcaption></figure><h3 id="研究背景" tabindex="-1"><a class="header-anchor" href="#研究背景"><span>研究背景</span></a></h3><p>追求通用人工智能(AGI)的发展推动了能够跨多种模态处理和生成内容的系统开发，这种能力类似于人类认知能力。虽然大语言模型(LLM)在个别领域取得了令人印象深刻的进展，但现有的多模态大语言模型(MLLM)在统一架构中面临着关键挑战：</p><ul><li><strong>表征差异</strong>：不同模态需要不同的数据结构和表征方式</li><li><strong>训练冲突</strong>：优化一个模态可能会降低其他模态的性能</li><li><strong>架构分离</strong>：感知和生成的分离模型导致复杂的处理流水线</li></ul><h3 id="核心问题" tabindex="-1"><a class="header-anchor" href="#核心问题"><span>核心问题</span></a></h3><p>现有的多模态模型通常只专注于理解任务，而缺乏生成能力，或者需要多个独立的模型来处理不同的任务。这种分离不仅增加了系统复杂性，还限制了不同模态之间的协同效应。</p><p><strong>Ming-Omni的目标</strong>是构建一个真正统一的架构，在单一模型中同时实现多模态的感知和生成，消除模态间的壁垒，实现高效的跨模态处理。</p><hr><h2 id="方法与创新点" tabindex="-1"><a class="header-anchor" href="#方法与创新点"><span>方法与创新点</span></a></h2><h3 id="整体架构设计" tabindex="-1"><a class="header-anchor" href="#整体架构设计"><span>整体架构设计</span></a></h3><figure><img src="https://arxiv.org/html/2506.09344v1/x2.png" alt="Ming-Omni整体框架图" tabindex="0" loading="lazy"><figcaption>Ming-Omni整体框架图</figcaption></figure><p>Ming-Omni采用统一的MoE架构，主要包含以下核心组件：</p><p><strong>输入处理层</strong>：</p><ul><li><em>视觉编码器</em>：基于Qwen2.5，支持图像和视频的任意分辨率处理</li><li><em>音频编码器</em>：处理语音输入，提取音频特征</li><li><em>标记对齐</em>：将不同模态的嵌入投影到Ling的维度空间</li></ul><p><strong>核心处理单元</strong>：</p><ul><li><em>Ling MoE骨干网络</em>：基于专家混合的语言模型核心</li><li><em>模态特定路由器</em>：为不同模态任务分配专门的专家</li></ul><p><strong>输出生成层</strong>：</p><ul><li><em>文本解码器</em>：标准的自回归文本生成</li><li><em>音频解码器</em>：高质量语音合成</li><li><em>图像生成器</em>：Ming-Lite-Uni集成的图像生成能力</li></ul><h3 id="核心创新点" tabindex="-1"><a class="header-anchor" href="#核心创新点"><span>核心创新点</span></a></h3><h4 id="_1-统一多模态处理架构" tabindex="-1"><a class="header-anchor" href="#_1-统一多模态处理架构"><span>1. 统一多模态处理架构</span></a></h4><p><strong>模态特定路由器(Modality-Specific Routers)</strong>：</p><ul><li>为每种模态设计专门的路由机制</li><li>动态分配计算资源，避免模态间的任务冲突</li><li>保持模型统一性的同时确保各模态的最优性能</li></ul><h4 id="_2-音频理解与生成创新" tabindex="-1"><a class="header-anchor" href="#_2-音频理解与生成创新"><span>2. 音频理解与生成创新</span></a></h4><p><strong>BPE音频标记压缩</strong>：</p><ul><li>将音频标记压缩36%，显著提高处理效率</li><li>保持音频质量的同时减少计算开销</li><li>支持端到端的语音理解和指令跟随</li></ul><p><strong>动态自适应平衡</strong>：</p><ul><li>针对跨模态训练的动态平衡策略</li><li>解决不同模态数据分布差异带来的训练不稳定问题</li></ul><h4 id="_3-轻量化图像生成" tabindex="-1"><a class="header-anchor" href="#_3-轻量化图像生成"><span>3. 轻量化图像生成</span></a></h4><p><strong>多尺度可学习标记融合</strong>：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>多尺度特征 → 可学习融合 → 统一表征 → 高质量生成</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><ul><li>支持原生分辨率的图像生成、编辑和风格转换</li><li>在保持轻量化的同时实现高质量图像生成</li><li>GenEval评分达到0.64，超越SDXL等主流模型</li></ul><p><strong>多尺度表征对齐</strong>：</p><ul><li>不同分辨率特征的有效对齐机制</li><li>提升生成图像的细节表现和整体一致性</li></ul><h3 id="两阶段训练策略" tabindex="-1"><a class="header-anchor" href="#两阶段训练策略"><span>两阶段训练策略</span></a></h3><p><strong>阶段一：多模态预训练</strong></p><ul><li>大规模多模态数据的联合训练</li><li>建立不同模态间的基础对应关系</li><li>学习跨模态的通用表征</li></ul><p><strong>阶段二：任务特定微调</strong></p><ul><li>针对具体下游任务的精细调优</li><li>平衡感知和生成任务的性能</li><li>优化用户交互体验</li></ul><hr><h2 id="实验与结果分析" tabindex="-1"><a class="header-anchor" href="#实验与结果分析"><span>实验与结果分析</span></a></h2><figure><img src="https://arxiv.org/html/2506.09344v1/x5.png" alt="数据配置概览图" tabindex="0" loading="lazy"><figcaption>数据配置概览图</figcaption></figure><h3 id="实验设置" tabindex="-1"><a class="header-anchor" href="#实验设置"><span>实验设置</span></a></h3><p><strong>模型规模</strong>：</p><ul><li>Ming-Omni：完整版本</li><li>Ming-Lite-Omni：2.8B激活参数的轻量版本</li></ul><p><strong>评估维度</strong>：</p><ul><li>图像感知：多项视觉理解基准</li><li>音频视觉交互：端到端语音理解</li><li>图像生成：FID、GenEval等生成质量指标</li><li>跨模态任务：GUI操作、知识问答等复合任务</li></ul><h3 id="主要实验结果" tabindex="-1"><a class="header-anchor" href="#主要实验结果"><span>主要实验结果</span></a></h3><h4 id="图像感知性能" tabindex="-1"><a class="header-anchor" href="#图像感知性能"><span>图像感知性能</span></a></h4><p>基于README中的详细基准测试结果：</p><p><strong>综合视觉理解基准</strong>：</p><table><thead><tr><th>基准测试</th><th>Ming-Lite-Omni</th><th>Qwen2.5-VL-7B</th><th>InternVL2.5-8B</th></tr></thead><tbody><tr><td>AI2D</td><td>83.1</td><td>84.4</td><td><strong>84.5</strong></td></tr><tr><td>HallusionBench</td><td><strong>55.0</strong></td><td>55.8</td><td>51.7</td></tr><tr><td>MMBench</td><td>80.8</td><td><strong>82.8</strong></td><td>82.0</td></tr><tr><td>MMMU</td><td>56.3</td><td><strong>56.6</strong></td><td>54.8</td></tr><tr><td>MathVista</td><td><strong>71.6</strong></td><td>68.1</td><td>67.9</td></tr><tr><td>OCRBench</td><td><strong>88.4</strong></td><td>87.8</td><td>88.2</td></tr><tr><td>平均分</td><td>71.4</td><td><strong>71.5</strong></td><td>70.3</td></tr></tbody></table><p><strong>专业知识领域表现</strong>：</p><table><thead><tr><th>对象识别</th><th>Ming-Lite-Omni</th><th>Qwen2.5-VL-7B</th></tr></thead><tbody><tr><td>植物</td><td><strong>54.96</strong></td><td>47.8</td></tr><tr><td>动物</td><td><strong>56.7</strong></td><td>50.85</td></tr><tr><td>车辆</td><td>41.91</td><td><strong>42.29</strong></td></tr><tr><td>食材</td><td><strong>62.28</strong></td><td>54.09</td></tr><tr><td>菜品</td><td><strong>44.3</strong></td><td>39.07</td></tr><tr><td>总体平均</td><td><strong>58.54</strong></td><td>54.43</td></tr></tbody></table><p><strong>关键发现</strong>：</p><ul><li>Ming-Lite-Omni仅用2.8B激活参数就达到了7B模型的性能水平</li><li>在数学推理(MathVista)和OCR任务上显著超越基线</li><li>在专业知识领域(植物、动物、食材)表现突出，体现出强大的知识整合能力</li></ul><h4 id="图像生成质量" tabindex="-1"><a class="header-anchor" href="#图像生成质量"><span>图像生成质量</span></a></h4><table><thead><tr><th>指标</th><th>Ming-Lite-Omni</th><th>SDXL</th><th>其他主流模型</th></tr></thead><tbody><tr><td>FID ↓</td><td><strong>4.85</strong></td><td>6.42</td><td>5.2-7.8</td></tr><tr><td>GenEval ↑</td><td><strong>0.64</strong></td><td>0.58</td><td>0.52-0.61</td></tr></tbody></table><p><strong>突出表现</strong>：</p><ul><li>FID分数4.85创造了新的SOTA记录</li><li>GenEval评分超越包括SDXL在内的所有主流模型</li><li>支持原生分辨率生成，无需额外的超分辨率后处理</li></ul><p><strong>图像生成效果展示</strong>：</p><figure><img src="https://arxiv.org/html/2506.09344v1/extracted/6507736/figures/f4.jpg" alt="指令式图像风格转换结果" tabindex="0" loading="lazy"><figcaption>指令式图像风格转换结果</figcaption></figure><figure><img src="https://arxiv.org/html/2506.09344v1/x3.png" alt="指令式文本到图像生成结果" tabindex="0" loading="lazy"><figcaption>指令式文本到图像生成结果</figcaption></figure><figure><img src="https://arxiv.org/html/2506.09344v1/x4.png" alt="指令式图像编辑结果" tabindex="0" loading="lazy"><figcaption>指令式图像编辑结果</figcaption></figure><p>如图所示，Ming-Omni能够高质量地完成指令式图像风格转换、文本到图像生成以及图像编辑等多种生成任务，展现了其在创意应用和实用场景中的广泛适用性。</p><h4 id="音频处理能力" tabindex="-1"><a class="header-anchor" href="#音频处理能力"><span>音频处理能力</span></a></h4><p><strong>语音问答任务表现</strong>：</p><table><thead><tr><th>模型</th><th>平均分</th><th>AlpacaEval</th><th>CommonEval</th><th>SD-QA</th><th>MMSU</th><th>OpenBookQA</th><th>IFEval</th><th>AdvBench</th></tr></thead><tbody><tr><td>Qwen2-Audio</td><td>3.545</td><td>3.69</td><td>3.40</td><td>35.35</td><td>35.43</td><td>49.01</td><td>22.57</td><td>98.85</td></tr><tr><td>Baichuan-Audio</td><td>3.695</td><td>4.00</td><td>3.39</td><td>49.64</td><td>48.80</td><td>63.30</td><td>41.32</td><td>86.73</td></tr><tr><td>GLM-4-Voice</td><td>3.77</td><td>4.06</td><td>3.48</td><td>43.31</td><td>40.11</td><td>52.97</td><td>24.91</td><td>88.08</td></tr><tr><td>Kimi-Audio</td><td>4.215</td><td>4.46</td><td>3.97</td><td><strong>63.12</strong></td><td><strong>62.17</strong></td><td><strong>83.52</strong></td><td><strong>61.10</strong></td><td><strong>100.00</strong></td></tr><tr><td>Qwen2.5-Omni</td><td>4.21</td><td>4.49</td><td>3.93</td><td>55.71</td><td>61.32</td><td>81.10</td><td>52.87</td><td>99.42</td></tr><tr><td><strong>Ming-Lite-Omni</strong></td><td><strong>4.34</strong></td><td><strong>4.63</strong></td><td><strong>4.06</strong></td><td>58.84</td><td>47.53</td><td>61.98</td><td>58.36</td><td>99.04</td></tr></tbody></table><p><strong>视频理解能力</strong>：</p><table><thead><tr><th>基准测试</th><th>Ming-Lite-Omni</th><th>Qwen2.5VL-7B</th></tr></thead><tbody><tr><td>VideoMME</td><td>67.0</td><td><strong>67.3</strong></td></tr><tr><td>MVBench</td><td>67.7</td><td><strong>67.4</strong></td></tr><tr><td>Video-MMMU</td><td>46.3</td><td><strong>47.4</strong></td></tr><tr><td>LongVideoBench</td><td><strong>56.6</strong></td><td>54.7</td></tr><tr><td>平均分</td><td><strong>59.4</strong></td><td>59.2</td></tr></tbody></table><p><strong>语音理解与生成</strong>：</p><ul><li>在语音问答的综合评分上达到4.34分，超越所有对比模型</li><li>支持复杂指令跟随和多轮对话</li><li>高质量自然语音生成和上下文感知的语音合成</li></ul><h3 id="消融实验分析" tabindex="-1"><a class="header-anchor" href="#消融实验分析"><span>消融实验分析</span></a></h3><p><strong>模态特定路由器的影响</strong>：</p><ul><li>移除路由器导致跨模态任务性能下降15-20%</li><li>证明了专门化路由对统一架构的重要性</li></ul><p><strong>BPE音频压缩效果</strong>：</p><ul><li>36%的标记压缩显著提升推理速度</li><li>音频质量损失小于2%，效率提升显著</li></ul><hr><h2 id="模型启发与方法延伸" tabindex="-1"><a class="header-anchor" href="#模型启发与方法延伸"><span>模型启发与方法延伸</span></a></h2><h3 id="核心技术价值" tabindex="-1"><a class="header-anchor" href="#核心技术价值"><span>核心技术价值</span></a></h3><p><strong>统一架构突破</strong>：Ming-Omni首次证明了单一模型同时处理多模态感知和生成的可行性，模态特定路由器为多任务协同提供了新范式。</p><p><strong>工程化创新</strong>：BPE音频压缩、多尺度融合、两阶段训练等技术具有广泛的迁移价值，可应用于其他复杂多任务学习场景。</p><h3 id="实际应用前景" tabindex="-1"><a class="header-anchor" href="#实际应用前景"><span>实际应用前景</span></a></h3><p><strong>🤖 下一代智能助手</strong>：真正的多模态交互体验，用户可以通过语音、图像、文本自然对话，并获得包含图像、音频的丰富回应。</p><p><strong>🎨 创意内容平台</strong>：统一的创作环境，支持文本、图像、音频的跨模态编辑，为短视频、播客等内容创作者提供一站式解决方案。</p><p><strong>📚 智能教育系统</strong>：自动生成多模态教学内容，根据学习者偏好适配不同的表达方式（文字、图片、语音），提升学习效果。</p><h3 id="技术发展方向" tabindex="-1"><a class="header-anchor" href="#技术发展方向"><span>技术发展方向</span></a></h3><p>Ming-Omni的成功指向了多模态AI的关键趋势：更大的模态覆盖（3D、触觉）、更高的计算效率、更广泛的开源普及。</p><hr><h2 id="结论与未来展望" tabindex="-1"><a class="header-anchor" href="#结论与未来展望"><span>结论与未来展望</span></a></h2><h3 id="主要贡献总结" tabindex="-1"><a class="header-anchor" href="#主要贡献总结"><span>主要贡献总结</span></a></h3><ol><li><strong>开源多模态突破</strong>：首个媲美GPT-4o模态覆盖的开源模型，填补重要空白</li><li><strong>统一架构创新</strong>：模态特定路由器+MoE设计，解决多模态训练核心挑战</li><li><strong>轻量化SOTA</strong>：2.8B参数达到7B模型性能，效率与效果兼得</li><li><strong>实用技术方案</strong>：BPE压缩、动态平衡等技术具备广泛应用价值</li></ol><h3 id="技术优势与局限" tabindex="-1"><a class="header-anchor" href="#技术优势与局限"><span>技术优势与局限</span></a></h3><p><strong>核心优势</strong>：统一架构避免多模型复杂性、轻量化设计具备部署价值、开源推动技术普及</p><p><strong>现有局限</strong>：专门化任务可能不及特定模型、训练复杂度高、长序列推理待提升</p><h3 id="发展趋势展望" tabindex="-1"><a class="header-anchor" href="#发展趋势展望"><span>发展趋势展望</span></a></h3><p>Ming-Omni预示着多模态AI的重要转向：</p><p><strong>统一化</strong> → 单一架构替代多模型系统<br><strong>轻量化</strong> → 高效率与高性能并存<br><strong>开源化</strong> → 先进技术民主化普及<br><strong>实用化</strong> → 从实验室走向产品应用</p><p>随着Ming-Omni开源发布，真正实用的通用AI系统将加速到来，为人机交互和内容创作带来革命性变化。</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h3><ol><li><a href="https://arxiv.org/abs/2506.09344" target="_blank" rel="noopener noreferrer">论文原文</a></li><li><a href="https://lucaria-academy.github.io/Ming-Omni/" target="_blank" rel="noopener noreferrer">项目主页</a></li><li><a href="https://github.com/inclusionAI/Ming" target="_blank" rel="noopener noreferrer">代码仓库</a></li><li><a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni" target="_blank" rel="noopener noreferrer">HuggingFace模型</a></li><li><a href="https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni" target="_blank" rel="noopener noreferrer">ModelScope模型</a></li></ol>',109)]))}]]),d=JSON.parse('{"path":"/zh/posts/papers/ming-omni.html","title":"【论文精读】Ming-Omni: 统一的多模态感知与生成模型","lang":"zh-CN","frontmatter":{"description":"【论文精读】Ming-Omni: 统一的多模态感知与生成模型 摘要 蚂蚁集团推出的Ming-Omni是首个开源统一多模态模型，支持文本、图像、视频和音频的感知与生成。创新MoE架构配合模态特定路由器提升跨模态协同。轻量版仅2.8B参数却达到SOTA性能，在模态覆盖上媲美GPT-4o，推动多模态AI普及。 目录 背景与研究目标 方法与创新点 实验与结果分...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/papers/ming-omni.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"【论文精读】Ming-Omni: 统一的多模态感知与生成模型"}],["meta",{"property":"og:description","content":"【论文精读】Ming-Omni: 统一的多模态感知与生成模型 摘要 蚂蚁集团推出的Ming-Omni是首个开源统一多模态模型，支持文本、图像、视频和音频的感知与生成。创新MoE架构配合模态特定路由器提升跨模态协同。轻量版仅2.8B参数却达到SOTA性能，在模态覆盖上媲美GPT-4o，推动多模态AI普及。 目录 背景与研究目标 方法与创新点 实验与结果分..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://arxiv.org/html/2506.09344v1/x1.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"【论文精读】Ming-Omni: 统一的多模态感知与生成模型\\",\\"image\\":[\\"https://arxiv.org/html/2506.09344v1/x1.png\\",\\"https://arxiv.org/html/2506.09344v1/x2.png\\",\\"https://arxiv.org/html/2506.09344v1/x5.png\\",\\"https://arxiv.org/html/2506.09344v1/extracted/6507736/figures/f4.jpg\\",\\"https://arxiv.org/html/2506.09344v1/x3.png\\",\\"https://arxiv.org/html/2506.09344v1/x4.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"背景与研究目标","slug":"背景与研究目标","link":"#背景与研究目标","children":[{"level":3,"title":"研究背景","slug":"研究背景","link":"#研究背景","children":[]},{"level":3,"title":"核心问题","slug":"核心问题","link":"#核心问题","children":[]}]},{"level":2,"title":"方法与创新点","slug":"方法与创新点","link":"#方法与创新点","children":[{"level":3,"title":"整体架构设计","slug":"整体架构设计","link":"#整体架构设计","children":[]},{"level":3,"title":"核心创新点","slug":"核心创新点","link":"#核心创新点","children":[]},{"level":3,"title":"两阶段训练策略","slug":"两阶段训练策略","link":"#两阶段训练策略","children":[]}]},{"level":2,"title":"实验与结果分析","slug":"实验与结果分析","link":"#实验与结果分析","children":[{"level":3,"title":"实验设置","slug":"实验设置","link":"#实验设置","children":[]},{"level":3,"title":"主要实验结果","slug":"主要实验结果","link":"#主要实验结果","children":[]},{"level":3,"title":"消融实验分析","slug":"消融实验分析","link":"#消融实验分析","children":[]}]},{"level":2,"title":"模型启发与方法延伸","slug":"模型启发与方法延伸","link":"#模型启发与方法延伸","children":[{"level":3,"title":"核心技术价值","slug":"核心技术价值","link":"#核心技术价值","children":[]},{"level":3,"title":"实际应用前景","slug":"实际应用前景","link":"#实际应用前景","children":[]},{"level":3,"title":"技术发展方向","slug":"技术发展方向","link":"#技术发展方向","children":[]}]},{"level":2,"title":"结论与未来展望","slug":"结论与未来展望","link":"#结论与未来展望","children":[{"level":3,"title":"主要贡献总结","slug":"主要贡献总结","link":"#主要贡献总结","children":[]},{"level":3,"title":"技术优势与局限","slug":"技术优势与局限","link":"#技术优势与局限","children":[]},{"level":3,"title":"发展趋势展望","slug":"发展趋势展望","link":"#发展趋势展望","children":[]},{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":8.3,"words":2489},"filePathRelative":"zh/posts/papers/ming-omni.md","excerpt":"\\n<h2>摘要</h2>\\n<p>蚂蚁集团推出的<strong>Ming-Omni</strong>是首个开源统一多模态模型，支持文本、图像、视频和音频的感知与生成。创新MoE架构配合模态特定路由器提升跨模态协同。轻量版仅2.8B参数却达到SOTA性能，在模态覆盖上媲美GPT-4o，推动多模态AI普及。</p>\\n<hr>\\n<h2>目录</h2>\\n<ol>\\n<li>背景与研究目标</li>\\n<li>方法与创新点</li>\\n<li>实验与结果分析</li>\\n<li>模型启发与方法延伸</li>\\n<li>结论与未来展望</li>\\n</ol>\\n<hr>\\n<h2>背景与研究目标</h2>\\n<figure><img src=\\"https://arxiv.org/html/2506.09344v1/x1.png\\" alt=\\"Ming-Omni多模态能力展示图\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>Ming-Omni多模态能力展示图</figcaption></figure>","autoDesc":true}')}}]);