"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[5021],{6262:(e,t)=>{t.A=(e,t)=>{const i=e.__vccOpts||e;for(const[e,n]of t)i[e]=n;return i}},745:(e,t,i)=>{i.r(t),i.d(t,{comp:()=>o,data:()=>a});var n=i(641);const r={},o=(0,i(6262).A)(r,[["render",function(e,t){return(0,n.uX)(),(0,n.CE)("div",null,t[0]||(t[0]=[(0,n.Fv)('<h1 id="colorizediffusion-v2实现最强动漫草图上色-reptext-多语言文本渲染-nexus-gen-图像生成统一模型【ai周报】" tabindex="-1"><a class="header-anchor" href="#colorizediffusion-v2实现最强动漫草图上色-reptext-多语言文本渲染-nexus-gen-图像生成统一模型【ai周报】"><span>ColorizeDiffusion v2实现最强动漫草图上色 | RepText 多语言文本渲染 | Nexus-Gen 图像生成统一模型【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/44cb3616-4cff-4dea-bcb6-caeedb99ceeb/original=true,quality=90/73901561.jpeg" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：ColorizeDiffusion v2 提升动漫草图上色质量；RepText 实现多语言文本渲染；ICEdit 多视角保持身份一致性编辑；Insert Anything 灵活插入任意物体；Nexus-Gen 融合 LLM 与扩散模型统一生成流程；X-Fusion 实现多任务图文对齐能力。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#colorizediffusion-v2%E5%8A%A8%E6%BC%AB%E8%8D%89%E5%9B%BE%E4%B8%8A%E8%89%B2%E7%9A%84%E5%8F%82%E8%80%83%E5%A2%9E%E5%BC%BA%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B">ColorizeDiffusion v2：动漫草图上色的参考增强扩散模型</a></li><li><a href="#reptext%E5%A4%9A%E8%AF%AD%E8%A8%80%E9%AB%98%E4%BF%9D%E7%9C%9F%E8%A7%86%E8%A7%89%E6%96%87%E6%9C%AC%E6%B8%B2%E6%9F%93%E6%A1%86%E6%9E%B6">RepText：多语言高保真视觉文本渲染框架</a></li><li><a href="#icedit%E9%AB%98%E6%95%88%E6%8C%87%E4%BB%A4%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91%E6%A1%86%E6%9E%B6%E6%94%AF%E6%8C%81%E9%9B%B6%E6%A0%B7%E6%9C%AC%E7%BC%96%E8%BE%91%E4%B8%8E%E9%AB%98%E8%B4%A8%E9%87%8F%E8%BE%93%E5%87%BA">ICEdit：高效指令图像编辑框架，支持零样本编辑与高质量输出</a></li><li><a href="#nexus-gen%E8%9E%8D%E5%90%88llm%E4%B8%8E%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%9F%E4%B8%80%E5%9B%BE%E5%83%8F%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E6%A1%86%E6%9E%B6">Nexus-Gen：融合LLM与扩散模型的统一图像理解与生成框架</a></li><li><a href="#insert-anything%E7%BB%9F%E4%B8%80%E7%9A%84%E5%8F%82%E8%80%83%E5%9B%BE%E5%83%8F%E6%8F%92%E5%85%A5%E6%A1%86%E6%9E%B6%E6%94%AF%E6%8C%81%E5%A4%9A%E7%A7%8D%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%BC%8F">Insert Anything：统一的参考图像插入框架，支持多种控制模式</a></li><li><a href="#x-fusion%E5%86%BB%E7%BB%93%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E6%89%A9%E5%B1%95%E6%A1%86%E6%9E%B6">X-Fusion：冻结大语言模型的多模态扩展框架</a></li></ol><hr><h2 id="colorizediffusion-v2-动漫草图上色的参考增强扩散模型" tabindex="-1"><a class="header-anchor" href="#colorizediffusion-v2-动漫草图上色的参考增强扩散模型"><span>ColorizeDiffusion v2：动漫草图上色的参考增强扩散模型</span></a></h2><figure><img src="https://github.com/tellurion-kanata/colorizeDiffusion/raw/master/assets/teaser.png" alt="ColorizeDiffusion v2 Teaser 图" tabindex="0" loading="lazy"><figcaption>ColorizeDiffusion v2 Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>ColorizeDiffusion v2</strong> 是由 <strong>东京工业大学</strong> 与 <strong>东京大学</strong> 联合提出的参考图像驱动草图上色框架，专为动漫风格设计。该方法引入分离式注意力机制，结合背景与风格编码器，提升对参考图像的空间适应性与细节还原能力。通过前景遮罩与背景漂白等预处理步骤，有效缓解训练与推理阶段的分布偏移问题，显著提升上色质量与稳定性。</p><p><strong>标签</strong>：#草图上色 #参考图像生成 #扩散模型 #动漫风格 #前后景分离</p><hr><h2 id="reptext-多语言高保真视觉文本渲染框架" tabindex="-1"><a class="header-anchor" href="#reptext-多语言高保真视觉文本渲染框架"><span>RepText：多语言高保真视觉文本渲染框架</span></a></h2><figure><img src="https://reptext.github.io/static/images/train.png" alt="RepText Pipeline 图" tabindex="0" loading="lazy"><figcaption>RepText Pipeline 图</figcaption></figure><p><strong>概要</strong>：<strong>RepText</strong> 是由 <strong>Shakker Labs</strong> 与 <strong>Liblib AI</strong> 提出的多语言视觉文本渲染方法，受书法临摹启发，创新性地采用&quot;复制&quot;而非&quot;理解&quot;范式。该方法无需理解文本语义，通过字形复制策略（Glyph latent copy）实现对60+语言的支持。技术上结合 ControlNet 架构处理 Canny 边缘与位置信息，推理阶段采用无噪声 glyph latent 替换文本区域的初始噪声，提供准确的结构和颜色指导。创新的区域掩码（Region mask）机制限定控制信号仅影响文本区域，保护背景质量，同时集成 OCR 感知损失提升可读性。模型设计保持高生态兼容性，可与 LoRA、IP-Adapter 等插件无缝结合，为广告设计、UI展示等应用场景提供灵活且低成本的文本渲染解决方案。</p><p><strong>标签</strong>：#视觉文本渲染 #多语言文本 #文本控制 #字形复制 #ControlNet</p><hr><h2 id="icedit-高效指令图像编辑框架-支持零样本编辑与高质量输出" tabindex="-1"><a class="header-anchor" href="#icedit-高效指令图像编辑框架-支持零样本编辑与高质量输出"><span>ICEdit：高效指令图像编辑框架，支持零样本编辑与高质量输出</span></a></h2><figure><img src="https://github.com/River-Zhang/ICEdit/raw/main/docs/images/teaser.png" alt="ICEdit Teaser 图" tabindex="0" loading="lazy"><figcaption>ICEdit Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>ICEdit</strong> 是由 <strong>浙江大学</strong> 与 <strong>哈佛大学</strong> 联合提出的高效指令图像编辑框架，借鉴语言模型的 in-context learning 思想设计&quot;IC Prompt&quot;结构，实现零样本编辑能力。该方法基于大规模 Diffusion Transformer（DiT），引入三项关键技术：一是将原图与编辑指令并列输入的 in-context 编辑框架，无需结构改动即可理解复杂指令；二是结合 LoRA 与 MoE 的混合微调架构，通过动态路由激活不同编辑专家，在保持参数效率的同时提升多样编辑能力；三是推理阶段的 Early Filter 策略，利用视觉语言模型（VLM）筛选最优初始噪声，提升编辑一致性。</p><p><strong>标签</strong>：#图像编辑 #VLM #零样本学习 #高效微调 #DiT</p><hr><h2 id="nexus-gen-融合llm与扩散模型的统一图像理解与生成框架" tabindex="-1"><a class="header-anchor" href="#nexus-gen-融合llm与扩散模型的统一图像理解与生成框架"><span>Nexus-Gen：融合LLM与扩散模型的统一图像理解与生成框架</span></a></h2><figure><img src="https://github.com/modelscope/Nexus-Gen/raw/main/assets/illustrations/gen_edit.jpg" alt="Nexus-Gen Teaser 图" tabindex="0" loading="lazy"><figcaption>Nexus-Gen Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>Nexus-Gen</strong> 是由 <strong>ModelScope 团队</strong> 发布的统一多模态模型，结合了大语言模型（LLM）的语言推理能力与扩散模型的图像生成能力，支持图像理解、生成与编辑等任务。该模型采用双阶段对齐训练策略：首先，LLM在多模态输入条件下预测图像嵌入；其次，视觉解码器从这些嵌入中重建高保真图像。为解决自回归训练与推理阶段的误差积累问题，Nexus-Gen 引入了预填充自回归策略，使用位置嵌入的特殊标记替代连续嵌入，提升生成质量。该模型在多种图像任务中表现出色，展示了强大的多模态理解与生成能力。</p><p><strong>标签</strong>：#多模态生成 #图像理解 #扩散模型 #大语言模型 #统一框架</p><hr><h2 id="insert-anything-统一的参考图像插入框架-支持多种控制模式" tabindex="-1"><a class="header-anchor" href="#insert-anything-统一的参考图像插入框架-支持多种控制模式"><span>Insert Anything：统一的参考图像插入框架，支持多种控制模式</span></a></h2><figure><img src="https://song-wensong.github.io/insert-anything/InsertAnything_files/images/teaser-v4_00.png" alt="Insert Anything Teaser 图" tabindex="0" loading="lazy"><figcaption>Insert Anything Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>Insert Anything</strong> 是由 <strong>浙江大学</strong> 等机构提出的统一图像插入框架，支持基于参考图像的对象、人物和服饰插入任务。该方法引入了包含12万对图像-提示对的 AnyInsertion 数据集，涵盖多种插入场景。模型基于 Diffusion Transformer（DiT）架构，结合多模态注意力机制，支持掩码和文本两种控制模式。通过创新的“in-context editing”机制，模型能够在保持插入元素特征的同时，实现与目标图像的自然融合。实验结果显示，Insert Anything 在多个基准测试中表现优异，适用于创意内容生成、虚拟试穿和场景合成等应用。</p><p><strong>标签</strong>：#图像插入 #多模态编辑 #DiT #参考图像编辑 #虚拟试穿</p><hr><h2 id="x-fusion-冻结大语言模型的多模态扩展框架" tabindex="-1"><a class="header-anchor" href="#x-fusion-冻结大语言模型的多模态扩展框架"><span>X-Fusion：冻结大语言模型的多模态扩展框架</span></a></h2><figure><img src="https://sichengmo.github.io/XFusion/static/images/xfusion_results_gen.jpg" alt="X-Fusion Results 图" tabindex="0" loading="lazy"><figcaption>X-Fusion Results 图</figcaption></figure><p><strong>概要</strong>：<strong>X-Fusion</strong> 是由 <strong>Adobe Research</strong> 联合 <strong>UCLA</strong> 提出的多模态扩展框架，旨在在不微调大语言模型（LLM）参数的前提下，增强其视觉理解与生成能力。该方法采用双塔架构，分别处理语言和视觉信息，通过引入视觉特定的权重，实现图像到文本和文本到图像的双向任务。实验表明，X-Fusion 在多个基准测试中表现优异，尤其在图像生成质量和文本描述准确性方面，优于现有的多模态模型。</p><p><strong>标签</strong>：#多模态学习 #冻结LLM #图像生成 #文本描述 #双塔架构</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://github.com/tellurion-kanata/colorizeDiffusion" target="_blank" rel="noopener noreferrer">ColorizeDiffusion v2 GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2504.06895v1" target="_blank" rel="noopener noreferrer">ColorizeDiffusion v2 论文链接</a></li><li><a href="https://reptext.github.io/" target="_blank" rel="noopener noreferrer">RepText 项目主页</a></li><li><a href="https://github.com/Shakker-Labs/RepText" target="_blank" rel="noopener noreferrer">RepText GitHub 仓库</a></li><li><a href="https://arxiv.org/pdf/2504.19724" target="_blank" rel="noopener noreferrer">RepText 论文链接</a></li><li><a href="https://river-zhang.github.io/ICEdit-gh-pages/" target="_blank" rel="noopener noreferrer">ICEdit 项目主页</a></li><li><a href="https://github.com/River-Zhang/ICEdit" target="_blank" rel="noopener noreferrer">ICEdit GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2504.20690v1" target="_blank" rel="noopener noreferrer">ICEdit 论文链接</a></li><li><a href="https://github.com/modelscope/Nexus-Gen" target="_blank" rel="noopener noreferrer">Nexus-Gen GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2504.21356v1" target="_blank" rel="noopener noreferrer">Nexus-Gen 论文链接</a></li><li><a href="https://song-wensong.github.io/insert-anything/" target="_blank" rel="noopener noreferrer">Insert Anything 项目主页</a></li><li><a href="https://github.com/song-wensong/insert-anything" target="_blank" rel="noopener noreferrer">Insert Anything GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2504.15009v1" target="_blank" rel="noopener noreferrer">Insert Anything 论文链接</a></li><li><a href="https://sichengmo.github.io/XFusion/" target="_blank" rel="noopener noreferrer">X-Fusion 项目主页</a></li><li><a href="https://arxiv.org/html/2504.20996v1" target="_blank" rel="noopener noreferrer">X-Fusion 论文链接</a></li></ol>',40)]))}]]),a=JSON.parse('{"path":"/zh/posts/ai-weekly/036.html","title":"ColorizeDiffusion v2实现最强动漫草图上色 | RepText 多语言文本渲染 | Nexus-Gen 图像生成统一模型【AI周报】","lang":"zh-CN","frontmatter":{"description":"ColorizeDiffusion v2实现最强动漫草图上色 | RepText 多语言文本渲染 | Nexus-Gen 图像生成统一模型【AI周报】 封面源自C站作者Koal2封面源自C站作者Koal2 摘要 本周亮点：ColorizeDiffusion v2 提升动漫草图上色质量；RepText 实现多语言文本渲染；ICEdit 多视角保持身份一致...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/036.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"ColorizeDiffusion v2实现最强动漫草图上色 | RepText 多语言文本渲染 | Nexus-Gen 图像生成统一模型【AI周报】"}],["meta",{"property":"og:description","content":"ColorizeDiffusion v2实现最强动漫草图上色 | RepText 多语言文本渲染 | Nexus-Gen 图像生成统一模型【AI周报】 封面源自C站作者Koal2封面源自C站作者Koal2 摘要 本周亮点：ColorizeDiffusion v2 提升动漫草图上色质量；RepText 实现多语言文本渲染；ICEdit 多视角保持身份一致..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/44cb3616-4cff-4dea-bcb6-caeedb99ceeb/original=true,quality=90/73901561.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"ColorizeDiffusion v2实现最强动漫草图上色 | RepText 多语言文本渲染 | Nexus-Gen 图像生成统一模型【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/44cb3616-4cff-4dea-bcb6-caeedb99ceeb/original=true,quality=90/73901561.jpeg\\",\\"https://github.com/tellurion-kanata/colorizeDiffusion/raw/master/assets/teaser.png\\",\\"https://reptext.github.io/static/images/train.png\\",\\"https://github.com/River-Zhang/ICEdit/raw/main/docs/images/teaser.png\\",\\"https://github.com/modelscope/Nexus-Gen/raw/main/assets/illustrations/gen_edit.jpg\\",\\"https://song-wensong.github.io/insert-anything/InsertAnything_files/images/teaser-v4_00.png\\",\\"https://sichengmo.github.io/XFusion/static/images/xfusion_results_gen.jpg\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"ColorizeDiffusion v2：动漫草图上色的参考增强扩散模型","slug":"colorizediffusion-v2-动漫草图上色的参考增强扩散模型","link":"#colorizediffusion-v2-动漫草图上色的参考增强扩散模型","children":[]},{"level":2,"title":"RepText：多语言高保真视觉文本渲染框架","slug":"reptext-多语言高保真视觉文本渲染框架","link":"#reptext-多语言高保真视觉文本渲染框架","children":[]},{"level":2,"title":"ICEdit：高效指令图像编辑框架，支持零样本编辑与高质量输出","slug":"icedit-高效指令图像编辑框架-支持零样本编辑与高质量输出","link":"#icedit-高效指令图像编辑框架-支持零样本编辑与高质量输出","children":[]},{"level":2,"title":"Nexus-Gen：融合LLM与扩散模型的统一图像理解与生成框架","slug":"nexus-gen-融合llm与扩散模型的统一图像理解与生成框架","link":"#nexus-gen-融合llm与扩散模型的统一图像理解与生成框架","children":[]},{"level":2,"title":"Insert Anything：统一的参考图像插入框架，支持多种控制模式","slug":"insert-anything-统一的参考图像插入框架-支持多种控制模式","link":"#insert-anything-统一的参考图像插入框架-支持多种控制模式","children":[]},{"level":2,"title":"X-Fusion：冻结大语言模型的多模态扩展框架","slug":"x-fusion-冻结大语言模型的多模态扩展框架","link":"#x-fusion-冻结大语言模型的多模态扩展框架","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":6.26,"words":1878},"filePathRelative":"zh/posts/ai-weekly/036.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/44cb3616-4cff-4dea-bcb6-caeedb99ceeb/original=true,quality=90/73901561.jpeg\\" alt=\\"封面源自C站作者Koal2\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面源自C站作者Koal2</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：ColorizeDiffusion v2 提升动漫草图上色质量；RepText 实现多语言文本渲染；ICEdit 多视角保持身份一致性编辑；Insert Anything 灵活插入任意物体；Nexus-Gen 融合 LLM 与扩散模型统一生成流程；X-Fusion 实现多任务图文对齐能力。</p>","autoDesc":true}')}}]);