"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[2862],{66262:(e,n)=>{n.A=(e,n)=>{const i=e.__vccOpts||e;for(const[e,a]of n)i[e]=a;return i}},95125:(e,n,i)=>{i.r(n),i.d(n,{comp:()=>t,data:()=>s});var a=i(20641);const l={},t=(0,i(66262).A)(l,[["render",function(e,n){return(0,a.uX)(),(0,a.CE)("div",null,n[0]||(n[0]=[(0,a.Fv)('<h1 id="【论文精读】qwen-image-原生文本渲染与一致性编辑的生成基础模型" tabindex="-1"><a class="header-anchor" href="#【论文精读】qwen-image-原生文本渲染与一致性编辑的生成基础模型"><span>【论文精读】Qwen-Image：原生文本渲染与一致性编辑的生成基础模型</span></a></h1><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>Qwen-Image 为 20B MMDiT 模型，聚焦复杂文本渲染与一致性编辑。借 MSRoPE、双编码与多任务训练及数据工程，在 OneIG/GenEval/DPG、GEdit/ImgEdit 等基准领先，中文与长文本尤佳。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li><li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li><li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li><li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li></ol><hr><h2 id="背景与研究目标" tabindex="-1"><a class="header-anchor" href="#背景与研究目标"><span>背景与研究目标</span></a></h2><ol><li>背景与挑战：主流文本到图像生成模型在“复杂文本渲染（多行、段落级、双语、非拉丁文字）”与“编辑一致性（仅改动目标、语义与视觉同时保真）”上仍存在明显短板。</li><li>数据与训练鸿沟：真实图像中的文本分布长尾，中文等表意文字更稀缺；多任务编辑对结构保持与指令对齐提出更高要求。</li><li>研究目标：构建一个统一的图像生成基础模型，兼顾高保真文本渲染与高一致性编辑，并在多基准广泛验证通用能力。</li></ol><hr><h2 id="方法与创新点" tabindex="-1"><a class="header-anchor" href="#方法与创新点"><span>方法与创新点</span></a></h2><figure><img src="https://paper-assets.alphaxiv.org/figures/2508.02324v1/img-11.jpeg" alt="Overview 图" tabindex="0" loading="lazy"><figcaption>Overview 图</figcaption></figure><h3 id="_1-整体流程-骨干与条件" tabindex="-1"><a class="header-anchor" href="#_1-整体流程-骨干与条件"><span>1. 整体流程（骨干与条件）</span></a></h3><ol><li>骨干网络：Multimodal Diffusion Transformer（MMDiT）。</li><li>条件编码：冻结的 Qwen2.5-VL 提供语义特征（文本/图像），VAE 提供像素级重建特征（单编码器、双解码器；仅图像解码器在富文本图像上微调，强化小字重建）。</li><li>双路条件融合：将 Qwen2.5-VL 的语义嵌入与 VAE 的重建潜变量分别注入 MMDiT 的文本流与图像流，实现“语义一致 + 视觉保真”的编辑约束。</li></ol><h3 id="_2-关键创新点" tabindex="-1"><a class="header-anchor" href="#_2-关键创新点"><span>2. 关键创新点</span></a></h3><ol><li>MSRoPE（Multimodal Scalable RoPE）联合位置编码：将文本视为具有相同位置 ID 的 2D 张量，并沿图像网格对角线对齐，既保留图像侧分辨率缩放优势，又保持文本侧与 1D-RoPE 等效，显著提升文图对齐与可扩展性。</li><li>多任务训练范式：在 T2I（文本到图像）、TI2I（文本+图像到图像编辑）与 I2I 重建间联合优化，利用双编码输入与帧维度位置区分机制，增强复杂编辑（风格变换、局部替换、姿态操控）的一致性。</li><li>课程式训练与数据工程：七阶段过滤管线（破损/质量/对齐/文本密度/分辨率/类别平衡/多尺度），配合三类合成数据（纯渲染、复合渲染、结构模板渲染）补齐长尾字符与复杂布局；分辨率从 256→640→1328 渐进提升。</li><li>后训练对齐：SFT + DPO/GRPO 强化复杂指令遵循与可控生成（在 GenEval 等对齐基准上显著提升）。</li></ol><h3 id="_3-关键技术细节" tabindex="-1"><a class="header-anchor" href="#_3-关键技术细节"><span>3. 关键技术细节</span></a></h3><ol><li>文本渲染增强：文本语料严格渲染与质控；密集/极小字体样本过滤与平衡采样，兼顾真实与合成分布。</li><li>编辑一致性：输入图像同时进入 Qwen2.5-VL 与 VAE，两路特征在 MMDiT 端协同约束，减少无关区域漂移与语义偏移。</li><li>训练基础设施：生产者-消费者解耦（缓存已编码潜变量），配合 Megatron 混合并行，保障大规模稳定收敛。</li></ol><hr><h2 id="实验与结果分析" tabindex="-1"><a class="header-anchor" href="#实验与结果分析"><span>实验与结果分析</span></a></h2><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bench.png" alt="Qwen-Image在生成与编辑、尤其中文文本渲染方面表现卓越（官方基准总览）" tabindex="0" loading="lazy"><figcaption>Qwen-Image在生成与编辑、尤其中文文本渲染方面表现卓越（官方基准总览）</figcaption></figure><h3 id="_1-统一生成与编辑能力" tabindex="-1"><a class="header-anchor" href="#_1-统一生成与编辑能力"><span>1. 统一生成与编辑能力</span></a></h3><ol><li>AI Arena 人评：开源模型中整体第 3，领先多款商用闭源模型（Elo 优势约 30+）。</li><li>OneIG-Bench：中/英双赛道 Overall 均居首；Alignment 与 Text 维度优势显著。</li><li>GenEval：基础版已超 SOTA，经 RL 提升至 0.91，是首个突破 0.9 的基础模型。</li><li>DPG：Overall 88.32，属性/关系等细粒度维度全面领先。</li></ol><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bench.png" alt="Qwen-Image 在生成与编辑、尤其中文文本渲染方面表现卓越（官方基准总览）" tabindex="0" loading="lazy"><figcaption>Qwen-Image 在生成与编辑、尤其中文文本渲染方面表现卓越（官方基准总览）</figcaption></figure><h3 id="_2-文本渲染-英文-中文-长文本" tabindex="-1"><a class="header-anchor" href="#_2-文本渲染-英文-中文-长文本"><span>2. 文本渲染（英文/中文/长文本）</span></a></h3><table><thead><tr><th>图片</th><th>标题</th></tr></thead><tbody><tr><td><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/aliyun.png" alt="中文场景下多店铺与招牌、人物、卡片等复杂文本渲染案例" loading="lazy"></td><td>中文场景下多店铺与招牌、人物、卡片等复杂文本渲染案例</td></tr><tr><td><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/tongyi.png" alt="对联、横批、青花瓷等多元素中文文本与场景渲染" loading="lazy"></td><td>对联、横批、青花瓷等多元素中文文本与场景渲染</td></tr><tr><td><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/book.png" alt="英文书店橱窗与多本书封面、海报等多文本渲染" loading="lazy"></td><td>英文书店橱窗与多本书封面、海报等多文本渲染</td></tr><tr><td><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/six.png" alt="英文信息图表，多个模块与图标、长文本排版渲染" loading="lazy"></td><td>英文信息图表，多个模块与图标、长文本排版渲染</td></tr><tr><td><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/small.png" alt="小纸片上长段英文手写体文本渲染" loading="lazy"></td><td>小纸片上长段英文手写体文本渲染</td></tr><tr><td><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/long.png" alt="玻璃板上中英文混合长文本渲染" loading="lazy"></td><td>玻璃板上中英文混合长文本渲染</td></tr><tr><td><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bi.png" alt="玻璃板上中英文双语文本渲染" loading="lazy"></td><td>玻璃板上中英文双语文本渲染</td></tr><tr><td><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/poster.png" alt="海报风格多行英文文本与复杂画面渲染" loading="lazy"></td><td>海报风格多行英文文本与复杂画面渲染</td></tr><tr><td><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/ppt.png" alt="PPT页面多图多文本、中文标题与排版渲染" loading="lazy"></td><td>PPT页面多图多文本、中文标题与排版渲染</td></tr></tbody></table><ol><li>ChineseWord：三难度层级均第一，Overall 58.30，显著超越同类模型；</li><li>LongText-Bench：中文第一、英文第二；</li><li>CVTG-2K（英文多区域渲染）：词级准确率与可读性指标处于第一梯队。</li></ol><h3 id="_3-图像编辑与视觉理解" tabindex="-1"><a class="header-anchor" href="#_3-图像编辑与视觉理解"><span>3. 图像编辑与视觉理解</span></a></h3><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s2.jpg" alt="多风格通用图像生成能力展示" tabindex="0" loading="lazy"><figcaption>多风格通用图像生成能力展示</figcaption></figure><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s3.jpg" alt="图像编辑操作（风格变换、文本编辑、物体增删、姿态操控等）案例" tabindex="0" loading="lazy"><figcaption>图像编辑操作（风格变换、文本编辑、物体增删、姿态操控等）案例</figcaption></figure><ol><li>GEdit/ImgEdit：总体评分与细分任务均名列前茅，展示指令编辑的稳定性与保真度；</li><li>新视角合成（GSO）：在 PSNR/SSIM/LPIPS 上与部分专用方法相当，整体具竞争力；</li><li>深度估计（多数据集零样本）：接近专用判别式模型，验证“生成式理解”的有效性。</li></ol><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s3.jpg" alt="Qwen-Image 在通用图像编辑中的展示，包括风格变换、文本编辑、背景更换、物体添加、删除、替换和姿态操控等。" tabindex="0" loading="lazy"><figcaption>Qwen-Image 在通用图像编辑中的展示，包括风格变换、文本编辑、背景更换、物体添加、删除、替换和姿态操控等。</figcaption></figure><h3 id="_4-结果解读" tabindex="-1"><a class="header-anchor" href="#_4-结果解读"><span>4. 结果解读</span></a></h3><ol><li>MSRoPE + 双编码条件有效缓解“文本错漏/重影/位置跑偏”等典型问题；</li><li>多阶段数据工程与三类文本合成策略共同提升中文与长文本可读性；</li><li>多任务范式统一“生成-理解-编辑”，在跨任务迁移与一致性上具备优势。</li></ol><hr><h2 id="模型启发与方法延伸" tabindex="-1"><a class="header-anchor" href="#模型启发与方法延伸"><span>模型启发与方法延伸</span></a></h2><ol><li>生成式理解范式：通过建模视觉内容的联合分布，让“理解任务”（深度、视角）作为特定编辑目标统一到生成框架内，减少“判别器依赖”。</li><li>文本优先的视觉接口（VLUI）：当语言难以准确传达布局/颜色/空间关系时，直接生成“图文一体”的可视化解释，提高交互与表达效率。</li><li>工程可迁移性：MSRoPE 的对角拼接思想与双路条件融合，可迁移到文生视频、版式生成、UI 自动生成等需要“结构+文本”双对齐的场景。</li><li>数据治理要点：长尾字符与复杂版式需“合成+过滤+再平衡”的闭环；对非拉丁语种（如中文）的专门增强是获得实际可用性的关键。</li></ol><hr><h2 id="结论与未来展望" tabindex="-1"><a class="header-anchor" href="#结论与未来展望"><span>结论与未来展望</span></a></h2><ol><li>贡献总结：提出 MSRoPE、双编码条件与多任务训练范式，配合系统性数据工程与课程式训练，显著提升复杂文本渲染与一致性编辑能力，并在多项权威基准上验证通用领先性。</li><li>优势与不足：在中文/长文本、复杂编辑与跨任务迁移上优势明显；对极端复杂排版、稀有字体/素材与更高分辨率的一致性仍有提升空间。</li><li>展望：向视频/3D 扩展的统一生成-理解系统；增强多语言长文本与结构化模板（海报/PPT/UI）可控生成；更强的人类偏好对齐与可解释性评测。</li></ol><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h3><ol><li><a href="https://qwen.ai" target="_blank" rel="noopener noreferrer">项目主页</a></li><li><a href="https://github.com/QwenLM/Qwen-Image" target="_blank" rel="noopener noreferrer">代码仓库</a></li><li><a href="https://huggingface.co/Qwen/Qwen-Image" target="_blank" rel="noopener noreferrer">模型仓库</a></li><li><a href="https://arxiv.org/pdf/2508.02324" target="_blank" rel="noopener noreferrer">论文原文</a></li><li><a href="https://qwenlm.github.io/zh/blog/qwen-image/" target="_blank" rel="noopener noreferrer">论文解读博客/视频</a></li></ol>',43)]))}]]),s=JSON.parse('{"path":"/zh/posts/papers/qwen-image.html","title":"【论文精读】Qwen-Image：原生文本渲染与一致性编辑的生成基础模型","lang":"zh-CN","frontmatter":{"description":"【论文精读】Qwen-Image：原生文本渲染与一致性编辑的生成基础模型 摘要 Qwen-Image 为 20B MMDiT 模型，聚焦复杂文本渲染与一致性编辑。借 MSRoPE、双编码与多任务训练及数据工程，在 OneIG/GenEval/DPG、GEdit/ImgEdit 等基准领先，中文与长文本尤佳。 目录 背景与研究目标 方法与创新点 实验与结...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/papers/qwen-image.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"【论文精读】Qwen-Image：原生文本渲染与一致性编辑的生成基础模型"}],["meta",{"property":"og:description","content":"【论文精读】Qwen-Image：原生文本渲染与一致性编辑的生成基础模型 摘要 Qwen-Image 为 20B MMDiT 模型，聚焦复杂文本渲染与一致性编辑。借 MSRoPE、双编码与多任务训练及数据工程，在 OneIG/GenEval/DPG、GEdit/ImgEdit 等基准领先，中文与长文本尤佳。 目录 背景与研究目标 方法与创新点 实验与结..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://paper-assets.alphaxiv.org/figures/2508.02324v1/img-11.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"【论文精读】Qwen-Image：原生文本渲染与一致性编辑的生成基础模型\\",\\"image\\":[\\"https://paper-assets.alphaxiv.org/figures/2508.02324v1/img-11.jpeg\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bench.png\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bench.png\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/aliyun.png\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/tongyi.png\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/book.png\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/six.png\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/small.png\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/long.png\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bi.png\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/poster.png\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/ppt.png\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s2.jpg\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s3.jpg\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s3.jpg\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"背景与研究目标","slug":"背景与研究目标","link":"#背景与研究目标","children":[]},{"level":2,"title":"方法与创新点","slug":"方法与创新点","link":"#方法与创新点","children":[{"level":3,"title":"1. 整体流程（骨干与条件）","slug":"_1-整体流程-骨干与条件","link":"#_1-整体流程-骨干与条件","children":[]},{"level":3,"title":"2. 关键创新点","slug":"_2-关键创新点","link":"#_2-关键创新点","children":[]},{"level":3,"title":"3. 关键技术细节","slug":"_3-关键技术细节","link":"#_3-关键技术细节","children":[]}]},{"level":2,"title":"实验与结果分析","slug":"实验与结果分析","link":"#实验与结果分析","children":[{"level":3,"title":"1. 统一生成与编辑能力","slug":"_1-统一生成与编辑能力","link":"#_1-统一生成与编辑能力","children":[]},{"level":3,"title":"2. 文本渲染（英文/中文/长文本）","slug":"_2-文本渲染-英文-中文-长文本","link":"#_2-文本渲染-英文-中文-长文本","children":[]},{"level":3,"title":"3. 图像编辑与视觉理解","slug":"_3-图像编辑与视觉理解","link":"#_3-图像编辑与视觉理解","children":[]},{"level":3,"title":"4. 结果解读","slug":"_4-结果解读","link":"#_4-结果解读","children":[]}]},{"level":2,"title":"模型启发与方法延伸","slug":"模型启发与方法延伸","link":"#模型启发与方法延伸","children":[]},{"level":2,"title":"结论与未来展望","slug":"结论与未来展望","link":"#结论与未来展望","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":7.5,"words":2250},"filePathRelative":"zh/posts/papers/qwen-image.md","excerpt":"\\n<h2>摘要</h2>\\n<p>Qwen-Image 为 20B MMDiT 模型，聚焦复杂文本渲染与一致性编辑。借 MSRoPE、双编码与多任务训练及数据工程，在 OneIG/GenEval/DPG、GEdit/ImgEdit 等基准领先，中文与长文本尤佳。</p>\\n<hr>\\n<h2>目录</h2>\\n<ol>\\n<li><a href=\\"#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87\\">背景与研究目标</a></li>\\n<li><a href=\\"#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9\\">方法与创新点</a></li>\\n<li><a href=\\"#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90\\">实验与结果分析</a></li>\\n<li><a href=\\"#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8\\">模型启发与方法延伸</a></li>\\n<li><a href=\\"#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B\\">结论与未来展望</a></li>\\n</ol>","autoDesc":true}')}}]);