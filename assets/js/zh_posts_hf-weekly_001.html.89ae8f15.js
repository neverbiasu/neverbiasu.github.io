"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[2747],{66262:(e,n)=>{n.A=(e,n)=>{const a=e.__vccOpts||e;for(const[e,t]of n)a[e]=t;return a}},1967:(e,n,a)=>{a.r(n),a.d(n,{comp:()=>r,data:()=>i});var t=a(20641);const o={},r=(0,a(66262).A)(o,[["render",function(e,n){return(0,t.uX)(),(0,t.CE)("div",null,n[0]||(n[0]=[(0,t.Fv)('<h1 id="flux-1-kontext-dev精控编辑-hunyuan-a13b超长moe-kimi-vl-a3b高效多模态【hf周报】" tabindex="-1"><a class="header-anchor" href="#flux-1-kontext-dev精控编辑-hunyuan-a13b超长moe-kimi-vl-a3b高效多模态【hf周报】"><span>FLUX.1 Kontext-dev精控编辑 | Hunyuan-A13B超长MoE | Kimi-VL-A3B高效多模态【HF周报】</span></a></h1><figure><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG" alt="封面图" tabindex="0" loading="lazy"><figcaption>封面图</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：FLUX多轮一致性编辑、Hunyuan-A13B超长MoE、Kimi-VL-A3B多模态推理等热搜模型持续引领AI社区，涵盖多模态生成、超长上下文、OCR等最新开源进展，推动AI能力边界不断拓展。详见正文，相关参考链接请见文末。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#flux1-kontext-dev%E5%BC%80%E6%BA%90%E9%AB%98%E4%B8%80%E8%87%B4%E6%80%A7%E5%A4%9A%E6%A8%A1%E6%80%81%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91">FLUX.1 Kontext-dev：开源高一致性多模态图像编辑</a></li><li><a href="#hunyuan-a13b-instruct%E8%85%BE%E8%AE%AF%E9%AB%98%E6%95%88moe%E4%B8%AD%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B">Hunyuan-A13B-Instruct：腾讯高效MoE中文大模型</a></li><li><a href="#magenta-realtimegoogle%E5%BC%80%E6%94%BE%E5%AE%9E%E6%97%B6%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">Magenta Realtime：Google开放实时音乐生成模型</a></li><li><a href="#nanonets-ocr-s%E7%BB%93%E6%9E%84%E5%8C%96%E6%99%BA%E8%83%BDocr%E4%B8%8E%E8%AF%AD%E4%B9%89%E6%A0%87%E6%B3%A8">Nanonets-OCR-s：结构化智能OCR与语义标注</a></li><li><a href="#gemma-3n-e4b-itgoogle%E9%AB%98%E6%95%88%E5%A4%9A%E6%A8%A1%E6%80%81%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B">Gemma-3n-E4B-it：Google高效多模态开源模型</a></li><li><a href="#omnigen2%E7%BB%9F%E4%B8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E7%94%9F%E6%88%90%E4%B8%8E%E7%BC%96%E8%BE%91%E7%9A%84%E9%AB%98%E6%95%88%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B">OmniGen2：统一多模态生成与编辑的高效开源模型</a></li><li><a href="#kimi-vl-a3b-thinking-2506moonshotai%E9%AB%98%E6%95%88%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">Kimi-VL-A3B-Thinking-2506：MoonshotAI高效长上下文视觉语言模型</a></li><li><a href="#jan-nano-128kmenlo-research%E8%B6%85%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E8%BD%BB%E9%87%8F%E6%A8%A1%E5%9E%8B">Jan-nano-128k：Menlo Research超长上下文轻量模型</a></li></ol><hr><h2 id="flux-1-kontext-dev-开源高一致性多模态图像编辑" tabindex="-1"><a class="header-anchor" href="#flux-1-kontext-dev-开源高一致性多模态图像编辑"><span>FLUX.1 Kontext-dev：开源高一致性多模态图像编辑</span></a></h2><figure><img src="https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev/resolve/main/teaser.png" alt="FLUX.1 Kontext Teaser 图" tabindex="0" loading="lazy"><figcaption>FLUX.1 Kontext Teaser 图</figcaption></figure><p><strong>概要</strong>：Black Forest Labs 发布 FLUX.1 Kontext-dev，12B参数的流匹配变换器，专注于高一致性多轮图像编辑。支持文本指令驱动的图像内容修改，具备角色、风格、物体参考能力，无需微调即可实现多轮精细编辑且视觉漂移极小。模型采用指导蒸馏训练，效率高，开源权重支持学术和非商用，适配 ComfyUI、Diffusers 等主流推理框架，并针对 NVIDIA Blackwell 架构优化。官方还提供内容安全过滤、内容溯源等多重安全机制。</p><p><strong>标签</strong>：#BlackForestLabs #FLUX1Kontext #流匹配变换器 #多模态图像编辑 #ComfyUI</p><hr><h2 id="hunyuan-a13b-instruct-腾讯高效moe中文大模型" tabindex="-1"><a class="header-anchor" href="#hunyuan-a13b-instruct-腾讯高效moe中文大模型"><span>Hunyuan-A13B-Instruct：腾讯高效MoE中文大模型</span></a></h2><figure><img src="https://pbs.twimg.com/media/GucF9LjbAAASd8c?format=jpg&amp;name=medium" alt="Hunyuan-A13B Benchmark 图" tabindex="0" loading="lazy"><figcaption>Hunyuan-A13B Benchmark 图</figcaption></figure><p><strong>概要</strong>：腾讯开源 Hunyuan-A13B-Instruct，采用80B参数MoE架构，13B激活参数，兼顾高性能与资源效率。原生支持256K超长上下文，具备快/慢思维切换、Agent任务优化、分组查询注意力（GQA）、多量化格式等特性。模型在数学、科学、Agent等多项基准测试中表现领先，支持 Huggingface Transformers、TensorRT-LLM、vLLM、SGLang 等多种推理和部署方式。官方同步开放技术报告和训练/推理手册。</p><p><strong>标签</strong>：#Tencent #HunyuanA13B #MoE架构 #超长上下文 #Agent优化</p><hr><h2 id="magenta-realtime-google开放实时音乐生成模型" tabindex="-1"><a class="header-anchor" href="#magenta-realtime-google开放实时音乐生成模型"><span>Magenta Realtime：Google开放实时音乐生成模型</span></a></h2><figure><img src="https://raw.githubusercontent.com/magenta/magenta-realtime/main/notebooks/diagram.gif" alt="Magenta Realtime Live 图" tabindex="0" loading="lazy"><figcaption>Magenta Realtime Live 图</figcaption></figure><p><strong>概要</strong>：Google Magenta 团队发布 Magenta RealTime，面向实时音乐生成场景。模型由 SpectroStream 音频编码器、MusicCoCa 音乐-文本对比嵌入、Transformer LLM 三部分组成，支持文本、音频、风格混合等多模态输入，能连续生成高保真音乐片段。适用于现场表演、辅助音乐创作、游戏配乐、音乐教育等场景。模型权重和代码分别采用 Apache 2.0 和 CC-BY-4.0 许可，支持本地TPU/GPU部署。</p><p><strong>标签</strong>：#Google #MagentaRealtime #SpectroStream #MusicCoCa #实时音乐生成</p><hr><h2 id="nanonets-ocr-s-结构化智能ocr与语义标注" tabindex="-1"><a class="header-anchor" href="#nanonets-ocr-s-结构化智能ocr与语义标注"><span>Nanonets-OCR-s：结构化智能OCR与语义标注</span></a></h2><figure><img src="https://nanonets.com/research/nanonets-ocr-s/assets/nanonets_logo.webp" alt="Nanonets Logo 图" tabindex="0" loading="lazy"><figcaption>Nanonets Logo 图</figcaption></figure><p><strong>概要</strong>：Nanonets-OCR-s 是一款基于 Qwen2.5-VL-3B 的视觉语言模型，专为结构化文档OCR设计。支持LaTeX公式识别、智能图片描述、签名/水印检测、表格/复选框提取等，输出结构化Markdown，极大提升下游LLM处理效率。模型在学术、金融、医疗、企业等多行业文档场景表现优异，支持 Transformers、vLLM、docext 等多种推理方式。</p><p><strong>标签</strong>：#Nanonets #NanonetsOCRs #Qwen2.5VL3B #结构化OCR #语义标注</p><hr><h2 id="gemma-3n-e4b-it-google高效多模态开源模型" tabindex="-1"><a class="header-anchor" href="#gemma-3n-e4b-it-google高效多模态开源模型"><span>Gemma-3n-E4B-it：Google高效多模态开源模型</span></a></h2><figure><img src="https://ai.google.dev/static/gemma/docs/images/gemma-3n-parameters.png" alt="Gemma-3n Params 图" tabindex="0" loading="lazy"><figcaption>Gemma-3n Params 图</figcaption></figure><p><strong>概要</strong>：Gemma-3n-E4B-it 是 Google 最新开源的多模态生成模型，支持文本、图像、音频、视频输入，具备32K上下文窗口。采用MatFormer嵌套子模型架构与PLE参数缓存，极大降低推理资源消耗，适配手机、PC等低资源设备。模型在140+语言、代码、数学、视觉等多领域训练，支持条件参数加载，灵活扩展。官方开放权重，支持商业用途，适配 Huggingface Transformers，适合多模态内容理解、生成、对话、音视频分析等场景。</p><p><strong>标签</strong>：#Google #Gemma3n #MatFormer #多模态 #高效推理</p><hr><h2 id="omnigen2-统一多模态生成与编辑的高效开源模型" tabindex="-1"><a class="header-anchor" href="#omnigen2-统一多模态生成与编辑的高效开源模型"><span>OmniGen2：统一多模态生成与编辑的高效开源模型</span></a></h2><figure><img src="https://huggingface.co/OmniGen2/OmniGen2/resolve/main/assets/teaser.jpg" alt="OmniGen2 Teaser 图" tabindex="0" loading="lazy"><figcaption>OmniGen2 Teaser 图</figcaption></figure><p><strong>概要</strong>：OmniGen2 是 VectorSpaceLab 发布的统一多模态生成模型，具备文本-图像双解码通路，参数不共享，图像分词器独立。支持视觉理解、文本生成图像、指令驱动图像编辑、上下文组合生成等能力，性能在开源同类模型中领先。支持CPU/显卡推理，资源占用低，适配Diffusers、Gradio等生态。官方已开放权重、技术报告和多种在线/本地Demo，适合多模态内容创作、编辑、理解等场景。</p><p><strong>标签</strong>：#VectorSpaceLab #OmniGen2 #多模态生成 #图像编辑 #Diffusers</p><hr><h2 id="kimi-vl-a3b-thinking-2506-moonshotai高效长上下文视觉语言模型" tabindex="-1"><a class="header-anchor" href="#kimi-vl-a3b-thinking-2506-moonshotai高效长上下文视觉语言模型"><span>Kimi-VL-A3B-Thinking-2506：MoonshotAI高效长上下文视觉语言模型</span></a></h2><figure><img src="https://arxiv.org/html/2504.07491v3/x1.png" alt="Kimi-VL-A3B-Thinking-2506 Comparison 图" tabindex="0" loading="lazy"><figcaption>Kimi-VL-A3B-Thinking-2506 Comparison 图</figcaption></figure><p><strong>概要</strong>：Kimi-VL-A3B-Thinking-2506 是 MoonshotAI 发布的高效MoE视觉语言模型，激活参数仅3B，支持128K超长上下文。该版本在多模态推理、视觉理解、视频分析、长文档处理等任务上大幅提升，支持高分辨率输入，推理效率高。模型在MathVision、MMBench、MMStar、VideoMMMU等多项基准测试中表现优异，适配VLLM、Transformers等主流推理框架，支持多轮推理与思维链输出。</p><p><strong>标签</strong>：#MoonshotAI #KimiVL #MoE #视觉语言模型 #长上下文</p><hr><h2 id="jan-nano-128k-menlo-research超长上下文轻量模型" tabindex="-1"><a class="header-anchor" href="#jan-nano-128k-menlo-research超长上下文轻量模型"><span>Jan-nano-128k：Menlo Research超长上下文轻量模型</span></a></h2><figure><img src="https://huggingface.co/Menlo/Jan-nano-128k/resolve/main/replay.gif" alt="Jan-nano-128k Live 图" tabindex="0" loading="lazy"><figcaption>Jan-nano-128k Live 图</figcaption></figure><p><strong>概要</strong>：Jan-nano-128k 是 Menlo Research 推出的轻量级大模型，原生支持128K超长上下文窗口，专为深度文档分析、长对话、多文档推理等研究场景设计。采用创新的上下文扩展机制，长文本推理性能无衰减，兼容MCP协议，适配vLLM、llama.cpp等推理框架。</p><p><strong>标签</strong>：#MenloResearch #JanNano128k #超长上下文 #轻量模型 #MCP</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h3><ol><li><a href="https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev" target="_blank" rel="noopener noreferrer">FLUX.1 Kontext-dev Huggingface</a></li><li><a href="https://bfl.ai/announcements/flux-1-kontext-dev" target="_blank" rel="noopener noreferrer">FLUX.1 Kontext-dev 官方公告</a></li><li><a href="https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-kontext-dev" target="_blank" rel="noopener noreferrer">FLUX.1 Kontext-dev ComfyUI 教程</a></li><li><a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct" target="_blank" rel="noopener noreferrer">Hunyuan-A13B-Instruct Huggingface</a></li><li><a href="https://github.com/Tencent-Hunyuan/Hunyuan-A13B" target="_blank" rel="noopener noreferrer">Hunyuan-A13B-Instruct GitHub</a></li><li><a href="https://huggingface.co/google/magenta-realtime" target="_blank" rel="noopener noreferrer">Magenta Realtime Huggingface</a></li><li><a href="https://github.com/magenta/magenta-realtime" target="_blank" rel="noopener noreferrer">Magenta Realtime GitHub</a></li><li><a href="https://colab.research.google.com/github/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Demo.ipynb" target="_blank" rel="noopener noreferrer">Magenta Realtime Colab Demo</a></li><li><a href="https://huggingface.co/nanonets/Nanonets-OCR-s" target="_blank" rel="noopener noreferrer">Nanonets-OCR-s Huggingface</a></li><li><a href="https://nanonets.com/research/nanonets-ocr-s/" target="_blank" rel="noopener noreferrer">Nanonets-OCR-s 官方介绍</a></li><li><a href="https://github.com/NanoNets/docext" target="_blank" rel="noopener noreferrer">docext Github</a></li><li><a href="https://huggingface.co/google/gemma-3n-E4B-it" target="_blank" rel="noopener noreferrer">Gemma-3n-E4B-it Huggingface</a></li><li><a href="https://ai.google.dev/gemma/docs/gemma-3n" target="_blank" rel="noopener noreferrer">Gemma-3n-E4B-it 官方文档</a></li><li><a href="https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide" target="_blank" rel="noopener noreferrer">Gemma-3n 技术博客</a></li><li><a href="https://huggingface.co/OmniGen2/OmniGen2" target="_blank" rel="noopener noreferrer">OmniGen2 Huggingface</a></li><li><a href="https://arxiv.org/abs/2506.18871" target="_blank" rel="noopener noreferrer">OmniGen2 技术报告</a></li><li><a href="https://huggingface.co/spaces/OmniGen2/OmniGen2" target="_blank" rel="noopener noreferrer">OmniGen2 Gradio Demo</a></li><li><a href="https://github.com/VectorSpaceLab/OmniGen2" target="_blank" rel="noopener noreferrer">OmniGen2 GitHub</a></li><li><a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking-2506" target="_blank" rel="noopener noreferrer">Kimi-VL-A3B-Thinking-2506 Huggingface</a></li><li><a href="https://arxiv.org/abs/2504.07491" target="_blank" rel="noopener noreferrer">Kimi-VL 技术报告</a></li><li><a href="https://github.com/MoonshotAI/Kimi-VL" target="_blank" rel="noopener noreferrer">Kimi-VL GitHub</a></li><li><a href="https://huggingface.co/blog/moonshotai/kimi-vl-a3b-thinking-2506" target="_blank" rel="noopener noreferrer">Kimi-VL 官方博客</a></li><li><a href="https://huggingface.co/Menlo/Jan-nano-128k" target="_blank" rel="noopener noreferrer">Jan-nano-128k Huggingface</a></li><li><a href="https://github.com/menloresearch/deep-research" target="_blank" rel="noopener noreferrer">Jan-nano-128k GitHub</a></li><li><a href="https://menloresearch.github.io/deep-research/" target="_blank" rel="noopener noreferrer">Jan-nano-128k 官方文档</a></li></ol>',50)]))}]]),i=JSON.parse('{"path":"/zh/posts/hf-weekly/001.html","title":"FLUX.1 Kontext-dev精控编辑 | Hunyuan-A13B超长MoE | Kimi-VL-A3B高效多模态【HF周报】","lang":"zh-CN","frontmatter":{"description":"FLUX.1 Kontext-dev精控编辑 | Hunyuan-A13B超长MoE | Kimi-VL-A3B高效多模态【HF周报】 封面图封面图 摘要 本周亮点：FLUX多轮一致性编辑、Hunyuan-A13B超长MoE、Kimi-VL-A3B多模态推理等热搜模型持续引领AI社区，涵盖多模态生成、超长上下文、OCR等最新开源进展，推动AI能力边界不...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/hf-weekly/001.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"FLUX.1 Kontext-dev精控编辑 | Hunyuan-A13B超长MoE | Kimi-VL-A3B高效多模态【HF周报】"}],["meta",{"property":"og:description","content":"FLUX.1 Kontext-dev精控编辑 | Hunyuan-A13B超长MoE | Kimi-VL-A3B高效多模态【HF周报】 封面图封面图 摘要 本周亮点：FLUX多轮一致性编辑、Hunyuan-A13B超长MoE、Kimi-VL-A3B多模态推理等热搜模型持续引领AI社区，涵盖多模态生成、超长上下文、OCR等最新开源进展，推动AI能力边界不..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"FLUX.1 Kontext-dev精控编辑 | Hunyuan-A13B超长MoE | Kimi-VL-A3B高效多模态【HF周报】\\",\\"image\\":[\\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\\",\\"https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev/resolve/main/teaser.png\\",\\"https://pbs.twimg.com/media/GucF9LjbAAASd8c?format=jpg&name=medium\\",\\"https://raw.githubusercontent.com/magenta/magenta-realtime/main/notebooks/diagram.gif\\",\\"https://nanonets.com/research/nanonets-ocr-s/assets/nanonets_logo.webp\\",\\"https://ai.google.dev/static/gemma/docs/images/gemma-3n-parameters.png\\",\\"https://huggingface.co/OmniGen2/OmniGen2/resolve/main/assets/teaser.jpg\\",\\"https://arxiv.org/html/2504.07491v3/x1.png\\",\\"https://huggingface.co/Menlo/Jan-nano-128k/resolve/main/replay.gif\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"FLUX.1 Kontext-dev：开源高一致性多模态图像编辑","slug":"flux-1-kontext-dev-开源高一致性多模态图像编辑","link":"#flux-1-kontext-dev-开源高一致性多模态图像编辑","children":[]},{"level":2,"title":"Hunyuan-A13B-Instruct：腾讯高效MoE中文大模型","slug":"hunyuan-a13b-instruct-腾讯高效moe中文大模型","link":"#hunyuan-a13b-instruct-腾讯高效moe中文大模型","children":[]},{"level":2,"title":"Magenta Realtime：Google开放实时音乐生成模型","slug":"magenta-realtime-google开放实时音乐生成模型","link":"#magenta-realtime-google开放实时音乐生成模型","children":[]},{"level":2,"title":"Nanonets-OCR-s：结构化智能OCR与语义标注","slug":"nanonets-ocr-s-结构化智能ocr与语义标注","link":"#nanonets-ocr-s-结构化智能ocr与语义标注","children":[]},{"level":2,"title":"Gemma-3n-E4B-it：Google高效多模态开源模型","slug":"gemma-3n-e4b-it-google高效多模态开源模型","link":"#gemma-3n-e4b-it-google高效多模态开源模型","children":[]},{"level":2,"title":"OmniGen2：统一多模态生成与编辑的高效开源模型","slug":"omnigen2-统一多模态生成与编辑的高效开源模型","link":"#omnigen2-统一多模态生成与编辑的高效开源模型","children":[]},{"level":2,"title":"Kimi-VL-A3B-Thinking-2506：MoonshotAI高效长上下文视觉语言模型","slug":"kimi-vl-a3b-thinking-2506-moonshotai高效长上下文视觉语言模型","link":"#kimi-vl-a3b-thinking-2506-moonshotai高效长上下文视觉语言模型","children":[]},{"level":2,"title":"Jan-nano-128k：Menlo Research超长上下文轻量模型","slug":"jan-nano-128k-menlo-research超长上下文轻量模型","link":"#jan-nano-128k-menlo-research超长上下文轻量模型","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":6.31,"words":1894},"filePathRelative":"zh/posts/hf-weekly/001.md","excerpt":"\\n<figure><img src=\\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\\" alt=\\"封面图\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面图</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：FLUX多轮一致性编辑、Hunyuan-A13B超长MoE、Kimi-VL-A3B多模态推理等热搜模型持续引领AI社区，涵盖多模态生成、超长上下文、OCR等最新开源进展，推动AI能力边界不断拓展。详见正文，相关参考链接请见文末。</p>","autoDesc":true}')}}]);