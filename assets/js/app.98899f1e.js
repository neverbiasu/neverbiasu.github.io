"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[3524],{61709:(e,t,a)=>{a.d(t,{l:()=>n});const n={category:{"/":{path:"/category/",map:{Guide:{path:"/category/guide/",indexes:[0,1,2,3,4,5]},"Generative AI":{path:"/category/generative-ai/",indexes:[6,7]},Explainer:{path:"/category/explainer/",indexes:[7]},News:{path:"/category/news/",indexes:[8]},"AI/ML":{path:"/category/aiml/",indexes:[9]},Reprints:{path:"/category/reprints/",indexes:[10,11]},"Model Development":{path:"/category/model-development/",indexes:[12,13,14]},reprint:{path:"/category/reprint/",indexes:[15,16,12,17]},"Model Training":{path:"/category/model-training/",indexes:[13,14]},"Image Generation":{path:"/category/image-generation/",indexes:[13,14]},"Anime Style":{path:"/category/anime-style/",indexes:[13,14]},Novice:{path:"/category/novice/",indexes:[18]},"AI Tools":{path:"/category/ai-tools/",indexes:[15]},reprints:{path:"/category/reprints/",indexes:[19,20]},Papers:{path:"/category/papers/",indexes:[21]},Advanced:{path:"/category/advanced/",indexes:[22]}}},"/zh/":{path:"/zh/category/",map:{使用指南:{path:"/zh/category/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/",indexes:[23,24,25,26,27]},指南:{path:"/zh/category/%E6%8C%87%E5%8D%97/",indexes:[28]},日记:{path:"/zh/category/%E6%97%A5%E8%AE%B0/",indexes:[29,30]},论文精读:{path:"/zh/category/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/",indexes:[31,32,33,34]},视频生成:{path:"/zh/category/%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90/",indexes:[33]},张吕敏:{path:"/zh/category/%E5%BC%A0%E5%90%95%E6%95%8F/",indexes:[33]},图像生成:{path:"/zh/category/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/",indexes:[35,34]},创造者工坊:{path:"/zh/category/%E5%88%9B%E9%80%A0%E8%80%85%E5%B7%A5%E5%9D%8A/",indexes:[36]},生成式AI:{path:"/zh/category/%E7%94%9F%E6%88%90%E5%BC%8Fai/",indexes:[37]},Explainer:{path:"/zh/category/explainer/",indexes:[38]},"Generative AI":{path:"/zh/category/generative-ai/",indexes:[38]},新闻:{path:"/zh/category/%E6%96%B0%E9%97%BB/",indexes:[39]},"AI/ML":{path:"/zh/category/aiml/",indexes:[40]},转载:{path:"/zh/category/%E8%BD%AC%E8%BD%BD/",indexes:[41,42,43]},模型开发:{path:"/zh/category/%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91/",indexes:[41]},"Model Development":{path:"/zh/category/model-development/",indexes:[44]},"Model Training":{path:"/zh/category/model-training/",indexes:[44]},"Image Generation":{path:"/zh/category/image-generation/",indexes:[44]},"Anime Style":{path:"/zh/category/anime-style/",indexes:[44]},模型研发:{path:"/zh/category/%E6%A8%A1%E5%9E%8B%E7%A0%94%E5%8F%91/",indexes:[35]},模型训练:{path:"/zh/category/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/",indexes:[35]},动漫风格:{path:"/zh/category/%E5%8A%A8%E6%BC%AB%E9%A3%8E%E6%A0%BC/",indexes:[35]},初学者:{path:"/zh/category/%E5%88%9D%E5%AD%A6%E8%80%85/",indexes:[45]},reprints:{path:"/zh/category/reprints/",indexes:[46,47]},论文:{path:"/zh/category/%E8%AE%BA%E6%96%87/",indexes:[48]},高级:{path:"/zh/category/%E9%AB%98%E7%BA%A7/",indexes:[49]},reprint:{path:"/zh/category/reprint/",indexes:[50]},教程指南:{path:"/zh/category/%E6%95%99%E7%A8%8B%E6%8C%87%E5%8D%97/",indexes:[51]},思考:{path:"/zh/category/%E6%80%9D%E8%80%83/",indexes:[52]}}}},tag:{"/":{path:"/tag/",map:{disable:{path:"/tag/disable/",indexes:[2]},encryption:{path:"/tag/encryption/",indexes:[3]},Layout:{path:"/tag/layout/",indexes:[4]},Markdown:{path:"/tag/markdown/",indexes:[5]},"Page config":{path:"/tag/page-config/",indexes:[0]},Guide:{path:"/tag/guide/",indexes:[0]},Art:{path:"/tag/art/",indexes:[6,19,20]},"Artificial Intelligence":{path:"/tag/artificial-intelligence/",indexes:[6,7]},Creators:{path:"/tag/creators/",indexes:[6]},GTC:{path:"/tag/gtc/",indexes:[6]},"resource guide":{path:"/tag/resource-guide/",indexes:[53]},script:{path:"/tag/script/",indexes:[53]},"stable diffusion":{path:"/tag/stable-diffusion/",indexes:[53]},merge:{path:"/tag/merge/",indexes:[53]},model:{path:"/tag/model/",indexes:[53]},editor:{path:"/tag/editor/",indexes:[54]},"Model Context Protocol":{path:"/tag/model-context-protocol/",indexes:[54]},tech:{path:"/tag/tech/",indexes:[54]},Inference:{path:"/tag/inference/",indexes:[7]},AI:{path:"/tag/ai/",indexes:[8,17,55,56,57]},"Image Generation":{path:"/tag/image-generation/",indexes:[8,12,10]},FLUX:{path:"/tag/flux/",indexes:[8,9]},"Machine Learning":{path:"/tag/machine-learning/",indexes:[8,58]},"FLUX.1":{path:"/tag/flux.1/",indexes:[59]},Optimization:{path:"/tag/optimization/",indexes:[59]},TaylorSeer:{path:"/tag/taylorseer/",indexes:[59]},Replicate:{path:"/tag/replicate/",indexes:[59]},LoRA:{path:"/tag/lora/",indexes:[22,9]},QLoRA:{path:"/tag/qlora/",indexes:[9]},"Fine-tuning":{path:"/tag/fine-tuning/",indexes:[9]},diffusers:{path:"/tag/diffusers/",indexes:[9]},quantization:{path:"/tag/quantization/",indexes:[9]},"Generative AI":{path:"/tag/generative-ai/",indexes:[10]},"Game Development":{path:"/tag/game-development/",indexes:[10]},"Stable Diffusion":{path:"/tag/stable-diffusion/",indexes:[10]},AWS:{path:"/tag/aws/",indexes:[10]},"Amazon Bedrock":{path:"/tag/amazon-bedrock/",indexes:[10]},Illustrious:{path:"/tag/illustrious/",indexes:[12,13,14]},LU:{path:"/tag/lu/",indexes:[12]},Lumina:{path:"/tag/lumina/",indexes:[12]},"AI Model":{path:"/tag/ai-model/",indexes:[12]},Training:{path:"/tag/training/",indexes:[12]},SDXL:{path:"/tag/sdxl/",indexes:[22,13,14]},"2048 Resolution":{path:"/tag/2048-resolution/",indexes:[13]},vpred:{path:"/tag/vpred/",indexes:[13]},"epsilon prediction":{path:"/tag/epsilon-prediction/",indexes:[13]},Anime:{path:"/tag/anime/",indexes:[14]},"Base model":{path:"/tag/base-model/",indexes:[14]},"Image generation":{path:"/tag/image-generation/",indexes:[14]},"Computer Vision":{path:"/tag/computer-vision/",indexes:[60]},"Image Recognition":{path:"/tag/image-recognition/",indexes:[60]},Prompt:{path:"/tag/prompt/",indexes:[18]},text2image:{path:"/tag/text2image/",indexes:[18]},LLM:{path:"/tag/llm/",indexes:[15,17]},Protocol:{path:"/tag/protocol/",indexes:[17]},Debate:{path:"/tag/debate/",indexes:[17]},StableDiffusion:{path:"/tag/stablediffusion/",indexes:[55,56,57]},ModelMerge:{path:"/tag/modelmerge/",indexes:[56,57]},AUTOMATIC1111:{path:"/tag/automatic1111/",indexes:[55,56,57]},MCP:{path:"/tag/mcp/",indexes:[15,16]},"News Agents":{path:"/tag/news-agents/",indexes:[15]},tmux:{path:"/tag/tmux/",indexes:[15]},"Amazon Q":{path:"/tag/amazon-q/",indexes:[15]},"Agent Systems":{path:"/tag/agent-systems/",indexes:[15]},Drawing:{path:"/tag/drawing/",indexes:[19,20]},Fundamentals:{path:"/tag/fundamentals/",indexes:[19,20]},niji:{path:"/tag/niji/",indexes:[19,20]},"Art Lesson":{path:"/tag/art-lesson/",indexes:[11,21]},"Visual Hierarchy":{path:"/tag/visual-hierarchy/",indexes:[11]},Notan:{path:"/tag/notan/",indexes:[21]},"Feature Announcement":{path:"/tag/feature-announcement/",indexes:[61]},"Usage Guide":{path:"/tag/usage-guide/",indexes:[61]},"Kohya SS GUI":{path:"/tag/kohya-ss-gui/",indexes:[22]},Qwen:{path:"/tag/qwen/",indexes:[16]},Qwen3:{path:"/tag/qwen3/",indexes:[16]},"Alibaba AI research":{path:"/tag/alibaba-ai-research/",indexes:[16]},"Alibaba AI":{path:"/tag/alibaba-ai/",indexes:[16]},"large language models":{path:"/tag/large-language-models/",indexes:[16]},LLMs:{path:"/tag/llms/",indexes:[16]},"LLM benchmarks":{path:"/tag/llm-benchmarks/",indexes:[16]},"Multilingual AI":{path:"/tag/multilingual-ai/",indexes:[16]},"Multimodal AI":{path:"/tag/multimodal-ai/",indexes:[16]},"AI reasoning":{path:"/tag/ai-reasoning/",indexes:[16]},"Diffusion Models":{path:"/tag/diffusion-models/",indexes:[58]},ModelMerging:{path:"/tag/modelmerging/",indexes:[55]}}},"/zh/":{path:"/zh/tag/",map:{禁用:{path:"/zh/tag/%E7%A6%81%E7%94%A8/",indexes:[25]},加密:{path:"/zh/tag/%E5%8A%A0%E5%AF%86/",indexes:[26]},布局:{path:"/zh/tag/%E5%B8%83%E5%B1%80/",indexes:[28]},Markdown:{path:"/zh/tag/markdown/",indexes:[27]},页面配置:{path:"/zh/tag/%E9%A1%B5%E9%9D%A2%E9%85%8D%E7%BD%AE/",indexes:[23]},使用指南:{path:"/zh/tag/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/",indexes:[23,62]},随想:{path:"/zh/tag/%E9%9A%8F%E6%83%B3/",indexes:[29,30]},开源:{path:"/zh/tag/%E5%BC%80%E6%BA%90/",indexes:[30]},大会:{path:"/zh/tag/%E5%A4%A7%E4%BC%9A/",indexes:[30]},实习:{path:"/zh/tag/%E5%AE%9E%E4%B9%A0/",indexes:[30]},做饭:{path:"/zh/tag/%E5%81%9A%E9%A5%AD/",indexes:[29]},公众号:{path:"/zh/tag/%E5%85%AC%E4%BC%97%E5%8F%B7/",indexes:[29]},ComfyUI:{path:"/zh/tag/comfyui/",indexes:[32]},AIGC:{path:"/zh/tag/aigc/",indexes:[31,32]},LLM:{path:"/zh/tag/llm/",indexes:[32,43]},"Workflow Generation":{path:"/zh/tag/workflow-generation/",indexes:[32]},"Reasoning Model":{path:"/zh/tag/reasoning-model/",indexes:[32]},FramePack:{path:"/zh/tag/framepack/",indexes:[33]},视频生成:{path:"/zh/tag/%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90/",indexes:[33]},扩散模型:{path:"/zh/tag/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/",indexes:[33,34,63]},输入预处理:{path:"/zh/tag/%E8%BE%93%E5%85%A5%E9%A2%84%E5%A4%84%E7%90%86/",indexes:[33]},多模态:{path:"/zh/tag/%E5%A4%9A%E6%A8%A1%E6%80%81/",indexes:[31]},"Ovis-U1":{path:"/zh/tag/ovis-u1/",indexes:[31]},阿里巴巴:{path:"/zh/tag/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4/",indexes:[31]},SDO:{path:"/zh/tag/sdo/",indexes:[34]},反向传播:{path:"/zh/tag/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/",indexes:[34]},计算优化:{path:"/zh/tag/%E8%AE%A1%E7%AE%97%E4%BC%98%E5%8C%96/",indexes:[34]},可控生成:{path:"/zh/tag/%E5%8F%AF%E6%8E%A7%E7%94%9F%E6%88%90/",indexes:[34]},技术教程:{path:"/zh/tag/%E6%8A%80%E6%9C%AF%E6%95%99%E7%A8%8B/",indexes:[36,51]},AI生成:{path:"/zh/tag/ai%E7%94%9F%E6%88%90/",indexes:[36]},ComfyMind:{path:"/zh/tag/comfymind/",indexes:[36]},开源项目:{path:"/zh/tag/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/",indexes:[36,51]},艺术:{path:"/zh/tag/%E8%89%BA%E6%9C%AF/",indexes:[37]},人工智能:{path:"/zh/tag/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/",indexes:[37,39]},创作者:{path:"/zh/tag/%E5%88%9B%E4%BD%9C%E8%80%85/",indexes:[37]},GTC:{path:"/zh/tag/gtc/",indexes:[37]},"resource guide":{path:"/zh/tag/resource-guide/",indexes:[64]},script:{path:"/zh/tag/script/",indexes:[64]},"stable diffusion":{path:"/zh/tag/stable-diffusion/",indexes:[64]},merge:{path:"/zh/tag/merge/",indexes:[64]},model:{path:"/zh/tag/model/",indexes:[64]},编辑器:{path:"/zh/tag/%E7%BC%96%E8%BE%91%E5%99%A8/",indexes:[65]},模型上下文协议:{path:"/zh/tag/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AE/",indexes:[65]},技术:{path:"/zh/tag/%E6%8A%80%E6%9C%AF/",indexes:[65]},"Artificial Intelligence":{path:"/zh/tag/artificial-intelligence/",indexes:[38]},Inference:{path:"/zh/tag/inference/",indexes:[38]},图像生成:{path:"/zh/tag/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/",indexes:[39,41,42,35]},FLUX:{path:"/zh/tag/flux/",indexes:[39,40]},机器学习:{path:"/zh/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/",indexes:[39,63]},"FLUX.1":{path:"/zh/tag/flux.1/",indexes:[66]},优化:{path:"/zh/tag/%E4%BC%98%E5%8C%96/",indexes:[66]},TaylorSeer:{path:"/zh/tag/taylorseer/",indexes:[66]},Replicate:{path:"/zh/tag/replicate/",indexes:[66]},LoRA:{path:"/zh/tag/lora/",indexes:[49,40]},QLoRA:{path:"/zh/tag/qlora/",indexes:[40]},微调:{path:"/zh/tag/%E5%BE%AE%E8%B0%83/",indexes:[40]},diffusers:{path:"/zh/tag/diffusers/",indexes:[40]},量化:{path:"/zh/tag/%E9%87%8F%E5%8C%96/",indexes:[40]},生成式AI:{path:"/zh/tag/%E7%94%9F%E6%88%90%E5%BC%8Fai/",indexes:[42]},游戏开发:{path:"/zh/tag/%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/",indexes:[42]},"Stable Diffusion":{path:"/zh/tag/stable-diffusion/",indexes:[42]},AWS:{path:"/zh/tag/aws/",indexes:[42]},"Amazon Bedrock":{path:"/zh/tag/amazon-bedrock/",indexes:[42]},Illustrious:{path:"/zh/tag/illustrious/",indexes:[41,44,35]},LU:{path:"/zh/tag/lu/",indexes:[41]},Lumina:{path:"/zh/tag/lumina/",indexes:[41]},AI模型:{path:"/zh/tag/ai%E6%A8%A1%E5%9E%8B/",indexes:[41]},训练:{path:"/zh/tag/%E8%AE%AD%E7%BB%83/",indexes:[41]},SDXL:{path:"/zh/tag/sdxl/",indexes:[49,44,35]},"2048分辨率":{path:"/zh/tag/2048%E5%88%86%E8%BE%A8%E7%8E%87/",indexes:[44]},vpred:{path:"/zh/tag/vpred/",indexes:[44]},epsilon预测:{path:"/zh/tag/epsilon%E9%A2%84%E6%B5%8B/",indexes:[44]},动漫:{path:"/zh/tag/%E5%8A%A8%E6%BC%AB/",indexes:[35]},基础模型:{path:"/zh/tag/%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B/",indexes:[35]},计算机视觉:{path:"/zh/tag/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/",indexes:[67]},图像识别:{path:"/zh/tag/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/",indexes:[67]},Prompt:{path:"/zh/tag/prompt/",indexes:[45]},text2image:{path:"/zh/tag/text2image/",indexes:[45]},AI:{path:"/zh/tag/ai/",indexes:[43]},协议:{path:"/zh/tag/%E5%8D%8F%E8%AE%AE/",indexes:[43]},辩论:{path:"/zh/tag/%E8%BE%A9%E8%AE%BA/",indexes:[43]},Art:{path:"/zh/tag/art/",indexes:[46,47]},Drawing:{path:"/zh/tag/drawing/",indexes:[46,47]},Fundamentals:{path:"/zh/tag/fundamentals/",indexes:[46,47]},niji:{path:"/zh/tag/niji/",indexes:[46,47]},艺术课程:{path:"/zh/tag/%E8%89%BA%E6%9C%AF%E8%AF%BE%E7%A8%8B/",indexes:[48]},Notan:{path:"/zh/tag/notan/",indexes:[48]},功能发布:{path:"/zh/tag/%E5%8A%9F%E8%83%BD%E5%8F%91%E5%B8%83/",indexes:[62]},"Kohya SS GUI":{path:"/zh/tag/kohya-ss-gui/",indexes:[49]},Qwen:{path:"/zh/tag/qwen/",indexes:[50]},Qwen3:{path:"/zh/tag/qwen3/",indexes:[50]},阿里巴巴AI研究:{path:"/zh/tag/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4ai%E7%A0%94%E7%A9%B6/",indexes:[50]},阿里巴巴AI:{path:"/zh/tag/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4ai/",indexes:[50]},大语言模型:{path:"/zh/tag/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/",indexes:[50]},LLMs:{path:"/zh/tag/llms/",indexes:[50]},LLM基准测试:{path:"/zh/tag/llm%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/",indexes:[50]},多语言AI:{path:"/zh/tag/%E5%A4%9A%E8%AF%AD%E8%A8%80ai/",indexes:[50]},多模态AI:{path:"/zh/tag/%E5%A4%9A%E6%A8%A1%E6%80%81ai/",indexes:[50]},AI推理:{path:"/zh/tag/ai%E6%8E%A8%E7%90%86/",indexes:[50]},MCP:{path:"/zh/tag/mcp/",indexes:[50]},GitHub:{path:"/zh/tag/github/",indexes:[51]},平台:{path:"/zh/tag/%E5%B9%B3%E5%8F%B0/",indexes:[52]},运营:{path:"/zh/tag/%E8%BF%90%E8%90%A5/",indexes:[52]},内容:{path:"/zh/tag/%E5%86%85%E5%AE%B9/",indexes:[52]},创新:{path:"/zh/tag/%E5%88%9B%E6%96%B0/",indexes:[52]}}}}}},94776:(e,t,a)=>{a.d(t,{M:()=>n});const n=["/demo/page.html","/demo/","/demo/disable.html","/demo/encrypt.html","/demo/layout.html","/demo/markdown.html","/posts/reprints/ai-art-gtc-paris-2025.html","/posts/reprints/explaining-tokens-the-language-and-currency-of-ai-nvidia-blog.html","/posts/reprints/flux-1-kontext.html","/posts/reprints/flux-qlora.html","/posts/reprints/generative-ai-powered-design.html","/posts/reprints/niji-lesson-2-the-terminator-line.html","/posts/reprints/illustrious-lu-v0.03.html","/posts/reprints/illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.html","/posts/reprints/illustrious-xl-v2.0-the-best-training-base-model-in-1536-age.html","/posts/reprints/news-agents-daily-recap.html","/posts/reprints/qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview.html","/posts/reprints/mcp-flash-in-the-pan-or-future-standard.html","/posts/reprints/introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression.html","/posts/reprints/niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything.html","/posts/reprints/niji-study-1-measuring-with-your-eyes.html","/posts/reprints/niji-study-2-notan.html","/posts/reprints/original-character-lora-sdxl-character-training.html","/zh/demo/page.html","/zh/demo/","/zh/demo/disable.html","/zh/demo/encrypt.html","/zh/demo/markdown.html","/zh/demo/layout.html","/zh/posts/dairys/250223.html","/zh/posts/dairys/250222.html","/zh/posts/papers/ovis-u1.html","/zh/posts/papers/comfyui-r1.html","/zh/posts/papers/framepack.html","/zh/posts/papers/sdo.html","/zh/posts/reprints/illustrious-xl-v2.0-the-best-training-base-model-in-1536-age.html","/zh/posts/repos/comfy-mind.html","/zh/posts/reprints/ai-art-gtc-paris-2025.html","/zh/posts/reprints/explaining-tokens-the-language-and-currency-of-ai-nvidia-blog.html","/zh/posts/reprints/flux-1-kontext.html","/zh/posts/reprints/flux-qlora.html","/zh/posts/reprints/illustrious-lu-v0.03.html","/zh/posts/reprints/generative-ai-powered-design.html","/zh/posts/reprints/mcp-flash-in-the-pan-or-future-standard.html","/zh/posts/reprints/illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.html","/zh/posts/reprints/introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression.html","/zh/posts/reprints/niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything.html","/zh/posts/reprints/niji-study-1-measuring-with-your-eyes.html","/zh/posts/reprints/niji-study-2-notan.html","/zh/posts/reprints/original-character-lora-sdxl-character-training.html","/zh/posts/reprints/qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview.html","/zh/posts/templates/repos.html","/zh/posts/thoughts/platform-operation-thoughts-after-comfycon.html","/posts/reprints/crody's-model-merge-guide.html","/posts/reprints/experimenting-with-mcp-using-github-copilot.html","/posts/reprints/what-is-block-merging.html","/posts/reprints/model-block-merge-1.html","/posts/reprints/model-block-merge-2.html","/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.html","/posts/reprints/flux-kontext-optimization.html","/posts/reprints/image-recognition.html","/posts/reprints/niji-video.html","/zh/posts/reprints/niji-video.html","/zh/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.html","/zh/posts/reprints/crody's-model-merge-guide.html","/zh/posts/reprints/experiments-with-mcp-using-github-copilot.html","/zh/posts/reprints/flux-kontext-optimization.html","/zh/posts/reprints/image-recognition.html","/posts/reprints/how-and-when-to-build-multi-agent-systems.html","/posts/reprints/announcing-illustrious-text%E2%80%91enhancer-tag-booster-and-mood-enhancer.html","/posts/reprints/ai-art-newsletter-jan-25.html","/posts/reprints/blog-images.html","/zh/posts/reprints/how-and-when-to-build-multi-agent-systems.html","/zh/posts/reprints/announcing-illustrious-text%E2%80%91enhancer-tag-booster-and-mood-enhancer.html","/zh/posts/ai-impls/yolov9.html","/zh/posts/ai-weekly/001.html","/zh/posts/ai-weekly/002.html","/zh/posts/ai-weekly/003.html","/zh/posts/ai-weekly/004.html","/zh/posts/ai-weekly/005.html","/zh/posts/ai-weekly/006.html","/zh/posts/ai-weekly/007.html","/zh/posts/ai-weekly/008.html","/zh/posts/ai-weekly/009.html","/zh/posts/ai-weekly/010.html","/zh/posts/ai-weekly/011.html","/zh/posts/ai-weekly/012.html","/zh/posts/ai-weekly/013.html","/zh/posts/ai-weekly/014.html","/zh/posts/ai-weekly/015.html","/zh/posts/ai-weekly/016.html","/zh/posts/ai-weekly/017.html","/zh/posts/ai-weekly/018.html","/zh/posts/ai-weekly/019.html","/zh/posts/ai-weekly/020.html","/zh/posts/ai-weekly/021.html","/zh/posts/ai-weekly/022.html","/zh/posts/ai-weekly/023.html","/zh/posts/ai-weekly/024.html","/zh/posts/ai-weekly/025.html","/zh/posts/ai-weekly/026.html","/zh/posts/ai-weekly/027.html","/zh/posts/ai-weekly/028.html","/zh/posts/ai-weekly/029.html","/zh/posts/ai-weekly/030.html","/zh/posts/ai-weekly/031.html","/zh/posts/ai-weekly/032.html","/zh/posts/ai-weekly/033.html","/zh/posts/ai-weekly/034.html","/zh/posts/ai-weekly/035.html","/zh/posts/ai-weekly/036.html","/zh/posts/ai-weekly/037.html","/zh/posts/ai-weekly/038.html","/zh/posts/ai-weekly/039.html","/zh/posts/ai-weekly/040.html","/zh/posts/ai-weekly/041.html","/zh/posts/ai-weekly/042.html","/zh/posts/ai-weekly/043.html","/zh/posts/ai-weekly/044.html","/zh/posts/ai-weekly/045.html","/zh/posts/ai-weekly/046.html","/zh/posts/ai-weekly/047.html","/zh/posts/ai-weekly/X01.html","/zh/posts/hf-weekly/001.html","/zh/posts/hf-weekly/002.html","/zh/posts/hf-weekly/003.html","/zh/posts/hf-weekly/004.html","/zh/posts/ielts/simon-task1.html","/zh/posts/ielts/simon-task2.html","/zh/posts/papers/3steps-paper-reading.html","/zh/posts/papers/alexnet.html","/zh/posts/papers/bagel.html","/zh/posts/papers/blip-3o.html","/zh/posts/papers/colorizediffusion.html","/zh/posts/papers/dgpst.html","/zh/posts/papers/ecomimic-v3.html","/zh/posts/papers/flux-kontext.html","/zh/posts/papers/hunyuancustom.html","/zh/posts/papers/icedit.html","/zh/posts/papers/ming-omni.html","/zh/posts/papers/nhr.html","/zh/posts/papers/omniconsistency.html","/zh/posts/papers/omnigen2.html","/zh/posts/papers/qr-lora.html","/zh/posts/papers/reptext.html","/zh/posts/papers/resnet.html","/zh/posts/papers/show-o2.html","/zh/posts/papers/transformer.html","/zh/posts/papers/vlv.html","/zh/posts/prompts/ai-weekly.prompt.html","/zh/posts/prompts/cover.prompt.html","/zh/posts/prompts/hf-weekly.prompt.html","/zh/posts/prompts/image-extract.prompt.html","/zh/posts/prompts/papers.prompt.html","/zh/posts/prompts/translate.prompt.html","/zh/posts/reprints/ai-art-newsletter-jan-25.html","/zh/posts/reprints/niji-lesson-2-the-terminator-line.html","/zh/posts/sci/conda.html","/zh/posts/templates/ai-weekly.html","/zh/posts/templates/hf-weekly.html","/zh/posts/templates/papers.html","/zh/posts/tutorials/qwen-code.html","/zh/posts/web/vue-1.html","/zh/posts/ielts/usage-of-collocations-in-speaking/ielts-collocations.html","/zh/posts/tutorials/comfyui/flux-kontext-beginner.html","/zh/posts/workflows/hf-weekly/checklist.html","/zh/posts/workflows/hf-weekly/workflow.html","/zh/posts/workflows/papers/checklist.html","/zh/posts/workflows/papers/workflow.html","/zh/posts/workflows/repos/checklist.html","/zh/posts/workflows/repos/material.html","/zh/posts/workflows/repos/workflow.html"]},53781:(e,t,a)=>{a.d(t,{U:()=>n});const n={article:{"/":{path:"/article/",indexes:[0,59,61,68,60,6,8,69,22,15,16,12,13,10,54,7,14,53,18,9,17,58,11,21,19,20,55,56,57,1,2,3,4,5,70,71]},"/zh/":{path:"/zh/article/",indexes:[29,30,23,66,31,62,72,32,67,36,51,37,39,73,49,50,33,41,52,44,42,65,38,35,64,45,40,34,43,48,46,47,24,25,26,28,27,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,63,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171]}},star:{"/":{path:"/star/",indexes:[0]},"/zh/":{path:"/zh/star/",indexes:[52,29,30,23]}},timeline:{"/":{path:"/timeline/",indexes:[59,61,68,60,6,8,69,22,15,16,12,13,10,54,7,14,53,18,9,17,58,11,21,19,20,55,56,57,0]},"/zh/":{path:"/zh/timeline/",indexes:[66,31,62,72,32,67,36,51,37,39,73,49,50,33,41,52,44,42,65,38,35,64,29,30,45,40,34,43,48,46,47,23]}}}},50711:(e,t,a)=>{a.d(t,{B:()=>V});var n={};a.r(n);var i={};a.r(i);var o={};a.r(o),a.d(o,{default:()=>b});var r={};a.r(r);var s={};a.r(s),a.d(s,{default:()=>B});var l={};a.r(l),a.d(l,{default:()=>L});var d={};a.r(d),a.d(d,{default:()=>j});var h={};a.r(h),a.d(h,{default:()=>X});var p=a(42402),g=a(13073),m=a(79403),c=a(70596),u=a(38831);const b={enhance:({app:e})=>{(0,p.L4)("FontIcon")||e.component("FontIcon",m.A),(0,p.L4)("Badge")||e.component("Badge",c.A),(0,p.L4)("VPCard")||e.component("VPCard",u.A)},setup:()=>{(0,g.r9V)("https://cdn.jsdelivr.net/npm/iconify-icon@1")},rootComponents:[]};var f=a(26358),E=a(23827),y=a(80355),A=a(31599),w=a(38913),x=a(80457),k=a(33349),z=a(11781),v=a(19212);const B={enhance:({app:e})=>{e.component("CodeTabs",z.S),e.component("Tabs",v.t)}};var I=a(58031),C=a(83027);const L={enhance:({app:e})=>{e.component("CodeDemo",I.A),e.component("MdDemo",C.A)}};var T=a(75753),D=a(75054),F=a(89672),M=a(38465),q=a(20641),S=a(69288),G=a(43974);(0,M.M_)((e=>{const t=e.t,a=!1!==e.I,n=e.i;return a?{title:t,content:n?()=>[(0,q.h)(F.GB,{icon:n}),t]:null,order:e.O,index:e.I}:null}));const j={enhance:({app:e,router:t})=>{const{scrollBehavior:a}=t.options;t.options.scrollBehavior=async(...e)=>(await F.lE.wait(),a(...e)),(0,F.fk)(e),e.component("HopeIcon",F.GB),e.component("BloggerInfo",S.tI),e.component("SocialMedias",S.rS),e.component("GlobalEncrypt",G.J),e.component("LocalEncrypt",G.n)},setup:()=>{(0,F.PV)(),(0,F.i$)(),(0,S.su)()},layouts:{Layout:F.PE,NotFound:F.Mk,BlogCategory:S.Pn,BlogHome:S.qX,BlogType:S.z7,Timeline:S.Kf}};var P=a(53456),O=a(50953),R=a(21974),H=a(52330),K=a(57388),N=a(22581);const _={class:"theme-hope-content"},U=(0,q.pM)({__name:"MyPostLayout",setup:e=>(e,t)=>{const a=(0,q.g2)("Share");return(0,q.uX)(),(0,q.CE)(q.FK,null,[(0,q.bF)((0,O.R1)(K.A)),(0,q.bF)((0,O.R1)(R.A),null,{default:(0,q.k6)((()=>[(0,q.bF)((0,O.R1)(N.N),null,{default:(0,q.k6)((()=>[(0,q.bF)((0,O.R1)(H.A),null,{contentAfter:(0,q.k6)((()=>[(0,q.Lk)("div",_,[(0,q.bF)(a,{colorful:"",services:"email,facebook,line,linkedin,messenger,qrcode,reddit,telegram,twitter"})])])),_:1})])),_:1})])),_:1})],64)}}),X=(0,P.re)({enhance:({app:e})=>{},layouts:{Layout:U,Origin:F.PE}}),V=[n,i,o,f,E,y,A,w,x,k,r,s,l,T,D,d,h].map((e=>e.default)).filter(Boolean)},68164:(e,t,a)=>{a.d(t,{J:()=>i,c:()=>n});const n=JSON.parse("{}"),i=Object.fromEntries([["/home.html",{loader:()=>a.e(9759).then(a.bind(a,55936)),meta:{t:"Blog Home",i:"home"}}],["/",{loader:()=>a.e(4470).then(a.bind(a,39908)),meta:{t:"Blog Home",i:"home"}}],["/intro.html",{loader:()=>a.e(3912).then(a.bind(a,69677)),meta:{t:"Intro Page",i:"circle-info"}}],["/demo/",{loader:()=>a.e(3320).then(a.bind(a,85351)),meta:{c:["Guide"],r:{minutes:.04,words:12},t:"Features demo",i:"laptop-code",y:"a"}}],["/demo/disable.html",{loader:()=>a.e(2756).then(a.bind(a,52411)),meta:{c:["Guide"],g:["disable"],e:"<p>You can disable some function and layout on the page by setting the Frontmatter of the page.</p>\n",r:{minutes:.28,words:83},t:"Disabling layout and features",i:"gears",O:4,y:"a"}}],["/demo/encrypt.html",{loader:()=>a.e(2581).then(a.bind(a,21280)),meta:{c:["Guide"],g:["encryption"],n:!0,r:{minutes:.3,words:90},t:"Encryption Article",i:"lock",y:"a"}}],["/demo/layout.html",{loader:()=>a.e(6216).then(a.bind(a,61804)),meta:{c:["Guide"],g:["Layout"],e:'<p>The layout contains:</p>\n<ul>\n<li><a href="https://theme-hope.vuejs.press/guide/layout/navbar.html" target="_blank" rel="noopener noreferrer">Navbar</a></li>\n<li><a href="https://theme-hope.vuejs.press/guide/layout/sidebar.html" target="_blank" rel="noopener noreferrer">Sidebar</a></li>\n<li><a href="https://theme-hope.vuejs.press/guide/layout/footer.html" target="_blank" rel="noopener noreferrer">Footer</a></li>\n</ul>',r:{minutes:.35,words:105},t:"Layout",i:"object-group",O:2,y:"a"}}],["/demo/markdown.html",{loader:()=>a.e(8711).then(a.bind(a,83947)),meta:{c:["Guide"],g:["Markdown"],e:"<p>VuePress basically generate pages from Markdown files. So you can use it to generate documentation or blog sites easily.</p>\n<p>You should create and write Markdown files, so that VuePress can convert them to different pages according to file structure.</p>\n",r:{minutes:2.63,words:789},t:"Markdown Enhance",i:"fab fa-markdown",O:2,y:"a"}}],["/demo/page.html",{loader:()=>a.e(645).then(a.bind(a,95397)),meta:{a:"Ms.Hope",d:15778368e5,l:"January 1, 2020",c:["Guide"],g:["Page config","Guide"],u:!0,e:"<p>Content before <code>more</code> comment is regarded as page excerpt.</p>\n",r:{minutes:1.14,words:341},t:"Page Config",i:"file",O:3,y:"a"}}],["/zh/home.html",{loader:()=>a.e(3090).then(a.bind(a,46072)),meta:{t:"Blog Home",i:"home"}}],["/zh/",{loader:()=>a.e(433).then(a.bind(a,95762)),meta:{t:"Blog Home",i:"home"}}],["/zh/intro.html",{loader:()=>a.e(787).then(a.bind(a,12414)),meta:{t:"Intro Page",i:"circle-info"}}],["/posts/reprints/ai-art-gtc-paris-2025.html",{loader:()=>a.e(2033).then(a.bind(a,5661)),meta:{a:"Heather Schoell",d:1749189624e3,l:"June 6, 2025",c:["Generative AI"],g:["Art","Artificial Intelligence","Creators","GTC"],e:"\n<p>The conference, taking place in one of Europe's iconic art capitals, will feature a curated gallery that showcases how AI helps bring creative visions to life.</p>\n<h2>A Poetic Counterpart in Creation</h2>\n<p>When Paul Mouginot first brought AI into his art practice, he knew he'd tapped an exciting new tool for creative expression. But only through its continued use did he come to appreciate AI's versatility as an introspective entity — what he calls \"a poetic counterpart in the act of creation.\"</p>",r:{minutes:4.73,words:1420},t:"Artists, Fashion Designers Tap State-of-the-Art AI for NVIDIA GTC Paris Gallery",y:"a"}}],["/posts/reprints/ai-art-newsletter-jan-25.html",{loader:()=>a.e(4772).then(a.bind(a,57940)),meta:{e:"\n<h3>First issue 🎉</h3>\n<p>The AI space is moving so fast it’s hard to believe that a year ago we still struggled to generate people with the correct amount of fingers 😂.</p>\n<p>The last couple of years have been pivotal for open source models and tools for artistic usage.\nAI tools for creative expression have never been more accessible, and we’re only scratching the surface.\nJoin us as we look back at the key milestones, tools, and breakthroughs in AI &amp; Arts from 2024,\nand forward for what’s to come in 2025.</p>",r:{minutes:7.56,words:2269},t:"The AI tools for Art Newsletter - Issue 1",y:"a"}}],["/posts/reprints/announcing-illustrious-text%E2%80%91enhancer-tag-booster-and-mood-enhancer.html",{loader:()=>a.e(3232).then(a.bind(a,94919)),meta:{a:"LivBigStar",d:17479584e5,l:"May 23, 2025",v:"/assets/images/reprints/illustrious/tag-enhancer/cover.jpg",e:"\n<p>Illustrious users often ask: <em>“How can I get better results without writing long prompts?”</em> Today, we’re excited to answer that with <strong>Text‑Enhancer</strong>, a new system that dramatically <strong>enriches user prompts</strong> for our image generation platform.</p>\n<p>Text‑Enhancer actually comprises <strong>two intelligent components</strong> working together:</p>",r:{minutes:8.73,words:2618},t:"Announcing Illustrious Text‑Enhancer: Tag Booster & Mood Enhancer",y:"a"}}],["/posts/reprints/blog-images.html",{loader:()=>a.e(3111).then(a.bind(a,47015)),meta:{e:'<p>Blog Images Generator | Generate Images For Blogs</p>\n<p>===============</p>\n<p><a href="https://www.junia.ai/" target="_blank" rel="noopener noreferrer"><img src="https://www.junia.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo-white.aee10a02.png&amp;w=384&amp;q=75" alt="Image 1: Logo of a robot cat | Junia.ai" loading="lazy"></a><a href="https://www.junia.ai/" target="_blank" rel="noopener noreferrer"><img src="https://www.junia.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo-white.aee10a02.png&amp;w=256&amp;q=75" alt="Image 2: Logo of a robot cat | Junia.ai" loading="lazy"></a><a href="https://www.junia.ai/bulk-content-creation" target="_blank" rel="noopener noreferrer">Bulk Content Generator</a><a href="https://www.junia.ai/brand-voice" target="_blank" rel="noopener noreferrer">Brand Voice</a><a href="https://www.junia.ai/tools/ai-text-editor" target="_blank" rel="noopener noreferrer">AI Text Editor</a><a href="https://www.junia.ai/templates" target="_blank" rel="noopener noreferrer">Templates</a></p>',r:{minutes:12.93,words:3879},t:"Blog Images Generator",y:"a"}}],["/posts/reprints/crody's-model-merge-guide.html",{loader:()=>a.e(1041).then(a.bind(a,66470)),meta:{a:"Crody",d:17419104e5,l:"March 14, 2025",g:["resource guide","script","stable diffusion","merge","model"],v:"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/2d3f67d3-4906-46b0-b27c-63ba7376cca7/width=1320/00000_2500974118.jpeg",e:"<p>Hi this is Crody from Team-C: creator of Nova Series</p>\n<p>In this article, I'll write down what kind of merge I use with some knowledge about SDXL models\nFor how I do, please read Merge Scripter Guide first</p>\n<h2>1. Weighted Sum / Sum Twice</h2>\n<p>Weighted Sum (WS) merges 2 models, Sum Twice (ST) merges first 2 and 1 model (which means doing WS twice)\nYou can use Block Merge as well\nUsing alpha (and beta) to determine how much similarity the result have\nHigher value means the results would be similar to latter model</p>",r:{minutes:5.18,words:1553},t:"Crody's Model Merge Guide // Team-C",y:"a"}}],["/posts/reprints/experimenting-with-mcp-using-github-copilot.html",{loader:()=>a.e(9839).then(a.bind(a,34360)),meta:{a:"kcon",d:1742256e6,l:"March 18, 2025",g:["editor","Model Context Protocol","tech"],e:'\n<h2>Overview</h2>\n<p>This article describes how to install the "Copilot MCP" extension in VS Code and use GitHub Copilot with MCP to fetch information from GitHub for testing purposes.<br>\nNote: Since the official GitHub Copilot implementation also seems to support MCP, this extension might become unnecessary once the feature is released.</p>',r:{minutes:1.6,words:480},t:"Experimenting with MCP using GitHub Copilot",y:"a"}}],["/posts/reprints/explaining-tokens-the-language-and-currency-of-ai-nvidia-blog.html",{loader:()=>a.e(8637).then(a.bind(a,26576)),meta:{a:"Dave Salvator",d:17421696e5,l:"March 17, 2025",c:["Explainer","Generative AI"],g:["Artificial Intelligence","Inference"],v:"https://blogs.nvidia.com/wp-content/uploads/2025/03/llm-blog-data-curator-2847806-1280x680-1.png",e:'\n<figure><img src="https://blogs.nvidia.com/wp-content/uploads/2025/03/llm-blog-data-curator-2847806-1280x680-1.png" alt="Image 1: Depiction of a large language model processing data" tabindex="0" loading="lazy"><figcaption>Image 1: Depiction of a large language model processing data</figcaption></figure>',r:{minutes:5.44,words:1632},t:"Explaining Tokens — the Language and Currency of AI",y:"a"}}],["/posts/reprints/flux-1-kontext.html",{loader:()=>a.e(7291).then(a.bind(a,42047)),meta:{a:"Black Forest Labs",d:17484768e5,l:"May 29, 2025",c:"News",g:["AI","Image Generation","FLUX","Machine Learning"],e:"\n<p>Today, we are excited to release FLUX.1 Kontext, a suite of generative flow matching models that allows you to generate and edit images. Unlike existing text-to-image models, the FLUX.1 Kontext family performs <strong><em>in-context</em></strong> image generation, allowing you to prompt with both text and images, and seamlessly extract and modify visual concepts to produce new, coherent renderings.</p>",r:{minutes:4.75,words:1424},t:"Introducing FLUX.1 Kontext and the BFL Playground",y:"a"}}],["/posts/reprints/flux-kontext-optimization.html",{loader:()=>a.e(4179).then(a.bind(a,96416)),meta:{d:17525376e5,l:"July 15, 2025",g:["FLUX.1","Optimization","TaylorSeer","Replicate"],e:'\n<figure><img src="https://replicate.com/_content/assets/top-graphic.CLh5lXp7_Z2h1V1F.webp" alt="FLUX.1 Kontext optimization graphic" tabindex="0" loading="lazy"><figcaption>FLUX.1 Kontext optimization graphic</figcaption></figure>\n<p>In addition to making our FLUX.1 Kontext [dev] implementation open-source, we wanted to provide more guidance on how we chose to optimize it without compromising on quality.</p>',r:{minutes:3.9,words:1171},t:"How we optimized FLUX.1 Kontext [dev]",y:"a"}}],["/posts/reprints/flux-qlora.html",{loader:()=>a.e(705).then(a.bind(a,11500)),meta:{a:"Derek Liu, Marc Sun, Sayak Paul, merve, Linoy Tsaban",d:17187552e5,l:"June 19, 2024",c:"AI/ML",g:["FLUX","LoRA","QLoRA","Fine-tuning","diffusers","quantization"],e:'\n<div class="hint-container tip">\n<p class="hint-container-title">Tips</p>\n<figure><a href="https://colab.research.google.com/github/DerekLiu35/notebooks/blob/main/flux_lora_quant_blogpost.ipynb" target="_blank" rel="noopener noreferrer"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" tabindex="0" loading="lazy"></a><figcaption>Open In Colab</figcaption></figure>\n</div>',r:{minutes:9.82,words:2947},t:"(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware",y:"a"}}],["/posts/reprints/generative-ai-powered-design.html",{loader:()=>a.e(5391).then(a.bind(a,41414)),meta:{a:"Isha Dua & Parth Patel",d:17423424e5,l:"March 19, 2025",c:["Reprints"],g:["Generative AI","Game Development","Stable Diffusion","Image Generation","AWS","Amazon Bedrock"],u:!1,v:"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/19/Picture1-11.jpg",e:"<p>In the competitive world of game development, staying ahead of technological advancements is crucial. Generative AI has emerged as a game changer, offering unprecedented opportunities for game designers to push boundaries and create immersive virtual worlds. At the forefront of this revolution is Stability AI’s cutting-edge text-to-image AI model, Stable Diffusion 3.5 Large (SD3.5 Large), which is transforming the way we approach game environment creation.</p>",r:{minutes:4.68,words:1405},t:"Generative AI-Powered Design: Creating Game Environments with SD3.5 Large",i:"openmoji:video-game",O:1,y:"a"}}],["/posts/reprints/how-and-when-to-build-multi-agent-systems.html",{loader:()=>a.e(5229).then(a.bind(a,35924)),meta:{d:175008553e4,l:"June 16, 2025",e:'\n<figure><img src="https://blog.langchain.com/content/images/size/w760/format/webp/2025/06/supervisor.png" alt="Image 2: How and when to build multi-agent systems" tabindex="0" loading="lazy"><figcaption>Image 2: How and when to build multi-agent systems</figcaption></figure>\n<p>Late last week two great blog posts were released with seemingly opposite titles. "Don\'t Build Multi-Agents" by the Cognition team, and "How we built our multi-agent research system" by the Anthropic team.</p>',r:{minutes:5.99,words:1797},t:"How and when to build multi-agent systems",y:"a"}}],["/posts/reprints/illustrious-lu-v0.03.html",{loader:()=>a.e(843).then(a.bind(a,9082)),meta:{a:"Angelbottomless",d:17449344e5,l:"April 18, 2025",c:["Model Development","reprint"],g:["Illustrious","LU","Lumina","AI Model","Image Generation","Training"],v:"https://illustrious-prod.s3.ap-northeast-2.amazonaws.com/blog/2025-04-11T07:16:56.712Z/2025-04-11%20Thumbnail.png",e:'\n<p>SD XL has been suffering from CLIP – I think this is true, at least partially. Recent models have shown some potential related to natural language, like understanding "left is red, right is blue". However, since CLIP was not trained with natural language sentences, base SD XL and its finetuned variants were significantly limited regarding processing it.</p>',r:{minutes:2.56,words:768},t:"Illustrious-LU v0.03",i:"fa-solid:microscope",y:"a"}}],["/posts/reprints/illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.html",{loader:()=>a.e(2296).then(a.bind(a,19134)),meta:{a:"Angelbottomless",d:17426016e5,l:"March 22, 2025",c:["Model Development","Model Training","Image Generation","Anime Style"],g:["SDXL","2048 Resolution","Illustrious","vpred","epsilon prediction"],v:"/assets/images/reprints/illustrious/v3.0-3.5/thumbnail.webp",e:"\n<p>Illustrious XL 3.0–3.5-vpred represents a major advancement in Stable Diffusion XL (SD XL) modeling, notably supporting resolutions ranging seamlessly from 256 up to 2048. The v3.5-vpred variant particularly emphasizes robust natural language understanding capabilities, comparable in sophistication to miniaturized large language models (LLMs), achieved through extensive simultaneous training of both CLIP and UNet components.</p>",r:{minutes:8.56,words:2568},t:"Illustrious XL 3.0-3.5-vpred: 2048 Resolution and Natural Language",i:"mdi:paint-outline",y:"a"}}],["/posts/reprints/illustrious-xl-v2.0-the-best-training-base-model-in-1536-age.html",{loader:()=>a.e(6959).then(a.bind(a,57246)),meta:{a:"Angelbottomless",d:17419968e5,l:"March 15, 2025",c:["Model Development","Model Training","Image Generation","Anime Style"],g:["SDXL","Anime","Illustrious","Base model","Image generation"],v:"/assets/images/reprints/illustrious/v2.0-2.0a/thumbnail.webp",e:"\n<h2>Introduction</h2>\n<p>Illustrious XL 1.0-2.0 series aims to stabilize native generation at 1536 resolution while significantly improving natural language understanding capabilities.</p>\n<p>While users sometimes observed successful 1024x1536 resolution generations, these were not stable. Similarly, 512x512 generations occasionally produced unwanted artifacts.</p>",r:{minutes:3.66,words:1097},t:"Illustrious XL v2.0—The best training base model in 1536 age",i:"mdi:paint-outline",y:"a"}}],["/posts/reprints/image-recognition.html",{loader:()=>a.e(7413).then(a.bind(a,10177)),meta:{a:"Timothy M.",d:1749577901e3,l:"June 10, 2025",g:["Computer Vision","Image Recognition"],e:'\n<figure><img src="https://blog.roboflow.com/content/images/size/w1200/2025/06/Screenshot-2025-06-10-at-10.46.38---AM.png" alt="What Is Image Recognition" tabindex="0" loading="lazy"><figcaption>What Is Image Recognition</figcaption></figure>\n<p>Imagine a young girl named Emma who is fascinated by birds. Every weekend, she visits a nearby park to watch birds with her grandfather. Over time, Emma learns to recognize different bird species by their color, size, shape, and even their chirps. One afternoon, while flipping through a book, she effortlessly points to a picture and says, "Look, Grandpa! It\'s a robin!" She doesn\'t measure wingspans or analyze feather types; her brain instantly connects the image to her experiences and memories of robins at the park.</p>',r:{minutes:14.02,words:4207},t:"What Is Image Recognition? Algorithms and Applications",y:"a"}}],["/posts/reprints/introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression.html",{loader:()=>a.e(6982).then(a.bind(a,47955)),meta:{d:1733895946e3,l:"December 11, 2024",c:["Novice"],g:["Prompt","text2image"],e:'\n<p>This guide covers prompts for AI illustration generation, focusing on camera angles, composition, and facial expressions to enhance your creative process.</p>\n<h2>Camera angle and composition prompts</h2>\n<h3>Camera angle</h3>\n<table>\n<thead>\n<tr>\n<th>Prompt</th>\n<th>Description</th>\n<th>Image</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>from the front</code>, <code>front view</code></td>\n<td>Shooting from the front.</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-from_the_front.jpg" alt="Front view" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>from above</code>, <code>high angle</code></td>\n<td>Shooting from above.</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-from_above.jpg" alt="High angle" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>from below</code>, <code>low angle</code></td>\n<td>Shooting from below.</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-from_below.jpg" alt="Low angle" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>side view</code>, <code>side angle</code></td>\n<td>Shooting from the side.</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-side_view.jpg" alt="Side view" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>back shot</code>, <code>back view</code></td>\n<td>Shooting from behind. Use <code>looking back</code> for a turned-around pose.</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-back_shot.jpg" alt="Back view" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>overhead shot</code>, <code>bird\'s-eye view</code></td>\n<td>Shooting overhead, often for lying subjects.</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-overhead_shot.jpg" alt="Bird\'s-eye view" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>pov</code></td>\n<td>First-person, subjective viewpoint. Combine with <code>pov hand</code> for hand perspective.</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples_pov.jpg" alt="POV" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>dutch angle</code></td>\n<td>Tilted shooting for eye-catching compositions.</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-dutch_angle.jpg" alt="Dutch angle" loading="lazy"></td>\n</tr>\n</tbody>\n</table>',r:{minutes:3.76,words:1128},t:"Introduction of prompts in AI illustration generation [composition / camera angle / facial expression]",y:"a"}}],["/posts/reprints/mcp-flash-in-the-pan-or-future-standard.html",{loader:()=>a.e(157).then(a.bind(a,84e3)),meta:{a:"Harrison Chase & Nuno Campos",d:17145216e5,l:"May 1, 2024",c:["reprint"],g:["AI","LLM","Protocol","Debate"],u:!1,v:"/assets/images/reprint/mcp-debate-cover.jpeg",e:"\n<p>Model Context Protocol (MCP) has stirred up quite a storm on Twitter—but is it actually useful, or just noise? In this debate, Harrison Chase (LangChain CEO) and Nuno Campos (Head of LangGraph) discuss whether MCP lives up to the hype.</p>\n<h2>Harrison's Take: MCP Is Actually Useful</h2>\n<p>I started skeptical about MCP, but I've begun to see its value. Essentially: <strong>MCP is useful when you want to add tools to an agent you don't control</strong>.</p>",r:{minutes:4.36,words:1309},t:"MCP: Flash in the Pan or Future Standard?",i:"openmoji:code-editor",O:1,y:"a"}}],["/posts/reprints/model-block-merge-1.html",{loader:()=>a.e(7532).then(a.bind(a,43605)),meta:{a:"bbcmc",d:1671812821e3,l:"December 23, 2022",g:["StableDiffusion","ModelMerge","AUTOMATIC1111","AI"],e:'\n<figure><img src="https://assets.st-note.com/production/uploads/images/93960055/rectangle_large_type_2_0e16ed0e2b288d1ab93dbd822ca17a46.png?width=1200" alt="Header image" tabindex="0" loading="lazy"><figcaption>Header image</figcaption></figure>\n\n<ol>\n<li>Introduction</li>\n<li>Experiment Overview</li>\n<li>Experiment 1</li>\n<li>Analysis 1</li>\n<li>Experiment 2</li>\n<li>Analysis 2</li>\n<li>Side Note 1</li>\n<li>Experiment 2 Setup</li>\n<li>Experiment 2: Results and Analysis</li>\n<li>Analysis Approach and Reminders (to me)</li>\n</ol>',r:{minutes:6.85,words:2056},t:"[Experiment Report] Investigating the Influence of Each U-Net Layer with Model Block Merge #1",y:"a"}}],["/posts/reprints/model-block-merge-2.html",{loader:()=>a.e(3329).then(a.bind(a,92250)),meta:{a:"bbcmc",d:1671812821e3,l:"December 23, 2022",g:["StableDiffusion","ModelMerge","AUTOMATIC1111","AI"],e:'\n<figure><img src="https://assets.st-note.com/production/uploads/images/93960055/rectangle_large_type_2_0e16ed0e2b288d1ab93dbd822ca17a46.png?width=1200" alt="Header image" tabindex="0" loading="lazy"><figcaption>Header image</figcaption></figure>\n',r:{minutes:6.68,words:2004},t:"[Experiment Report] Investigating the Influence of Each U-Net Layer with Model Block Merge #1",y:"a"}}],["/posts/reprints/news-agents-daily-recap.html",{loader:()=>a.e(3216).then(a.bind(a,24780)),meta:{a:"Eugene Yan",d:17460576e5,l:"May 1, 2025",c:["AI Tools","reprint"],g:["MCP","News Agents","tmux","LLM","Amazon Q","Agent Systems"],v:"https://eugeneyan.com/assets/news-agents.jpg",e:'\n<p>[ <a href="https://eugeneyan.com/tag/llm/" target="_blank" rel="noopener noreferrer">llm</a> <a href="https://eugeneyan.com/tag/learning/" target="_blank" rel="noopener noreferrer">learning</a> <a href="https://eugeneyan.com/tag/%F0%9F%9B%A0/" target="_blank" rel="noopener noreferrer">🛠</a> ] · 8 min read</p>',r:{minutes:5.67,words:1701},t:"Building News Agents for Daily News Recaps with MCP, Q, and tmux",i:"fa-solid:newspaper",y:"a"}}],["/posts/reprints/niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything.html",{loader:()=>a.e(5109).then(a.bind(a,18838)),meta:{a:"niji・journey",d:16898976e5,l:"July 21, 2023",c:["reprints"],g:["Art","Drawing","Fundamentals","niji"],u:!1,v:"/assets/images/reprints/nijijourney/lesson1/thumb.webp",e:'\n<p>In this class, instead of focusing on how to draw specific subjects, we\'ll be teaching you how to teach yourself, with some help from niji.</p>\n<p>For the exercise attached to this lesson, see <a href="https://nijijourney.com/blog/niji-study-1-Measuring-With-Your-Eyes" target="_blank" rel="noopener noreferrer">📏 Study 1: Measuring With Your Eyes.</a></p>',r:{minutes:6.14,words:1843},t:"Lesson 1: Fundamentals of Measurement and Abstraction: The Theory of (How to Draw) Everything",i:"palette",O:1,y:"a"}}],["/posts/reprints/niji-lesson-2-the-terminator-line.html",{loader:()=>a.e(7287).then(a.bind(a,98588)),meta:{a:"niji・journey",d:16902432e5,l:"July 25, 2023",c:["Reprints"],g:["Art Lesson","Visual Hierarchy"],u:!1,v:"/assets/images/reprints/nijijourney/lesson2/thumb.webp",e:'\n<h2>Establishing Visual Hierarchy</h2>\n<figure><img src="/assets/images/reprints/nijijourney/lesson2/f974b143-006e-4802-8202-20241b5104db.jpeg" alt="School of Athens by Raphael" tabindex="0" loading="lazy"><figcaption>School of Athens by Raphael</figcaption></figure>\n<p>School of Athens by Raphael</p>',r:{minutes:5.13,words:1540},t:"Lesson 2: The Terminator (Line)",i:"palette",O:2,y:"a"}}],["/posts/reprints/niji-study-1-measuring-with-your-eyes.html",{loader:()=>a.e(8904).then(a.bind(a,29355)),meta:{a:"niji・journey",d:16898976e5,l:"July 21, 2023",c:["reprints"],g:["Art","Drawing","Fundamentals","niji"],u:!1,v:"/assets/images/reprints/nijijourney/study1/thumb.webp",e:'\n<p><a href="/posts/reprints/niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything">Previous Lesson 1: Fundamentals of Measurement and Abstraction: The Theory of (How to Draw) Everything</a> | <a href="https://nijijourney.com/blog/niji-study-2-notan" target="_blank" rel="noopener noreferrer">Next Study 2: Notan</a></p>',r:{minutes:3.3,words:991},t:"Study 1: Measuring With Your Eyes",i:"ruler",O:2,y:"a"}}],["/posts/reprints/niji-study-2-notan.html",{loader:()=>a.e(5336).then(a.bind(a,10935)),meta:{a:"niji・journey",d:16902432e5,l:"July 25, 2023",c:["Papers"],g:["Art Lesson","Notan"],u:!1,v:"/assets/images/reprints/nijijourney/study2/thumb.webp",e:'\n<h2>What Does "Notan" Mean in Art?</h2>\n<figure><img src="/assets/images/reprints/nijijourney/study2/f7577f87-812e-46b6-83ff-98a24eb59a3e.jpeg" alt="Rooster and mouse illustration by Siddarth Chaturvedi" tabindex="0" loading="lazy"><figcaption>Rooster and mouse illustration by Siddarth Chaturvedi</figcaption></figure>',r:{minutes:1.2,words:359},t:"Study 2: Notan",i:"contrast",O:3,y:"a"}}],["/posts/reprints/niji-video.html",{loader:()=>a.e(1038).then(a.bind(a,37908)),meta:{d:17501184e5,l:"June 17, 2025",g:["Feature Announcement","Usage Guide"],e:"\n<p>Long time no see! We've been long overdue for an update.</p>\n<p>And this one is really impressive. WE MADE AN ANIME OPENING!!! Er, I mean, Niji can now generate videos!!!!!!!!</p>\n<p>While I'm generally psyched about each and every feature we make, I can say for sure that video-fying your pictures is the best damn thing we've shipped in a while. I really hope you enjoy it!</p>",r:{minutes:3.47,words:1040},t:"How to make videos with niji・journey",y:"a"}}],["/posts/reprints/original-character-lora-sdxl-character-training.html",{loader:()=>a.e(5317).then(a.bind(a,7050)),meta:{d:17470333e5,l:"May 12, 2025",c:["Advanced"],g:["Kohya SS GUI","LoRA","SDXL"],e:'\n<figure><img src="http://digitalcreativeai.net/_next/image?url=https%3A%2F%2Fdca.data-hub-center.com%2Fcontent%2Fuploads%2F2025%2F05%2Feye_catch_original-character-lora-sdxl-character-training-en.jpg&amp;w=3840&amp;q=80" alt="How to create an original character LoRA [SDXL Training] SDXL Character Training featured Image" tabindex="0" loading="lazy"><figcaption>How to create an original character LoRA [SDXL Training] SDXL Character Training featured Image</figcaption></figure>',r:{minutes:8.54,words:2561},t:"How to create an original character LoRA [SDXL Training] SDXL Character Training",y:"a"}}],["/posts/reprints/qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview.html",{loader:()=>a.e(8467).then(a.bind(a,99194)),meta:{a:"Jainil Prajapati",d:17458848e5,l:"April 29, 2025",c:["reprint"],g:["Qwen","Qwen3","Alibaba AI research","Alibaba AI","large language models","LLMs","LLM benchmarks","Multilingual AI","Multimodal AI","AI reasoning","MCP"],u:!1,v:"https://altctrlai.com/content/images/size/w2000/2025/04/834B968B-9CED-4EF4-B81F-845F8241AC0A.webp",e:'\n<figure><img src="https://altctrlai.com/content/images/size/w2000/2025/04/834B968B-9CED-4EF4-B81F-845F8241AC0A.webp" alt="Qwen3: Next-Gen AI with Hybrid Thinking and Multilingual Mastery | 2025 Overview" tabindex="0" loading="lazy"><figcaption>Qwen3: Next-Gen AI with Hybrid Thinking and Multilingual Mastery | 2025 Overview</figcaption></figure>',r:{minutes:6.03,words:1808},t:"Qwen3: Next-Gen AI with Hybrid Thinking and Multilingual Mastery | 2025 Overview",i:"fa-solid:robot",y:"a"}}],["/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.html",{loader:()=>a.e(7446).then(a.bind(a,2022)),meta:{a:"Kemal Erdem",d:16987968e5,l:"November 1, 2023",g:["Machine Learning","Diffusion Models"],e:"\n<h2>What is a diffusion model?</h2>\n<p>The idea of the diffusion model is not that old. In the 2015 paper called <strong>“Deep Unsupervised Learning using Nonequilibrium Thermodynamics”</strong>[1], the Authors described it like this:</p>\n<blockquote>\n<p>The essential idea, inspired by non-equilibrium statistical physics, is to systematically and <strong>slowly destroy structure in a data distribution</strong> through an <strong>iterative forward diffusion process</strong>. We then learn a <strong>reverse diffusion process</strong> that <strong>restores structure in data</strong>, yielding a highly flexible and tractable generative model of the data.</p>\n</blockquote>",r:{minutes:13.64,words:4092},t:"Step by Step visual introduction to Diffusion Models",y:"a"}}],["/posts/reprints/what-is-block-merging.html",{loader:()=>a.e(8573).then(a.bind(a,27083)),meta:{a:"researchanon",d:16740864e5,l:"January 19, 2023",g:["StableDiffusion","ModelMerging","AUTOMATIC1111","AI"],e:'\n<div class="hint-container warning">\n<p class="hint-container-title">NOTE</p>\n<p>This guide is still work in progress. Any and all feedback is highly appreciated, it doesn\'t have to be suggestions, even questions regarding things you didn\'t understand can help me figure out what to refine.</p>\n</div>',r:{minutes:12.88,words:3865},t:"What is Block Merging?",y:"a"}}],["/zh/demo/",{loader:()=>a.e(1145).then(a.bind(a,782)),meta:{c:["使用指南"],r:{minutes:.07,words:22},t:"主要功能与配置演示",i:"laptop-code",y:"a"}}],["/zh/demo/disable.html",{loader:()=>a.e(6297).then(a.bind(a,87891)),meta:{c:["使用指南"],g:["禁用"],e:"<p>你可以通过设置页面的 Frontmatter，在页面禁用功能与布局。</p>\n",r:{minutes:.43,words:128},t:"布局与功能禁用",i:"gears",O:4,y:"a"}}],["/zh/demo/encrypt.html",{loader:()=>a.e(4344).then(a.bind(a,31321)),meta:{c:["使用指南"],g:["加密"],n:!0,r:{minutes:.51,words:154},t:"密码加密的文章",i:"lock",y:"a"}}],["/zh/demo/layout.html",{loader:()=>a.e(7119).then(a.bind(a,94191)),meta:{c:["指南"],g:["布局"],e:'<p>布局包括:</p>\n<ul>\n<li><a href="https://theme-hope.vuejs.press/zh/guide/layout/navbar.html" target="_blank" rel="noopener noreferrer">导航栏</a></li>\n<li><a href="https://theme-hope.vuejs.press/zh/guide/layout/sidebar.html" target="_blank" rel="noopener noreferrer">侧边栏</a></li>\n<li><a href="https://theme-hope.vuejs.press/zh/guide/layout/footer.html" target="_blank" rel="noopener noreferrer">页脚</a></li>\n</ul>',r:{minutes:.53,words:159},t:"布局",i:"object-group",O:2,y:"a"}}],["/zh/demo/markdown.html",{loader:()=>a.e(7584).then(a.bind(a,6732)),meta:{c:["使用指南"],g:["Markdown"],e:"<p>VuePress 主要从 Markdown 文件生成页面。因此，你可以使用它轻松生成文档或博客站点。</p>\n<p>你需要创建并编写 Markdown，以便 VuePress 可以根据文件结构将它们转换为不同的页面。</p>\n",r:{minutes:3.47,words:1041},t:"Markdown 展示",i:"fab fa-markdown",O:2,y:"a"}}],["/zh/demo/page.html",{loader:()=>a.e(6590).then(a.bind(a,68604)),meta:{a:"Ms.Hope",d:15778368e5,l:"2020年1月1日",c:["使用指南"],g:["页面配置","使用指南"],u:!0,v:"/assets/images/cover1.jpg",e:"<p><code>more</code> 注释之前的内容被视为文章摘要。</p>\n",r:{minutes:1.76,words:529},t:"页面配置",i:"file",O:3,y:"a"}}],["/zh/posts/ai-impls/yolov9.html",{loader:()=>a.e(1668).then(a.bind(a,83160)),meta:{e:'\n<h2>导航</h2>\n<ul>\n<li><a href="notion://www.notion.so/Blog1-YOLOv9-5bfb1b8d97844ba99c6f081ed667721d?pvs=12#%E5%BC%95%E8%A8%80" target="_blank" rel="noopener noreferrer">引言</a>\n<ul>\n<li><a href="notion://www.notion.so/Blog1-YOLOv9-5bfb1b8d97844ba99c6f081ed667721d?pvs=12#YOLOv9%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0" target="_blank" rel="noopener noreferrer">YOLOv9 模型概述</a>\n<ul>\n<li><a href="notion://www.notion.so/Blog1-YOLOv9-5bfb1b8d97844ba99c6f081ed667721d?pvs=12#%E6%A8%A1%E5%9E%8B%E6%A1%86%E6%9E%B6%E5%9B%BE" target="_blank" rel="noopener noreferrer">模型框架图</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href="notion://www.notion.so/Blog1-YOLOv9-5bfb1b8d97844ba99c6f081ed667721d?pvs=12#%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86" target="_blank" rel="noopener noreferrer">环境搭建及训练推理</a>\n<ul>\n<li><a href="notion://www.notion.so/Blog1-YOLOv9-5bfb1b8d97844ba99c6f081ed667721d?pvs=12#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE" target="_blank" rel="noopener noreferrer">环境配置</a></li>\n<li><a href="notion://www.notion.so/Blog1-YOLOv9-5bfb1b8d97844ba99c6f081ed667721d?pvs=12#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87" target="_blank" rel="noopener noreferrer">数据集准备</a></li>\n<li><a href="notion://www.notion.so/Blog1-YOLOv9-5bfb1b8d97844ba99c6f081ed667721d?pvs=12#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B" target="_blank" rel="noopener noreferrer">训练过程</a></li>\n<li><a href="notion://www.notion.so/Blog1-YOLOv9-5bfb1b8d97844ba99c6f081ed667721d?pvs=12#%E6%B5%8B%E8%AF%95%E5%92%8C%E8%AF%84%E4%BC%B0" target="_blank" rel="noopener noreferrer">测试和评估</a></li>\n<li><a href="notion://www.notion.so/Blog1-YOLOv9-5bfb1b8d97844ba99c6f081ed667721d?pvs=12#%E5%AE%9E%E8%B7%B5%E5%BA%94%E7%94%A8" target="_blank" rel="noopener noreferrer">实践应用</a></li>\n</ul>\n</li>\n<li><a href="notion://www.notion.so/Blog1-YOLOv9-5bfb1b8d97844ba99c6f081ed667721d?pvs=12#%E6%8A%A5%E9%94%99%E4%BF%AE%E5%A4%8D" target="_blank" rel="noopener noreferrer">报错修复</a></li>\n<li><a href="notion://www.notion.so/Blog1-YOLOv9-5bfb1b8d97844ba99c6f081ed667721d?pvs=12#%E6%80%BB%E7%BB%93%E5%92%8C%E5%B1%95%E6%9C%9B" target="_blank" rel="noopener noreferrer">总结和展望</a></li>\n<li><a href="notion://www.notion.so/Blog1-YOLOv9-5bfb1b8d97844ba99c6f081ed667721d?pvs=12#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5" target="_blank" rel="noopener noreferrer">参考链接</a></li>\n</ul>',r:{minutes:6.16,words:1847},t:"YOLOv9 代码复现",y:"a"}}],["/zh/posts/ai-weekly/001.html",{loader:()=>a.e(1583).then(a.bind(a,37903)),meta:{e:'\n<h2>目录</h2>\n<ol>\n<li><a href="notion://www.notion.so/657f0864717c46feb4e178340c835a83?v=b369a7ce1cdd4ec185a435fc80bf1dc1&amp;p=d3052cfc27554302961952cc405f6431&amp;pm=s#show-o-%E7%BB%9F%E4%B8%80%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E4%B8%8E%E7%94%9F%E6%88%90%E7%9A%84transformer" target="_blank" rel="noopener noreferrer">Show-o: 统一多模态理解与生成的 Transformer</a></li>\n<li><a href="notion://www.notion.so/657f0864717c46feb4e178340c835a83?v=b369a7ce1cdd4ec185a435fc80bf1dc1&amp;p=d3052cfc27554302961952cc405f6431&amp;pm=s#csgo-%E5%9F%BA%E4%BA%8E%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E4%B8%8E%E7%94%9F%E6%88%90" target="_blank" rel="noopener noreferrer">CSGO: 基于扩散模型的风格迁移与生成</a></li>\n<li><a href="notion://www.notion.so/657f0864717c46feb4e178340c835a83?v=b369a7ce1cdd4ec185a435fc80bf1dc1&amp;p=d3052cfc27554302961952cc405f6431&amp;pm=s#instantstyle-plus-%E4%BC%98%E5%8C%96%E5%86%85%E5%AE%B9%E4%BF%9D%E7%95%99%E4%B8%8E%E9%A3%8E%E6%A0%BC%E5%A2%9E%E5%BC%BA%E7%9A%84%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E6%96%B9%E6%B3%95" target="_blank" rel="noopener noreferrer">InstantStyle-Plus: 优化内容保留与风格增强的风格迁移方法</a></li>\n<li><a href="notion://www.notion.so/657f0864717c46feb4e178340c835a83?v=b369a7ce1cdd4ec185a435fc80bf1dc1&amp;p=d3052cfc27554302961952cc405f6431&amp;pm=s#swiftbrush-v2-%E4%B8%8B%E4%B8%80%E4%BB%A3%E9%AB%98%E6%95%88ai%E7%BB%98%E7%94%BB%E5%B7%A5%E5%85%B7" target="_blank" rel="noopener noreferrer">SwiftBrush V2: 下一代高效 AI 绘画工具</a></li>\n<li><a href="notion://www.notion.so/657f0864717c46feb4e178340c835a83?v=b369a7ce1cdd4ec185a435fc80bf1dc1&amp;p=d3052cfc27554302961952cc405f6431&amp;pm=s#kotaemon-%E9%AB%98%E6%95%88%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA" target="_blank" rel="noopener noreferrer">Kotaemon: 高效的多模态聊天机器人</a></li>\n<li><a href="notion://www.notion.so/657f0864717c46feb4e178340c835a83?v=b369a7ce1cdd4ec185a435fc80bf1dc1&amp;p=d3052cfc27554302961952cc405f6431&amp;pm=s#linly-dubbing-%E5%9F%BA%E4%BA%8Eai%E7%9A%84%E9%AB%98%E6%95%88%E9%85%8D%E9%9F%B3%E5%B7%A5%E5%85%B7" target="_blank" rel="noopener noreferrer">Linly-Dubbing: 基于 AI 的高效配音工具</a></li>\n<li><a href="notion://www.notion.so/657f0864717c46feb4e178340c835a83?v=b369a7ce1cdd4ec185a435fc80bf1dc1&amp;p=d3052cfc27554302961952cc405f6431&amp;pm=s#hivisionidphotos-%E5%9F%BA%E4%BA%8Eai%E7%9A%84%E8%AF%81%E4%BB%B6%E7%85%A7%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7" target="_blank" rel="noopener noreferrer">HivisionIDPhotos: 基于 AI 的证件照生成工具</a></li>\n</ol>',r:{minutes:6.63,words:1990},t:"AI 证件照生成工具 HivisionIDPhotos 引爆社交媒体 | Show-o 探索多模态未来 | 小红书 InstantX 团队提出风格迁移新方法 CSGO【AI 周报】",y:"a"}}],["/zh/posts/ai-weekly/002.html",{loader:()=>a.e(2486).then(a.bind(a,76963)),meta:{e:"\n<p>https://mmbiz.qpic.cn/sz_mmbiz_png/NM6DecUSXYtWYIE9aUXFP1YiaWE4UoQCz0MUrkqRNap6WlMeYABdXSgmxsnKAibk0GwZicjt1HX0nCTjVO6HYpzAQ/0?wx_fmt=png&amp;from=appmsg</p>\n<h2><strong>目录</strong></h2>\n<ol>\n<li>DepthCrafter: 为开放世界视频生成一致的长深度序列</li>\n<li>LinFusion: 高效生成 16K 图像的新方法</li>\n<li>Mini-Omni: 实时语音对话模型</li>\n<li>StyleTokenizer: 控制 Diffusion 模型的图像风格生成</li>\n<li>HiPrompt: 强化提示词优化框架</li>\n<li>Geometry Image Diffusion: 基于图像的高效文本生成 3D 模型</li>\n<li>Guide-and-Rescale: 实现无调优高效图像编辑的自引导机制</li>\n</ol>",r:{minutes:4.53,words:1359},t:"Unity 自研 3D 材质生成大模型 | 支付宝推出 Style Tokenizer|LinFusion 高效生成 16K 图像【AI 周报】",y:"a"}}],["/zh/posts/ai-weekly/003.html",{loader:()=>a.e(9437).then(a.bind(a,34012)),meta:{e:'\n<h2></h2>\n<figure><img src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/NM6DecUSXYsGhqP9NJy88No01ZhhQgNN0Kyx6ZYST4ZKicnDJXLVEnvKfDRicYzhc1hJ7UW0tibP5X0VZG02voBvQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>\n<h2><strong>概要</strong></h2>',r:{minutes:4.98,words:1494},t:"SaRA 高效微调 Diffusion 模型|Fish-Speech 更新多语种语音生成|IFAdapter 提升图像特征控制【AI 周报】",y:"a"}}],["/zh/posts/ai-weekly/004.html",{loader:()=>a.e(9092).then(a.bind(a,77950)),meta:{e:"\n<h2>摘要</h2>\n<p>本期周报涵盖 Qwen 2.5 Coder 的代码生成进展、InstantDrag 的交互式图像生成工具、以及 Omnigen 的多模态生成研究。此外，还介绍了 Diffusion-e2e-ft 扩散模型优化、LVCD 的线稿视频上色方法和 MoCoop 多模态合作学习。</p>\n<h2>目录</h2>\n<ol>\n<li>Qwen 2.5 Coder: 代码生成的最新突破</li>\n<li>InstantDrag: 交互式图像拖拽生成新工具</li>\n<li>Diffusion-e2e-ft: 自监督扩散模型优化</li>\n<li>Omnigen: 灵活生成模型的研究进展</li>\n<li>LVCD: 基于扩散模型的线稿视频上色</li>\n<li>MoCoop: 多模态合作学习的新方法</li>\n</ol>",r:{minutes:4.26,words:1278},t:"Qwen 2.5 Coder: 多语言代码生成新高度，InstantDrag 实时交互式图像生成，Omnigen 多模态生成研究进展【AI 周报】",y:"a"}}],["/zh/posts/ai-weekly/005.html",{loader:()=>a.e(7307).then(a.bind(a,97816)),meta:{e:"\n<h2><strong>摘要</strong></h2>\n<p>本周 AI 周报聚焦最新图像生成技术：Imagine yourself 无需调优即可生成个性化图像，NovelAI DiffusionV3 提升了生成效率与质量，智源研究院发布的多模态模型 Emu3 展现跨模态生成强大潜力，为 AI 发展带来更多创新可能。</p>\n<h2><strong>目录</strong></h2>\n<ol>\n<li>Imagine yourself：无需调优的个性化图像生成</li>\n<li>NovelAI Diffusion V3 中对 SDXL 的改进</li>\n<li>Make Pixels Dance: 扩散模型中的动态图像生成</li>\n<li>Molecular Modeling AI Initiative by Allen Institute</li>\n<li>MIMO: Vision Transformer with Multimodal Input</li>\n<li>Emu: BAAI 的多模态大模型项目</li>\n<li>PromptSliders: 基于滑动条的提示优化模型</li>\n</ol>",r:{minutes:7.62,words:2287},t:"Imagine yourself 无调优图像生成亮相 | NovelAI Diffusion V3 提升生成效率 | BAAI 推出多模态模型 Emu3【AI 周报】",y:"a"}}],["/zh/posts/ai-weekly/006.html",{loader:()=>a.e(9682).then(a.bind(a,20737)),meta:{e:'\n<h2><strong>摘要</strong></h2>\n<p>本周 AI 周报聚焦生成模型与视觉技术突破：Meta 发布 Movie Gen，推动文本到视频的生成新模式；Ultralytics 推出 YOLOv11，优化目标检测与图像分类性能；华盛顿大学的 Inverse Painting 模型重现艺术创作过程。此外，Illustrious XL 与 FabricDiffusion 等模型在插画生成和 3D 服装纹理迁移上也带来显著提升，进一步拓展了 AI 在多领域的应用潜力。</p>\n<h2><strong>目录</strong></h2>\n<ol>\n<li><a href="#inverse-painting-%E5%9F%BA%E4%BA%8E-diffusion-%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%98%E7%94%BB%E9%87%8D%E6%9E%84">Inverse Painting: 基于 Diffusion 模型的绘画过程重构</a></li>\n<li><a href="#illustrious-xl-%E4%B8%93%E4%B8%BA%E6%8F%92%E7%94%BB%E8%AE%BE%E8%AE%A1%E7%9A%84%E8%89%BA%E6%9C%AF%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">Illustrious XL: 专为插画设计的艺术生成模型</a></li>\n<li><a href="#comfygen-%E5%9F%BA%E4%BA%8E-llm-%E7%9A%84%E8%87%AA%E9%80%82%E5%BA%94%E7%94%9F%E6%88%90%E5%B7%A5%E4%BD%9C%E6%B5%81">ComfyGen: 基于 LLM 的自适应生成 ComfyUI 工作流</a></li>\n<li><a href="#fabricdiffusion-%E9%AB%98%E4%BF%9D%E7%9C%9F-3d-%E6%9C%8D%E8%A3%85%E7%BA%B9%E7%90%86%E8%BF%81%E7%A7%BB">FabricDiffusion: 高保真 3D 服装纹理迁移</a></li>\n<li><a href="#training-free-image-style-transfer-%E5%88%A9%E7%94%A8-latent-diffusion-%E8%BF%9B%E8%A1%8C%E6%97%A0%E8%AE%AD%E7%BB%83%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB">STRDP: 利用 Latent Diffusion 进行无训练风格迁移</a></li>\n<li><a href="#movie-gen-metas-ai%E9%A9%B1%E5%8A%A8%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90">Movie Gen: Meta\'s AI 驱动视频生成</a></li>\n<li><a href="#yolov11-%E6%96%B0%E4%B8%80%E4%BB%A3%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B">YOLOv11: 新一代目标检测与分类模型</a></li>\n</ol>',r:{minutes:9.95,words:2984},t:"Meta 发布 Movie Gen 开启 AI 生成视频新时代 | YOLOv11 引领目标检测革新 | 华盛顿大学 Inverse Painting 重构绘画过程【AI 周报】",y:"a"}}],["/zh/posts/ai-weekly/007.html",{loader:()=>a.e(5961).then(a.bind(a,37083)),meta:{e:"\n<p>!https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/3a0efd56-5a31-4661-910b-55d91f9f82b0/width=450/33654117.jpeg</p>\n<h2>摘要</h2>\n<p>本期汇总展示了六个前沿AI项目，涵盖从图像、视频生成的创新。CtrlX提供无损图像编辑，PixelShuffler实现自监督图像去噪，TextToon推动漫画生成技术，HybridBooth聚焦个性化头像生成，其余论文及项目详见正文。</p>\n<h2>目录</h2>\n<ol>\n<li>CtrlX: 可控图像编辑框架</li>\n<li>PixelShuffler: 高效的自监督图像去噪</li>\n<li>TextToon: 文本驱动的实时漫画化头像生成</li>\n<li>HybridBooth: 高效的个性化生成模型</li>\n<li>Pyramid-Flow: 高效的分层可逆生成模型</li>\n<li>Aria: 开源多模态专家模型</li>\n</ol>",r:{minutes:5.02,words:1506},t:"英伟达发布可控图像编辑框架|字节跳动TextToon实时漫画化头像生成|Vivo推出HybridBooth个性化生成【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/008.html",{loader:()=>a.e(8784).then(a.bind(a,44274)),meta:{e:'\n<figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img12@main/2024/10/20/1729433178592-bd22ef03-8cac-448b-9ab1-3f2dcddac6c3.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>\n<h2><strong>摘要</strong></h2>\n<p>本周AI周报关注几项前沿生成模型：ZeroComp在3D合成领域开辟新路径，CtrLoRA实现可控图像生成的高效框架，F5-TTS通过流匹配技术提升语音生成效果，HyperDreamBooth加快个性化文本到图像的速度。其余成果详见正文。</p>',r:{minutes:4.36,words:1309},t:"阿里开源Animate-X革新AI角色动画|ZeroComp实现创新3D对象合成|F5-TTS提升TTS语音的自然度【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/009.html",{loader:()=>a.e(9383).then(a.bind(a,88565)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/4e3f1d8a-beb1-4b84-bbdf-b3f61b4b2d3f/width=450/35854696.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>\n<h2><strong>摘要</strong></h2>\n<p>本周，Stability AI的Stable Diffusion 3.5提升图像生成精度，Hugging Face推出无代码AutoTrain Advanced平台降低机器学习门槛，MagicTailor则在图像个性化编辑上实现更高控制度。</p>',r:{minutes:6.76,words:2027},t:"Stable Diffusion3.5革新生成精度|AutoTrain高效炼丹|MagicTailor个性化图像编辑【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/010.html",{loader:()=>a.e(1005).then(a.bind(a,68781)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/c1d5f4ea-9cad-4e0b-8923-492d97bfce0e/width=450/1941-(img_1018CR210), Hack Forums scrapped-flux1-dev-fp8-compact-99638172.jpeg" alt="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/c1d5f4ea-9cad-4e0b-8923-492d97bfce0e/width=450/1941-(img_1018CR210),%20Hack%20Forums%20scrapped-flux1-dev-fp8-compact-99638172.jpeg" tabindex="0" loading="lazy"><figcaption>https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/c1d5f4ea-9cad-4e0b-8923-492d97bfce0e/width=450/1941-(img_1018CR210),%20Hack%20Forums%20scrapped-flux1-dev-fp8-compact-99638172.jpeg</figcaption></figure>',r:{minutes:5.96,words:1788},t:"FaceChain更新人脸转换引领个性生成|GenArtist多模态艺术生成系统|AutoKaggle竞赛助手革新【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/011.html",{loader:()=>a.e(4246).then(a.bind(a,38738)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bb18d643-4bac-439b-a647-35b9355aee31/width=450/38613991.jpeg" alt="封面" tabindex="0" loading="lazy"><figcaption>封面</figcaption></figure>\n<h2>摘要</h2>\n<p>本周聚焦视觉与生成领域突破：腾讯混元3D 推出高效3D重建工具，支持实时场景重建；MVPaint提升3D材质一致性，实现高质量多视角纹理生成；PromptFix利用Diffusion模型进行多任务图像修复，覆盖上色、去雾等。其余详见正文。</p>',r:{minutes:6.53,words:1959},t:"腾讯Hunyuan3D重塑3D重建|MVPaint多视角提升3D材质一致性|PromptFix实现高质量图像修复【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/012.html",{loader:()=>a.e(5519).then(a.bind(a,57643)),meta:{e:'\n<figure><img src="https://prod-files-secure.s3.us-west-2.amazonaws.com/021ded55-a224-419c-939c-70c6888912f7/d1a9dc14-03bb-4a6d-9bde-08c809989d52/微信logo.png" alt="微信logo.png" tabindex="0" loading="lazy"><figcaption>微信logo.png</figcaption></figure>\n<h2><strong>摘要</strong></h2>\n<p>本周聚焦多模态AI发展：TANGO创新语音驱动手势视频；StoryTeller支持长视频剧情生成；ADD-IT无训练对象插入；MikuDance合成动漫角色舞蹈；LLaMA-Mesh统一3D网格与语言模型。其余详见正文。</p>',r:{minutes:6.38,words:1915},t:"TANGO赋能数字人|ADD-IT重构图像编辑|MikuDance动漫角色舞蹈动画【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/013.html",{loader:()=>a.e(1944).then(a.bind(a,48276)),meta:{e:'\n<figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img18@main/2024/11/24/1732457006798-1774d1ab-5764-437c-a50e-f922f2198952.png" alt="封面" tabindex="0" loading="lazy"><figcaption>封面</figcaption></figure>\n<h2><strong>摘要</strong></h2>\n<p>本周生成与检测技术亮点：JoyVASA 利用扩散模型实现音频驱动动画生成，支持人像与动物；DINO-X 面向开放世界目标检测，通过多模态提示提升长尾检测表现；StyleCodes 将图像风格编码为Base64格式，简化迁移过程并提升生成灵活性与质量。详情见正文。</p>',r:{minutes:5.1,words:1531},t:"JoyVASA突破多模态动画生成 | DINO-X定义开放世界检测 | StyleCodes实现风格编码迁移【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/014.html",{loader:()=>a.e(8617).then(a.bind(a,27801)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/7ec38594-6a9f-469a-9515-cc2a8d4ae2f5/original=true,quality=90/42025535.jpeg" alt="封面源自C站作者Hannibal_Lecter" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Hannibal_Lecter</figcaption></figure>\n<h2>摘要</h2>\n<p>本周聚焦生成式AI创新：MaterialAnything自动生成PBR材质；OminiControl为Diffusion模型提供通用轻量控制框架；FlipSketch生成草图动画。这些进步展示了生成式AI的迭代效率之高，其余内容详见正文。</p>',r:{minutes:7.46,words:2239},t:"MaterialAnything生成PBR材质|ConsisID优化ID视频生成|FlipSketch实现草图动画【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/015.html",{loader:()=>a.e(6226).then(a.bind(a,62958)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/1c9c0f79-771e-4132-95a5-75f511af7850/width=450/43470959.jpeg" alt="封面源自C站作者navimixu" tabindex="0" loading="lazy"><figcaption>封面源自C站作者navimixu</figcaption></figure>\n<h2>摘要</h2>\n<p>本周聚焦生成式AI：TokenFlow提供多模态生成框架；NitroFusion实现一步图像生成；Imagine360支持全景内容创作；HunyuanVideo引领多模态视频生成；Art-Free-Diffusion打破风格限制。详见正文。</p>',r:{minutes:5.53,words:1660},t:"多模态理解与生成统一Token化框架|NitroFusion单步图像生成|Imagine360探索全景生成视频【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/016.html",{loader:()=>a.e(4171).then(a.bind(a,49182)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/49053226-463c-4b1a-ae6d-74e4079be9f0/width=450/44419355.jpeg" alt="封面源自C站作者tavoltennis837" tabindex="0" loading="lazy"><figcaption>封面源自C站作者tavoltennis837</figcaption></figure>\n<h2>摘要</h2>\n<p>本周聚焦生成与编辑：EasyRef 推出高效视频参考生成；SynCamMaster 实现多相机视角同步；StyleMaster 提供艺术风格视频转换；SwiftEdit 用单步扩散实现高速图像编辑，助力创意表达。详见正文。</p>',r:{minutes:4.48,words:1344},t:"EasyRef多模态高效图参考生成框架|SynCamMaster实现视角同步|SwiftEdit支持高速图像编辑【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/017.html",{loader:()=>a.e(2676).then(a.bind(a,72425)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/18c08401-31c6-4737-a8f6-cc937016cb91/original=true,quality=90/46150617.jpeg" alt="封面源自C站作者ShiroNekoAlpha" tabindex="0" loading="lazy"><figcaption>封面源自C站作者ShiroNekoAlpha</figcaption></figure>\n<h2>摘要</h2>\n<p>本周聚焦生成与编辑：AniDoc简化动画上色；ChatDiT聊天解决图像编辑；GenEx生成3D视频；DynamicControl加强图像生成控制，BrushEdit高效修复编辑图像。其余详见正文。</p>',r:{minutes:5.14,words:1543},t:"AniDoc简化动画上色||ChatDiT聊天解决图像编辑|GenEx生成3D视频【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/018.html",{loader:()=>a.e(9957).then(a.bind(a,87242)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/b338b22c-394c-4ae0-88d2-2bf15fa83809/original=true,quality=90/47422369.jpeg" alt="封面源自C站作者Meower2024" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Meower2024</figcaption></figure>\n<h2>摘要</h2>\n<p>本周聚焦大模型与多模态：Qwen2.5 优化预训练与后训练，定义多模态模型新标准；DeepSeek-V3 引入混合专家架构，提升训练效率；Mulberry 结合蒙特卡罗树搜索，增强推理与反思能力。详见正文。</p>',r:{minutes:6.01,words:1803},t:"Qwen2.5定义多模态模型新高度|DeepSeek-V3突破混合专家能力|Mulberry强化推理反思【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/019.html",{loader:()=>a.e(8078).then(a.bind(a,55072)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/151c21e5-8ab2-449b-a7b3-b315df75432e/original=true,quality=90/6GXND6WWA1T2KAEJE9AWEA47G0.jpeg" alt="封面源自C站作者Klasker2025" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Klasker2025</figcaption></figure>\n<h2>摘要</h2>\n<p>本周重点：StoryWeaver提出Character Graph解决角色一致性与语义对齐问题；1.58bitFLUX极低比特量化FLUX；Orient-Anything实现零样本3D方向估计；Edicho推动一致性图像编辑。详情见正文。</p>',r:{minutes:5.45,words:1634},t:"StoryWeaver重塑视觉故事生成|Edicho实现一致性图像编辑|VideoAnyDoor增强视频对象插入【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/020.html",{loader:()=>a.e(3230).then(a.bind(a,73890)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/70d2b83a-dd62-4984-b15c-4150de0344ea/original=true,quality=90/49673554.jpeg" alt="封面源自C站作者martinffm_pg" tabindex="0" loading="lazy"><figcaption>封面源自C站作者martinffm_pg</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：AutoPresent提高幻灯片制作效率；ConceptMaster解耦多概念视频生成；Hallo3推动动态肖像动画；Cosmos引领物理AI开发；SPAR3D实现单图像3D重建；R3GAN推动GAN重回图像生成之巅。详情见正文。</p>',r:{minutes:5.84,words:1753},t:"AutoPresent提升幻灯片制作效率 | Hallo3实现动态肖像动画 | R3GAN媲美Diffusion模型【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/021.html",{loader:()=>a.e(277).then(a.bind(a,93749)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/c6f924f2-97b3-45ab-94f6-0c29066798aa/anim=false,width=450/51311073.jpeg" alt="封面源自C站作者karlanan" tabindex="0" loading="lazy"><figcaption>封面源自C站作者karlanan</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：MiniMax-01采用Lightning Attention支持超长文本处理，提升多模态理解；Seaweed-APT通过对抗性后训练实现高分辨率视频的即时生成；AnyDressing利用定制化网络提供多服装虚拟试衣。详情见正文。</p>',r:{minutes:6.16,words:1848},t:"MiniMax-01扩展长文本处理|Seaweed-APT一步视频生成|AnyDressing个性化虚拟试衣【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/022.html",{loader:()=>a.e(4864).then(a.bind(a,96799)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/5ea5d962-dcc2-4afe-983e-9b4eedb1f20d/original=true,quality=90/52768659.jpeg" alt="封面源自C站作者AIdaFONDA" tabindex="0" loading="lazy"><figcaption>封面源自C站作者AIdaFONDA</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：FilmAgent实现多智能体协作虚拟电影制作；DeepSeek-R1通过强化学习提升推理性能；EMO-2单图驱动数字人；PASA增强学术搜索效率；Textoon文本生成Live2D模型。详情见正文。</p>',r:{minutes:6.97,words:2092},t:"DeepSeek-R1定义强化学习推理|EMO2高效语音驱动数字|Textoon文本生成Live2D模型【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/023.html",{loader:()=>a.e(7959).then(a.bind(a,39527)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/1b61e25e-d444-4ca5-9a95-4db03f2aafc8/anim=false,width=450/54073781.jpeg" alt="封面源自C站作者roxin282" tabindex="0" loading="lazy"><figcaption>封面源自C站作者roxin282</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：CatV2TON利用DiT统一视频虚拟试穿；Janus-Pro增强多模态理解生成；Baichuan-Omni-1.5开源全模态模型；Qwen2.5-1M突破128K长文本生成；AtlaAI提出小型语言模型评估器。详情见正文。</p>',r:{minutes:5.75,words:1724},t:"CatV2TON利用扩散Transformer实现虚拟试穿|Janus-Pro多模态理解与生成|Qwen2.5-1M扩展输入上限【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/024.html",{loader:()=>a.e(9098).then(a.bind(a,2525)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/e958ef00-1183-47ee-91f5-c1f1023f08a3/anim=false,width=450/55557070.jpeg" alt="封面源自C站作者Clear_Note" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Clear_Note</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：MILS展示了无需训练的多模态理解方法；LayerTracer通过DiT改进SVG生成；MakeAnything利用DiT优化程序化序列生成；OmniHuman生成高保真数字人视频；MatAnyone提供稳定的视频抠像方案；s1模型在推理任务中展现强劲实力。详情见正文。</p>',r:{minutes:5.75,words:1725},t:"MILS零训练多模态理解|LayerTracer优化SVG生成|OmniHuman高保真数字人【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/025.html",{loader:()=>a.e(1233).then(a.bind(a,75702)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/f570fad2-1880-4978-a1cd-8f1e4e6e5494/original=true,quality=90/56995455.jpeg" alt="封面源自C站作者iviyaa" tabindex="0" loading="lazy"><figcaption>封面源自C站作者iviyaa</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：Whisk将概念嵌入场景并风格迁移；Zonos领先的开源文本转语音模型；Light-A-Video无需训练即可重光照视频；Goku 提供高效的视频生成方案；Data Formulator让数据可视化更智能。详情见正文。</p>',r:{minutes:5.84,words:1751},t:"Magic 1-For-1高效生成1分钟视频|Zonos最强开源文本转语音|Light-A-Video零样本重光照视频生成【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/026.html",{loader:()=>a.e(3500).then(a.bind(a,68624)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/caa2baeb-3ea6-4ff5-9cbb-24e61fc4157e/original=true,quality=90/58399173.jpeg" alt="封面源自C站作者PBtheCreator" tabindex="0" loading="lazy"><figcaption>封面源自C站作者PBtheCreator</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：Step-Video-T2V提升视频质量和一致性；YOLOv12在目标检测任务中提供更强推理能力和高精度；WHAM模型首次为游戏内容生成提供了创新性解决方案；Dynamic Concepts实现了跨模态的动态概念推理。详情见正文。</p>',r:{minutes:7.08,words:2125},t:"Step-Video-T2V文本到视频生成|YOLOv12目标检测优化|WHAM游戏内容生成【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/027.html",{loader:()=>a.e(1603).then(a.bind(a,86928)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/9c4a95e1-b11d-4215-8948-5a4352700c60/original=true,quality=90/60618576.jpeg" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：PhotoDoodle少样本艺术化图像编辑；Wan2.1优化资源高效生成视频；K-LoRA无训练融合增强风格适配；FractalGen利用分形生成复杂结构；KV-Edit精准保持背景编辑图像；GHOST 2.0提升头部替换保真度。</p>',r:{minutes:5.96,words:1788},t:"PhotoDoodle艺术化图像编辑|Wan2.1成为最新视频生成SOTA|FractalGen分形自回归生图【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/028.html",{loader:()=>a.e(7622).then(a.bind(a,34382)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/ba745a8d-475e-4060-9590-870302076a56/original=true,quality=90/60528420.jpeg" alt="封面源自C站作者Qvoheu" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Qvoheu</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：Attention Distillation基于扩散模型注意力机制实现精准风格迁移；微软Phi-4数学推理远超同类；EgoLife提出EgoGPT/EgoRAG，推进自我视角AI助手；HunyuanVideo-I2V优化图转视频。</p>',r:{minutes:5.85,words:1756},t:"Attention Distillation精准风格迁移 | Phi-4数学推理登顶 | EgoLife自我视角AI助手【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/029.html",{loader:()=>a.e(7549).then(a.bind(a,91242)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/271d5fc4-6d65-4e6d-ab43-30b3e253eb46/original=true,quality=90/00053-3724718039.jpeg" alt="封面源自C站作者AmberFog" tabindex="0" loading="lazy"><figcaption>封面源自C站作者AmberFog</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：YuE 基于 LLaMA2 架构，实现长篇 AI 音乐创作与风格转换；VACE 采用 Video Condition Unit，统一视频生成与编辑任务；VideoPainter 提出双分支框架，提升视频修复质量与身份一致性；ConsisLoRA 解决 LoRA 风格迁移中的一致性问题。</p>',r:{minutes:7.68,words:2305},t:"YuE赋能长篇AI音乐创作|VACE统一视频编辑|MagicInfinite无限制视频生成【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/030.html",{loader:()=>a.e(5483).then(a.bind(a,30840)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/0f0f3d34-216b-4954-bede-82cc10657d8d/original=true,quality=90/00178-3302738830-masterpiece, best quality, good quality, very aesthetic, absurdres, newest, 8K, depth of field, focused subject, dynamic ange, w.jpeg" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>',r:{minutes:6.14,words:1841},t:"ReCamMaster重塑视频视角 | PLADIS优化Diffusion推理 | Impossible Videos探索反现实【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/031.html",{loader:()=>a.e(3668).then(a.bind(a,88609)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/9469f5ed-e000-4aeb-ab93-4b40865a7c7b/original=true,quality=90/00320-919909395-1girl, hatsune_miku, lips, thin lips, parted lips, solo, looking at viewer, camisole, upper body, dark, underlightling, masterpi.jpeg" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>',r:{minutes:7.57,words:2270},t:"Conceptrol精准概念控制 | ChatAnyone实时数字人|Lumina-Image 2.0突【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/032.html",{loader:()=>a.e(3161).then(a.bind(a,79307)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/43d1dfeb-a0bc-4d38-a7d5-2694070fac43/original=true,quality=90/00687-2746924664-vaporwave" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：TextCrafter精准渲染文本；MoCha推出电影级角色合成；Any2Caption增强视频生成；AnimeGamer实现动漫生活模拟；ACTalker多模态音视频生成；OpenDeepSearch赋能搜索AI。其余详见正文。</p>',r:{minutes:7.34,words:2202},t:"TextCrafter精准文本渲染|MoCha电影级角色合成|AnimeGamer动漫生活模拟【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/033.html",{loader:()=>a.e(3842).then(a.bind(a,69444)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/7ca09232-41e5-47f6-a2dc-39eb85b35853/original=true,quality=90/00883-3137192565-masterpiece" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：OmniCaptioner实现跨视觉域语言描述；UNO统一个性化定制多主体生成；SPF-Portrait解决语义污染；FantasyTalking生成音频驱动数字人；SmolVLM2发布轻量级视频理解模型。其余详见正文。</p>',r:{minutes:7.48,words:2245},t:"OmniCaptioner统一视觉描述 | UNO多主体定制 | SPF-Portrait消除污染提升肖像定制【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/034.html",{loader:()=>a.e(1551).then(a.bind(a,76262)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/781e2f08-150f-4ce9-8cfe-0b9865785381/original=true,quality=90/00021-2797360292.jpeg" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：HiDream-I1 发布17B参数基础模型，支持多风格图像生成；Cobra 引入稀疏DiT，支持200+图像指导漫画上色；InstantCharacter 结合视觉编码与适配器结构，实现灵活角色定制。其余详见正文。</p>',r:{minutes:6.32,words:1895},t:"HiDream-I1引领图像生成 | Cobra支持海量上色参考 | InstantCharacter高效定制角色【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/035.html",{loader:()=>a.e(6712).then(a.bind(a,6002)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/cc9b2c75-69a4-4156-88c6-4a3ba910cff8/width=800,original=false/01145-1691900756-masterpiece,best quality,amazing quality,ultra high res,_kanna kamui,silver dragon horns,_bioluminescent scales,blue sailor unif.jpeg" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>',r:{minutes:6.01,words:1802},t:"FramePack压缩加速视频生成 | Step1X-Edit开源高质量图像编辑 | DreamID高保真换脸【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/036.html",{loader:()=>a.e(5021).then(a.bind(a,40745)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/44cb3616-4cff-4dea-bcb6-caeedb99ceeb/original=true,quality=90/73901561.jpeg" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：ColorizeDiffusion v2 提升动漫草图上色质量；RepText 实现多语言文本渲染；ICEdit 多视角保持身份一致性编辑；Insert Anything 灵活插入任意物体；Nexus-Gen 融合 LLM 与扩散模型统一生成流程；X-Fusion 实现多任务图文对齐能力。</p>',r:{minutes:6.26,words:1878},t:"ColorizeDiffusion v2实现最强动漫草图上色 | RepText 多语言文本渲染 | Nexus-Gen 图像生成统一模型【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/037.html",{loader:()=>a.e(8566).then(a.bind(a,67551)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/44cb3616-4cff-4dea-bcb6-caeedb99ceeb/original=true,quality=90/73901561.jpeg" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：PixelHacker引入语义结构引导，提升图像修复质量；Voila实现低延迟情感语音交互；LegoGPT从文本生成稳定LEGO结构；HunyuanCustom支持多模态视频定制；SOAP实现单图3D头像建模与动画控制。其余详见正文。</p>',r:{minutes:7.12,words:2136},t:"PixelHacker图像修复新突破 | Voila开源语音大模型 | LegoGPT生成可搭建乐高积木【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/038.html",{loader:()=>a.e(2547).then(a.bind(a,52947)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/646c745b-b1dd-441d-a510-4416734f49a7/anim=false,width=450/ca47c2dc-4c18-4015-b397-bd86d53ba442_0.jpeg" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：BLIP3-o统一理解与生成任务；MiniMax-Speech实现32语种零样本语音克隆；INTELLECT-2首次实现全球去中心化强化学习大模型训练；Step1X-3D发布高保真3D资产生成框架；CAST支持从单图重建结构化3D场景。</p>',r:{minutes:5.91,words:1774},t:"BLIP3-o多模态统一模型 | MiniMax-Speech零样本语音合成 | INTELLECT-2去中心化RL训练【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/039.html",{loader:()=>a.e(12).then(a.bind(a,84456)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：OmniStyle发布百万级风格迁移数据集；Index-AniSora开源二次元视频生成系统；MMaDA实现统一多模态扩散架构；Dolphin革新文档解析效率；NovelSeek构建科研自动化闭环；BAGEL开源GPT Image级多模态模型。</p>',r:{minutes:6.57,words:1970},t:"OmniStyle统一风格迁移 | BAGEL开源实现了GPT Image | NovelSeek构建科研多智能体闭环【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/040.html",{loader:()=>a.e(9212).then(a.bind(a,93108)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg" alt="封面源自C站作者Meower2024" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Meower2024</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：Paper2Poster推出论文转海报自动化工具；OmniConsistency发布风格一致性学习框架；Jodi统一视觉生成与理解；FlowRL优化文本到图像生成流程；HunyuanVideo-Avatar实现音频驱动数字人。</p>',r:{minutes:6.94,words:2082},t:"Paper2Poster论文到海报自动化|OmniConsistency统一风格化|Jodi融合理解与生成的视觉模型【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/041.html",{loader:()=>a.e(9219).then(a.bind(a,12273)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：北大推出统一视觉框架UniWorld-V1；阿里发布Qwen3-Embedding检索模型；ComfyUI-Copilot简化工作流设计；SeedEdit 3.0升级图像编辑系统；RelationAdapter增强视觉关系迁移。</p>',r:{minutes:6.41,words:1924},t:"UniWorld刷榜多模态生成 | ComfyUI智能助手Copilot | SeedEdit 3.0升级图像编辑【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/042.html",{loader:()=>a.e(410).then(a.bind(a,65113)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：MiniCPM4发布超高效端侧模型；Seedance 1.0引领视频生成；ComfyUI-R1自动化工作流创建；PartCrafter革新3D生成；Magistral推出透明多语言推理系统；AniMaker实现AI动画制作。</p>',r:{minutes:7.51,words:2252},t:"MiniCPM4发布高效端侧模型 | Seedance 1.0引领视频生成进阶 | ComfyUI‑R1打造智能工作流引擎【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/043.html",{loader:()=>a.e(6193).then(a.bind(a,6807)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：Wan2.1-FusionX视频生成提速25%；Hunyuan3D-2.1生产级PBR材质3D生成；Show-o2统一多模态架构；MiniMax-M1百万级上下文推理；PartPacker零件级3D生成；其余详见正文。</p>',r:{minutes:7.12,words:2135},t:"Wan2.1 FusionX视频生成升级 | Hunyuan3D-2.1高保真3D创作 | Show-o2统一多模态架构【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/044.html",{loader:()=>a.e(6880).then(a.bind(a,92101)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>\n<h2>摘要</h2>\n<p>本周两点：DnD零训练LoRA高效适配、OmniGen2多模态统一生成、ShareGPT-4o-Image开放多模态、JarvisArt智能修图、Hunyuan3D-2.5高保真3D生成、3D Arena人类偏好评测。相关参考链接请见文末。</p>',r:{minutes:6.22,words:1866},t:"DnD零训练LoRA权重生成 | OmniGen2多模态统一理解与生成 | Hunyuan3D-2.5高保真3D模型生成【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/045.html",{loader:()=>a.e(8103).then(a.bind(a,69904)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：GLM‑4.1V‑Thinking多模态推理、Keye-VL短视频理解、Ovis-U1统一多模态生成、DepthAnything-AC深度估计、Calligrapher数字书法等。其余内容详见正文，代码、论文见文末参考链接。</p>',r:{minutes:6.62,words:1986},t:"GLM‑4.1V‑Thinking注入多模态推理 | Keye-VL聚焦短视频理解 | Ovis-U1统一多模态生成【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/046.html",{loader:()=>a.e(4222).then(a.bind(a,53706)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：MemOS 推出首个大模型记忆操作系统，T‑LoRA 实现单图扩散微调，OmniPart 支持可控 3D 生成，FM‑Vision‑Evals 建立多模态视觉评测基准。详见正文，参考链接见文末。</p>',r:{minutes:6.67,words:2002},t:"T-LoRA开启单图微调新时代 | MemOS发布LLM记忆操作系统 | OmniPart推动3D可编辑生成【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/047.html",{loader:()=>a.e(7973).then(a.bind(a,90959)),meta:{e:'\n<figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg" alt="封面图源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面图源自C站作者Koal2</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：谷歌发布 Gemini 2.5，全面提升多模态理解、超长上下文与视频推理能力；阿里推出 Lumos‑1，高效自回归视频生成；FantasyPortrait 表达增强驱动多角色动画，重构人像生成上限。详见正文，相关参考链接请见文末。</p>',r:{minutes:5.46,words:1638},t:"Gemini 2.5实现多模态大升级|Lumos‑1架构高效生成视频|FantasyPortrait支持多角色动画【AI周报】",y:"a"}}],["/zh/posts/ai-weekly/X01.html",{loader:()=>a.e(9959).then(a.bind(a,79256)),meta:{e:'\n<p><strong>作者</strong>：hzwer(黄哲威), DingXiaoH(丁霄汉)</p>\n<p><a href="https://drive.google.com/file/d/1hbQ8qvVPUndNRSwK2Hq6wLwh8sxapi95/view?usp=sharing" target="_blank" rel="noopener noreferrer">PDF 下载</a> | <a href="https://github.com/hzwer/AIPaperWriting" target="_blank" rel="noopener noreferrer">GitHub 页面</a> | 知乎讨论：<a href="https://zhuanlan.zhihu.com/p/593195527" target="_blank" rel="noopener noreferrer">1</a>-<a href="https://zhuanlan.zhihu.com/p/639732057" target="_blank" rel="noopener noreferrer">2</a>-<a href="https://zhuanlan.zhihu.com/p/627032371" target="_blank" rel="noopener noreferrer">3</a> ｜<a href="https://yuewen.cn/share/145749938443137024?utm_source=share&amp;utm_content=web_linkcopy&amp;version=2" target="_blank" rel="noopener noreferrer">跃问翻译</a> | <a href="https://www.doubao.com/chat/" target="_blank" rel="noopener noreferrer">豆包</a></p>',r:{minutes:15.36,words:4609},t:"AI 会议论文写作完全指南：从零开始构建高质量论文",y:"a"}}],["/zh/posts/dairys/250222.html",{loader:()=>a.e(5871).then(a.bind(a,73467)),meta:{a:"学生小陈",d:17401824e5,l:"2025年2月22日",c:["日记"],g:["随想","开源","大会","实习"],u:!0,v:"/assets/images/dairys/250222/GDC-Cover.jpeg",e:'<h2>介绍</h2>\n<p>今天去参加了在徐汇西岸召开的2025GDC大会（Global Developer Conference），中文名叫全球开发者先锋大会，TA的官方介绍如下：\n今年的主题是”模塑全球 无限可能“，围绕大模型，结合了<strong>算力</strong>、语料、基金等要素，聚焦具身智能、无人驾驶和科学智能等五大领域，及金融、医疗、智能制造等六大行业，做厚产业生态；以”社区的社区“为基础，打造高创新浓度、高创业热度、高人才密度的开发者生态。</p>\n<h2>内容</h2>\n<p><img src="/assets/images/dairys/250222/人形机器人.jpeg" alt="人形机器人" width="300" height="400" loading="lazy">\n<img src="/assets/images/dairys/250222/宇树机器人1.jpeg" alt="宇树机器人" width="300" height="400" loading="lazy">\n<img src="/assets/images/dairys/250222/智元机器人.jpeg" alt="智元机器人" width="300" height="400" loading="lazy"></p>',r:{minutes:9.03,words:2708},t:"参加GDC大会有感",i:"openmoji:code-editor",O:1,y:"a"}}],["/zh/posts/dairys/250223.html",{loader:()=>a.e(2360).then(a.bind(a,54656)),meta:{a:"非厨",d:17402688e5,l:"2025年2月23日",c:["日记"],g:["随想","做饭","公众号"],u:!0,v:"/assets/images/dairys/250223/食材.jpeg =400x300",e:'<h2>早上</h2>\n<p>9点多，自然醒吧，感觉现在睡7个小时就已经开始做梦了，非常浅的睡眠，于是就还是早起了点。起床之后的习惯还是不好，开始躺在床上刷短视频和打游戏。点了个叮咚买菜，玩到差不多11点多才下床拿我的菜。</p>\n<h2>中午</h2>\n<p><img src="/assets/images/dairys/250223/青椒丝.jpeg" alt="青椒丝" width="300" height="400" loading="lazy">\n<img src="/assets/images/dairys/250223/备菜.jpeg" alt="备菜" width="400" height="300" loading="lazy"></p>',r:{minutes:3.69,words:1108},t:"闲暇的周日",i:"twemoji:bell-pepper",O:2,y:"a"}}],["/zh/posts/hf-weekly/001.html",{loader:()=>a.e(2747).then(a.bind(a,1967)),meta:{e:'\n<figure><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG" alt="封面图" tabindex="0" loading="lazy"><figcaption>封面图</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：FLUX多轮一致性编辑、Hunyuan-A13B超长MoE、Kimi-VL-A3B多模态推理等热搜模型持续引领AI社区，涵盖多模态生成、超长上下文、OCR等最新开源进展，推动AI能力边界不断拓展。详见正文，相关参考链接请见文末。</p>',r:{minutes:6.31,words:1894},t:"FLUX.1 Kontext-dev精控编辑 | Hunyuan-A13B超长MoE | Kimi-VL-A3B高效多模态【HF周报】",y:"a"}}],["/zh/posts/hf-weekly/002.html",{loader:()=>a.e(9170).then(a.bind(a,75201)),meta:{e:'\n<figure><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/ai-weekly-cover-002.jpg" alt="封面图" tabindex="0" loading="lazy"><figcaption>封面图</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：GLM-4.1V-9B-Thinking多模态推理、OCRFlux-3B文档视觉理解、DiffuCoder-7B-cpGRPO代码智能等模型引发热议，涵盖视觉理解、OCR、代码、Agent等方向。详见正文，相关参考链接请见文末。</p>',r:{minutes:4.79,words:1437},t:"GLM-4.1V-9B-Thinking刷榜多模态理解 | OCRFlux-3B突破文档理解 | 百度开源ERNIE-4.5系列【HF周报】",y:"a"}}],["/zh/posts/hf-weekly/003.html",{loader:()=>a.e(5305).then(a.bind(a,91934)),meta:{e:'\n<figure><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-with-title.png" alt="封面图" tabindex="0" loading="lazy"><figcaption>封面图</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：兆级大模型Kimi-K2开源，Devstral专攻代码，Phi-4-mini主打轻量推理。此外，Gemma-3n、SmolLM3、AQUA-7B、天工Skywork及NAI Anime V2等模型也齐齐亮相，覆盖从通用到特定领域的创新。AI模型发展持续加速，向更强、更专、更高效演进。详见正文，相关参考链接请见文末。</p>',r:{minutes:5.52,words:1656},t:"月之暗面开源兆级模型Kimi-K2 | Mistral发布代码助手Devstral | 微软Phi-4-mini主打端侧推理【HF周报】",y:"a"}}],["/zh/posts/hf-weekly/004.html",{loader:()=>a.e(40).then(a.bind(a,62426)),meta:{e:'\n<figure><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/hf-weekly-dc-cover.png" alt="封面图" tabindex="0" loading="lazy"><figcaption>封面图</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：Mistral推多模态音频模型，HiDream革新图像编辑，PusaV1高效视频生成。英伟达、LG等模型及Hermes-3、rStar-Coder数据集同步亮相，详情见正文。</p>',r:{minutes:5.27,words:1582},t:"Voxtral-Mini多模态音频 | HiDream-E1-1稀疏扩散图像编辑 | PusaV1高效视频生成【HF周报】",y:"a"}}],["/zh/posts/ielts/simon-task1.html",{loader:()=>a.e(1306).then(a.bind(a,99613)),meta:{e:"\n<h2>A 类小作文 (Task 1) 讲义</h2>\n<h3>第一课：概述</h3>\n<h4>讲义</h4>\n<p><strong>注意：</strong><br>\n题目要求的是将你所看见的内容形成一个报告。千万不要给出你的观点。不要写总结段，而要写概括段。</p>\n<p><strong>题型：</strong></p>\n<table>\n<thead>\n<tr>\n<th>题型</th>\n<th>描述</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1. 线图</td>\n<td></td>\n<td>鸭</td>\n</tr>\n<tr>\n<td>2. 柱状图</td>\n<td>数字 描述</td>\n<td></td>\n</tr>\n<tr>\n<td>3. 饼图</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>4. 表格</td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>5. 地图</td>\n<td>比较</td>\n<td></td>\n</tr>\n<tr>\n<td>6. 流程图</td>\n<td>步骤</td>\n<td></td>\n</tr>\n</tbody>\n</table>",r:{minutes:7.74,words:2321},t:"跟雅思考官 Simon 学写作",y:"a"}}],["/zh/posts/ielts/simon-task2.html",{loader:()=>a.e(4403).then(a.bind(a,49434)),meta:{e:'<p>跟雅思考官 Simon 学写作大作文(task2)讲义</p>\n<h1></h1>\n<h1>第一课 大作文简介</h1>\n<figure><img src="https://cdn-mineru.openxlab.org.cn/extract/74c105a2-a568-42d4-8dc3-52b145096ef9/3e0928ad01f2f18e01d7b07795d9b7a785ef59df7ec90032f228c12f7ca5fcd6.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',r:{minutes:29.03,words:8708},t:"",y:"a"}}],["/zh/posts/papers/3steps-paper-reading.html",{loader:()=>a.e(887).then(a.bind(a,35814)),meta:{e:'\n<figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img13@main/2025/01/15/1736921821494-d69f9ea8-0882-40c3-a3fa-70760d09dda5.png" alt="Teaser" tabindex="0" loading="lazy"><figcaption>Teaser</figcaption></figure>\n<h2>摘要</h2>\n<p>论文阅读是学术研究的核心技能。本文结合李沐与吴恩达的经验，详细总结了三步法读论文的技巧：快速筛选、全面了解、深入精读，帮助研究者高效筛选和深度理解文献内容，从而提升科研效率，掌握核心知识点。</p>',r:{minutes:3.43,words:1029},t:"【高效科研】李沐与吴恩达推荐的论文三步精读法",y:"a"}}],["/zh/posts/papers/alexnet.html",{loader:()=>a.e(9838).then(a.bind(a,99329)),meta:{e:'\n<figure><img src="https://faych.notion.site/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2F021ded55-a224-419c-939c-70c6888912f7%2F983b058d-48a2-4e90-8cf8-76d44a179591%2Fteaser.png?table=block&amp;id=1845f3c4-a139-8098-b059-d2ef2d3cd9f9&amp;spaceId=021ded55-a224-419c-939c-70c6888912f7&amp;width=1420&amp;userId=&amp;cache=v2" alt="teaser" tabindex="0" loading="lazy"><figcaption>teaser</figcaption></figure>',r:{minutes:6.25,words:1876},t:"【论文精读】AlexNet：ImageNet Classification with Deep Convolutional Neural Networks",y:"a"}}],["/zh/posts/papers/bagel.html",{loader:()=>a.e(5346).then(a.bind(a,87725)),meta:{e:'\n<figure><img src="https://paper-assets.alphaxiv.org/figures/2505.14683/img-0.jpeg" alt="BAGEL 的多模态能力展示" tabindex="0" loading="lazy"><figcaption>BAGEL 的多模态能力展示</figcaption></figure>\n<h2>摘要</h2>\n<p>BAGEL是由字节跳动Seed团队研发的统一多模态基础模型，拥有7B活跃参数。通过在万亿级别交错多模态数据上进行预训练，展现出复杂的推理能力。BAGEL采用Transformer专家混合（MoT）架构，在单个解码器模型中统一理解和生成能力。</p>',r:{minutes:9.25,words:2774},t:"【论文精读】BAGEL: 统一多模态预训练中的涌现属性 (Emerging Properties in Unified Multimodal Pretraining)",y:"a"}}],["/zh/posts/papers/blip-3o.html",{loader:()=>a.e(2029).then(a.bind(a,33728)),meta:{e:'\n<figure><img src="https://github.com/JiuhaiChen/BLIP3o/raw/main/figure/image.png" alt="BLIP3-o模型展示" tabindex="0" loading="lazy"><figcaption>BLIP3-o模型展示</figcaption></figure>\n<h2>摘要</h2>\n<p>Salesforce Research发布BLIP3-o，完全开源的统一多模态模型，系统探索自回归-扩散架构，在图像理解和生成任务上均取得领先表现。模型在VQAv2、MMBench等基准和人类评测中表现优异，推动多模态AI发展。</p>',r:{minutes:6.26,words:1877},t:"【论文精读】BLIP3-o：完全开源的统一多模态模型家族",y:"a"}}],["/zh/posts/papers/colorizediffusion.html",{loader:()=>a.e(2175).then(a.bind(a,25681)),meta:{e:'\n<figure><img src="https://arxiv.org/html/2401.01456v3/x1.png" alt="ColorizeDiffusion Teaser" tabindex="0" loading="lazy"><figcaption>ColorizeDiffusion Teaser</figcaption></figure>\n<h2>摘要</h2>\n<p>ColorizeDiffusion由东京工业大学研究团队提出，旨在解决草图上色中的"分布问题"——参考图像与草图结构的平衡困境。基于扩散模型，该方法通过三种创新训练策略和零样本文本调控，实现精确可控的上色效果，支持动漫/漫画风格创作。</p>',r:{minutes:8.33,words:2499},t:"【论文精读】ColorizeDiffusion：基于参考图像和文本的可调整草图上色方法",y:"a"}}],["/zh/posts/papers/comfyui-r1.html",{loader:()=>a.e(5225).then(a.bind(a,59761)),meta:{d:17496864e5,l:"2025年6月12日",c:["论文精读"],g:["ComfyUI","AIGC","LLM","Workflow Generation","Reasoning Model"],e:'\n<h2>摘要</h2>\n<p>ComfyUI-R1是一个创新的推理模型，能将自然语言指令转换为结构复杂的ComfyUI工作流。通过结合精心构建的知识库和两阶段训练流程（监督微调和GRPO强化学习），该模型在格式有效性、节点选择和端到端性能方面表现卓越。</p>\n<hr>\n<h2>目录</h2>\n<ol>\n<li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li>\n<li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li>\n<li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li>\n<li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li>\n<li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li>\n<li><a href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5">参考链接</a></li>\n</ol>',r:{minutes:7.98,words:2394},t:"【论文精读】ComfyUI-R1：探索用于工作流生成的推理模型",y:"a"}}],["/zh/posts/papers/dgpst.html",{loader:()=>a.e(5145).then(a.bind(a,30745)),meta:{e:'\n<table>\n<thead>\n<tr>\n<th style="text-align:center"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/contenttext.png" alt="" loading="lazy"></th>\n<th style="text-align:center"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/style0.png" alt="" loading="lazy"></th>\n<th style="text-align:center"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/style3.png" alt="" loading="lazy"></th>\n<th style="text-align:center"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/sketch.png" alt="" loading="lazy"></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style="text-align:center"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/contenttext.png" alt="" loading="lazy"></td>\n<td style="text-align:center"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/style5.png" alt="" loading="lazy"></td>\n<td style="text-align:center"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/style3.5.png" alt="" loading="lazy"></td>\n<td style="text-align:center"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/sketch.png" alt="" loading="lazy"></td>\n</tr>\n</tbody>\n</table>',r:{minutes:11.39,words:3417},t:"【论文精读】DGPST：精通百变风格的通用肖像画师",y:"a"}}],["/zh/posts/papers/ecomimic-v3.html",{loader:()=>a.e(5103).then(a.bind(a,48270)),meta:{e:'\n<figure><img src="https://arxiv.org/html/2507.03905v1/x1.png" alt="EchoMimicV3 可根据文本、音频和参考图生成多样化的人类动画" tabindex="0" loading="lazy"><figcaption>EchoMimicV3 可根据文本、音频和参考图生成多样化的人类动画</figcaption></figure>\n<h2>摘要</h2>\n<p>蚂蚁集团推出EchoMimicV3，一个1.3B参数的高效框架，统一了多模态、多任务人类动画。它通过统一生成范式、SFT+DPO训练等核心创新，解决了传统模型任务割裂、效率低下的痛点，以小模型实现了媲美SOTA的质量与效率。</p>',r:{minutes:8.57,words:2570},t:"【论文精读】EchoMimicV3：1.3B参数，统一多模态多任务人类动画",y:"a"}}],["/zh/posts/papers/flux-kontext.html",{loader:()=>a.e(5400).then(a.bind(a,60587)),meta:{e:'\n<figure><img src="https://arxiv.org/html/2506.15742v1/extracted/6548889/img/kontext_v2.jpg" alt="FLUX.1 Kontext技术架构概览" tabindex="0" loading="lazy"><figcaption>FLUX.1 Kontext技术架构概览</figcaption></figure>\n<h2>摘要</h2>\n<p>Black Forest Labs推出的FLUX.1 Kontext，以单一架构统一图像生成与编辑，实现3-5秒交互速度与94.1%角色一致性，有效抑制“视觉漂移”，并在KontextBench达SOTA性能。模型、论文详见文末参考链接。</p>',r:{minutes:12.34,words:3702},t:"【论文精读】FLUX.1 Kontext：统一图像生成与编辑的流匹配模型",y:"a"}}],["/zh/posts/papers/framepack.html",{loader:()=>a.e(6645).then(a.bind(a,48700)),meta:{a:"Lvmin Zhang, Maneesh Agrawala",d:17456256e5,l:"2025年4月26日",c:["视频生成","论文精读","张吕敏"],g:["FramePack","视频生成","扩散模型","输入预处理"],v:"/assets/images/papers/framepack/teaser.png",e:'\n<p align="center">\n        <img src="https://lllyasviel.github.io/frame_pack_gitpage/img/logo.png" width="200">\n</p>\n<h2>摘要</h2>\n<p>FramePack由斯坦福大学张吕敏等提出，是一种输入预处理模块，可无缝集成到主流视频扩散模型（如混元视频模型），通过自适应帧压缩和反漂移采样，有效提升长时序一致性和生成质量，支持13B模型在6GB显存上流畅生成长视频，显著降低算力门槛。</p>\n<hr>\n<h2>目录</h2>\n<ol>\n<li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li>\n<li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li>\n<li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li>\n<li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li>\n<li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li>\n</ol>',r:{minutes:6.22,words:1865},t:"【论文精读】FramePack：在下一帧预测视频生成模型中打包输入帧上下文",i:"material-symbols:screenshot-frame-2",y:"a"}}],["/zh/posts/papers/hunyuancustom.html",{loader:()=>a.e(6776).then(a.bind(a,69812)),meta:{e:'\n<h2>摘要</h2>\n<p>腾讯提出的HunyuanCustom框架解决视频定制化生成中的身份一致性问题，通过图像ID增强与文本-图像融合技术，实现多模态控制(文本、图像、音频、视频)下的身份保持，支持虚拟人物广告、试穿和视频编辑等多种应用场景。</p>\n<figure><img src="https://arxiv.org/html/2505.04512v1/x1.png" alt="混元定制化支持多种输入模态的视频生成，包括文本、图像、音频和视频条件" tabindex="0" loading="lazy"><figcaption>混元定制化支持多种输入模态的视频生成，包括文本、图像、音频和视频条件</figcaption></figure>',r:{minutes:7.38,words:2214},t:"【论文精读】HunyuanCustom：多模态驱动的定制视频生成架构",y:"a"}}],["/zh/posts/papers/icedit.html",{loader:()=>a.e(1001).then(a.bind(a,81450)),meta:{e:'\n<figure><img src="https://github.com/River-Zhang/ICEdit/raw/main/docs/images/teaser.png" alt="ICEdit多轮编辑示例" tabindex="0" loading="lazy"><figcaption>ICEdit多轮编辑示例</figcaption></figure>\n<h2>摘要</h2>\n<p>ICEdit 基于大规模 Diffusion Transformer，提出 in-context 编辑、LoRA-MoE 微调和 Early Filter 策略，实现高效高质的指令图像编辑。仅用极少数据和参数即超越 SOTA，具备强泛化与实际应用潜力。</p>',r:{minutes:5.99,words:1797},t:"【论文精读】ICEdit：In-Context Edit——大规模扩散Transformer的指令图像编辑新范式",y:"a"}}],["/zh/posts/papers/ming-omni.html",{loader:()=>a.e(2084).then(a.bind(a,53227)),meta:{e:'\n<h2>摘要</h2>\n<p>蚂蚁集团推出的<strong>Ming-Omni</strong>是首个开源统一多模态模型，支持文本、图像、视频和音频的感知与生成。创新MoE架构配合模态特定路由器提升跨模态协同。轻量版仅2.8B参数却达到SOTA性能，在模态覆盖上媲美GPT-4o，推动多模态AI普及。</p>\n<hr>\n<h2>目录</h2>\n<ol>\n<li>背景与研究目标</li>\n<li>方法与创新点</li>\n<li>实验与结果分析</li>\n<li>模型启发与方法延伸</li>\n<li>结论与未来展望</li>\n</ol>\n<hr>\n<h2>背景与研究目标</h2>\n<figure><img src="https://arxiv.org/html/2506.09344v1/x1.png" alt="Ming-Omni多模态能力展示图" tabindex="0" loading="lazy"><figcaption>Ming-Omni多模态能力展示图</figcaption></figure>',r:{minutes:8.3,words:2489},t:"【论文精读】Ming-Omni: 统一的多模态感知与生成模型",y:"a"}}],["/zh/posts/papers/nhr.html",{loader:()=>a.e(8697).then(a.bind(a,17995)),meta:{e:'\n<figure><img src="https://paper-assets.alphaxiv.org/figures/2507.14119v1/img-1.jpeg" alt="高质量样本" tabindex="0" loading="lazy"><figcaption>高质量样本</figcaption></figure>\n<h2>摘要</h2>\n<p>NHR提出了全自动高质量图像编辑三元组挖掘管道，结合Gemini验证器和反转、组合增强，构建了35.8万三元组的NHR-Edit数据集。该方法提升了训练效率，为图像编辑模型提供了高保真数据支持。</p>\n<hr>\n<h2>目录</h2>\n',r:{minutes:5.47,words:1641},t:"【论文精读】NoHumansRequired：高质量图像编辑的自主三元组挖掘",y:"a"}}],["/zh/posts/papers/omniconsistency.html",{loader:()=>a.e(8398).then(a.bind(a,96473)),meta:{e:'\n<p><img src="https://arxiv.org/html/2505.18445v1/x1.png" alt="OmniConsistency Teaser" loading="lazy">\nOmniConsistency 在不同场景和未见过的风格 LoRA 下均能实现风格一致且保留结构的图像风格化，其性能优于现有基线方法，且无风格退化现象。</p>\n<h2>摘要</h2>\n<p>OmniConsistency 是基于扩散Transformer的图像风格化框架，通过两阶段训练和滚动LoRA库从成对数据中学习与风格无关的一致性模式，有效解决传统方法在保持内容结构和语义信息方面的挑战。</p>',r:{minutes:9.84,words:2951},t:"【论文精读】OmniConsistency：从成对风格化数据中学习与风格无关的一致性",y:"a"}}],["/zh/posts/papers/omnigen2.html",{loader:()=>a.e(7952).then(a.bind(a,31010)),meta:{e:'\n<figure><img src="https://vectorspacelab.github.io/OmniGen2/static/img/omnigen/omnigen2_overview_new.jpg" alt="OmniGen2 架构图" tabindex="0" loading="lazy"><figcaption>OmniGen2 架构图</figcaption></figure>\n<h2>摘要</h2>\n<p>OmniGen2 是 BAAI 推出的多模态开源模型，统一文本到图像、图像编辑和情境生成。其解耦架构与视频数据管道显著提升理解与生成能力，在多项主流基准上表现领先，并开源全部资源，推动多模态发展。</p>',r:{minutes:6.18,words:1855},t:"【论文精读】OmniGen2：探索先进多模态生成（OmniGen2: Advancing Unified Multimodal Generation）",y:"a"}}],["/zh/posts/papers/ovis-u1.html",{loader:()=>a.e(2607).then(a.bind(a,88731)),meta:{d:17515872e5,l:"2025年7月4日",c:["论文精读"],g:["多模态","Ovis-U1","阿里巴巴","AIGC"],e:'\n<h2>摘要</h2>\n<p>阿里巴巴 Ovis-U1 是一个仅3B参数的统一多模态模型，通过创新的六阶段训练，集成了理解、生成与编辑能力。该模型在 OpenCompass 基准上超越同级，生成和编辑能力媲美更大模型，展现了紧凑模型实现通用多模态能力的潜力。</p>\n<hr>\n<h2>目录</h2>\n<ol>\n<li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li>\n<li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li>\n<li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li>\n<li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li>\n<li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li>\n</ol>',r:{minutes:6.14,words:1842},t:"【论文精读】Ovis-U1：统一多模态理解、生成与编辑的3B模型",y:"a"}}],["/zh/posts/papers/qr-lora.html",{loader:()=>a.e(4867).then(a.bind(a,83274)),meta:{e:'\n<figure><img src="https://arxiv.org/html/2507.04599v1/x1.png" alt="QR-LoRA 方法示意图，通过正交分解实现内容与风格特征的高效解耦控制。" tabindex="0" loading="lazy"><figcaption>QR-LoRA 方法示意图，通过正交分解实现内容与风格特征的高效解耦控制。</figcaption></figure>\n<h2>摘要</h2>\n<p>QR-LoRA 基于 QR 分解结构化参数更新，实现内容与风格正交分离，参数量减半，提升多 LoRA 融合的独立性与生成质量，适用于多种扩散模型如SDXL、SD3和FLUX。论文与代码见文末参考链接。</p>',r:{minutes:8.1,words:2430},t:"【论文精读】QR-LoRA：基于QR分解的高效解耦微调",y:"a"}}],["/zh/posts/papers/reptext.html",{loader:()=>a.e(121).then(a.bind(a,32584)),meta:{e:'\n<figure><img src="https://github.com/Shakker-Labs/RepText/raw/main/assets/teaser.png" alt="RepText Teaser" tabindex="0" loading="lazy"><figcaption>RepText Teaser</figcaption></figure>\n<h2>摘要</h2>\n<p>RepText 由 Shakker Labs 提出，通过“复制”视觉元素（glyph latent）实现多语言高质量文本渲染，无需语义理解。基于 ControlNet，支持灵活控制字体、颜色和位置，兼容性强，资源消耗低，效果接近闭源大模型。</p>',r:{minutes:6.36,words:1909},t:"【论文精读】RepText：通过复制实现视觉文本渲染",y:"a"}}],["/zh/posts/papers/resnet.html",{loader:()=>a.e(4960).then(a.bind(a,53716)),meta:{e:'\n<h2>摘要</h2>\n<p>ResNet 通过残差学习成功解决超深网络的训练难题，克服梯度消失与退化问题。在 ImageNet 分类任务中以 3.57% Top-5 错误率刷新纪录，并在目标检测、分割等任务中展现卓越性能。本文解析其核心设计、实验验证及影响。</p>\n<hr>\n<h2>目录</h2>\n<ol>\n<li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li>\n<li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li>\n<li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li>\n<li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li>\n<li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li>\n</ol>',r:{minutes:6.51,words:1953},t:"【论文精读】ResNet：Deep Residual Learning for Image Recognition",y:"a"}}],["/zh/posts/papers/sdo.html",{loader:()=>a.e(9061).then(a.bind(a,98152)),meta:{a:"Hongkun Dou, Zeyu Li, Xingyu Jiang, Hongjue Li, Lijun Yang, Wen Yao, and Yue Deng",d:17151264e5,l:"2024年5月8日",c:["图像生成","论文精读"],g:["SDO","扩散模型","反向传播","计算优化","可控生成"],v:"https://arxiv.org/html/2505.07477v1/x1.png",e:'\n<figure><img src="https://arxiv.org/html/2505.07477v1/x1.png" alt="SDO通过扩散采样中的反向传播实现多样化应用" tabindex="0" loading="lazy"><figcaption>SDO通过扩散采样中的反向传播实现多样化应用</figcaption></figure>\n<h2>摘要</h2>\n<p>扩散模型 (DMs) 在下游任务中反向传播计算成本高昂。本文提出捷径扩散优化 (SDO)，通过仅保留一步计算图优化目标函数，显著降低约90%计算成本，同时保持或超越完整反向传播性能。SDO适用于潜变量优化、模型微调等任务，兼具通用性、高性能和轻量级特点。</p>',r:{minutes:6.77,words:2030},t:"【论文精读】SDO: 用梯度捷径加速扩散采样中的反向传播",i:"material-symbols:image",y:"a"}}],["/zh/posts/papers/show-o2.html",{loader:()=>a.e(8464).then(a.bind(a,44610)),meta:{e:'\n<figure><img src="https://github.com/showlab/Show-o/raw/main/show-o2/docs/showo2.png" alt="Show-o2 Logo" tabindex="0" loading="lazy"><figcaption>Show-o2 Logo</figcaption></figure>\n<h2>摘要</h2>\n<p>Show-o2是新加坡国立大学Show Lab与字节跳动联合开发的统一多模态模型。首次实现文本、图像、视频的原生统一处理，通过3D因果VAE和双路径融合机制，在单一框架内同时掌握理解与生成能力。该模型在多项基准测试中达到SOTA性能。</p>',r:{minutes:7.22,words:2165},t:"【论文精读】Show-o2: 改进的原生统一多模态模型",y:"a"}}],["/zh/posts/papers/transformer.html",{loader:()=>a.e(5822).then(a.bind(a,53637)),meta:{e:'\n<h2>摘要</h2>\n<p>Transformer 由 Vaswani 等人于 2017 年提出，首次完全基于自注意力机制（Self-Attention）实现序列建模，摒弃了 RNN/CNN 结构。该模型极大提升了并行效率和长距离依赖建模能力，成为 NLP、CV 等多模态领域的基础架构。论文提出的多头注意力、位置编码等机制，推动了大模型和生成式 AI 的快速发展。</p>\n<hr>\n<h2>目录</h2>\n<ol>\n<li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li>\n<li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li>\n<li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li>\n<li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li>\n<li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li>\n</ol>',r:{minutes:6.29,words:1887},t:"【论文精读】Transformer：Attention Is All You Need",y:"a"}}],["/zh/posts/papers/vlv.html",{loader:()=>a.e(465).then(a.bind(a,88449)),meta:{e:'\n<figure><img src="https://arxiv.org/html/2507.07104v1/x2.png" alt="VLV pipeline overview showing the two-stage training framework" tabindex="0" loading="lazy"><figcaption>VLV pipeline overview showing the two-stage training framework</figcaption></figure>\n<h2>摘要</h2>\n<p>约翰霍普金斯大学联合清华大学等机构提出VLV自编码器，通过冻结T2I扩散模型解码器创建信息瓶颈，以低于1000美元成本实现与GPT-4o相当的图像描述性能。该方法主要使用单模态图像训练，显著降低配对数据需求。</p>',r:{minutes:5.93,words:1779},t:"【论文精读】VLV：视觉-语言-视觉自编码器的可扩展知识蒸馏",y:"a"}}],["/zh/posts/prompts/ai-weekly.prompt.html",{loader:()=>a.e(7735).then(a.bind(a,27919)),meta:{e:"<p>请根据ai-weekly的模板，结合以往优秀AI周报的写作风格，对本期AI周报进行结构和表达的检查与优化：</p>\n<p>主标题需为一级标题（#），格式为“关键词1 | 关键词2 | 关键词3【AI周报】”，关键词间用“|”分隔，结尾为“【AI周报】”，总字数不超过64字。\n摘要部分应为一段话，简明总结本期主要内容，结尾注明“相关参考链接请见文末”，字数在100-120字之间。\n目录需完整、带锚点。每个条目需包含二级标题、配图（可选，配图需包含alt属性，格式为“方法名（项目名）+ 图类型（英文） + 图”。且之间用空格隔开）、概要、标签，结构统一。\n参考链接需集中在文末，格式规范。整体要求语言简洁、重点突出、结构清晰。</p>",r:{minutes:.8,words:240},t:"",y:"a"}}],["/zh/posts/prompts/cover.prompt.html",{loader:()=>a.e(3056).then(a.bind(a,43014)),meta:{e:"<p>请根据这篇博客，提取文中至少3个关键词，并加上标题的主题词，博客的分类（比如论文精读）作为图像里必须包含的关键词。\n需要根据博客的类别进行分类，比如如果是论文精读，就要包含论文精读这几个字，其他同理。\n关键词的字号至少有24px，标题字号至少有36px，分类字号至少有36px。文字部分位于图的顶部，分类和标题之间用冒号相隔，且放在中间，其余关键词（只有三个，不能带有开源等词）分布在标题的下方一行，但不低于图顶1/3的区域。\n然后根据博客的内容总结出风格，以及图像里内容的布局，主体主角是一个淡紫色头发的猫耳日漫女生，她的着装可以根据博客内容进行调整。\n最后整理成一段英文的描述，可以让GPT-Image生成一张符合要求的1:1比例的封面图。</p>",r:{minutes:.91,words:272},t:"",y:"a"}}],["/zh/posts/prompts/hf-weekly.prompt.html",{loader:()=>a.e(1667).then(a.bind(a,6388)),meta:{e:"<p>请根据我给出的链接来撰写本期HF周报的内容，按照如下要求撰写：</p>\n<ul>\n<li>主标题需遵循“【模型/事件A简述】|【模型/事件B简述】|【模型/事件C简述】【HF周报】”的格式，提炼3个核心亮点，每个简述都应具吸引力且概括核心优势，总字数控制在64字以内。</li>\n<li>摘要100-120字，突出本期亮点，首句“本周亮点：”，末句“详见正文，相关参考链接请见文末。”</li>\n<li>每个条目的标题格式为“【模型/工具全称】：【一句话亮点概括】”。</li>\n<li>每个条目的“概要”部分，应以“[公司/组织]发布[模型/工具]，……”的句式开头，自然流畅地介绍其核心特性、应用场景和重要意义，避免生硬的列表式描述。</li>\n<li>每个条目需包含：模型/工具/事件全称、权威配图、简明概要、5个唯一且具代表性的标签、权威链接。</li>\n<li>标签需突出独特性，不与其他条目重复。</li>\n<li>目录与正文条目顺序、标题一致。</li>\n<li>全文结构、格式、字数、逻辑流畅。</li>\n<li>封面图prompt不输出。</li>\n</ul>",r:{minutes:1.01,words:304},t:"",y:"a"}}],["/zh/posts/prompts/image-extract.prompt.html",{loader:()=>a.e(1794).then(a.bind(a,45375)),meta:{e:"<p>提取该文档下的所有图片的链接到同目录下的image.md中，每个图片都要有相应的说明（保持原有的图注为说明），组成一个Markdown表格形式。</p>\n",r:{minutes:.18,words:53},t:"",y:"a"}}],["/zh/posts/prompts/papers.prompt.html",{loader:()=>a.e(9152).then(a.bind(a,43624)),meta:{e:"<p>请根据papers的模板（template），撰写流程（workflow），检查表（checklist）以及这篇文章的的所有参考材料（blog、paper和readme）并主要参考Alphaxiv的blog，以及参考papers文件夹中的优秀论文精读博文，写这篇文章的论文精读。\n同时要注意模板一定要满足规范，并且不擅自在对格式进行修改。\n需要注意的是，图片一定要从论文中获取链接，并且根据论文中的图注对作为图的alt文本进行描述。\n更要注意的是标题不能超过64字，摘要不能超过120字，正文需要集中在方法和实验部分，下面是论文的资料，请开始撰写：</p>\n",r:{minutes:.63,words:188},t:"",y:"a"}}],["/zh/posts/prompts/translate.prompt.html",{loader:()=>a.e(6883).then(a.bind(a,53849)),meta:{e:'<p>你是一位精通简体中文的专业翻译，曾参与《纽约时报》和《经济学人》中文版的翻译工作，因此对于新闻和时事文章的翻译有深入的理解。我希望你能帮我将以下英文新闻段落翻译成中文，风格与上述杂志的中文版相似。</p>\n<p>规则：</p>\n<ul>\n<li>翻译时要准确传达新闻事实和背景。</li>\n<li>保留特定的英文术语或名字，并在其前后加上空格，例如："中 UN 文"。</li>\n<li>分成两次翻译，并且打印每一次结果：</li>\n</ul>\n<ol>\n<li>根据新闻内容直译，不要遗漏任何信息</li>\n<li>根据第一次直译的结果重新意译，遵守原意的前提下让内容更通俗易懂，符合中文表达习惯</li>\n</ol>',r:{minutes:.83,words:249},t:"",y:"a"}}],["/zh/posts/repos/comfy-mind.html",{loader:()=>a.e(6152).then(a.bind(a,64495)),meta:{d:17492544e5,l:"2025年6月7日",c:["创造者工坊"],g:["技术教程","AI生成","ComfyMind","开源项目"],v:"https://neverbiasu.github.io/assets/images/repos/comfy-mind/comfymind-cover.jpg",e:'\n<figure><img src="https://arxiv.org/html/2505.17908v1/x3.png" alt="ComfyMind项目架构图" tabindex="0" loading="lazy"><figcaption>ComfyMind项目架构图</figcaption></figure>\n<h2>摘要</h2>\n<p>ComfyMind是一个基于ComfyUI平台的协作式AI系统，通过树状规划与反馈机制实现通用型AI图像、视频生成功能。本教程将带你从零开始搭建并运行ComfyMind，体验其在图像生成、编辑和推理任务上的强大能力。</p>\n<hr>\n<h2>目录</h2>',r:{minutes:4.84,words:1453},t:"创造者工坊：ComfyMind跑通教程",y:"a"}}],["/zh/posts/reprints/ai-art-gtc-paris-2025.html",{loader:()=>a.e(8635).then(a.bind(a,97881)),meta:{a:"Heather Schoell",d:1749189624e3,l:"2025年6月6日",c:["生成式AI"],g:["艺术","人工智能","创作者","GTC"],e:'\n<p>本次会议在欧洲标志性艺术之都之一举行，将展出一个精心策划的画廊，展示AI如何帮助将创意愿景变为现实。</p>\n<h2>创作中的诗意伙伴</h2>\n<p>当Paul Mouginot首次将AI引入他的艺术实践时，他知道自己已经掌握了一个令人兴奋的新创意表达工具。但只有通过持续使用，他才开始欣赏AI作为内省实体的多功能性——他称之为"创作行为中的诗意伙伴"。</p>\n<p>"当AI在深度个人化数据上训练时，它就不再只是一个工具，"自2019年以来一直以艺术实体aurèce vettier身份工作的Mouginot说道。"它变成了一个用于诗意思辨的反思装置。"</p>\n<p>这位法国艺术家是在<a href="https://www.nvidia.com/en-eu/gtc/" target="_blank" rel="noopener noreferrer">NVIDIA GTC巴黎</a>展出AI创作艺术品的七位参展者之一，会议将于6月10-12日在VivaTech举行。</p>',r:{minutes:7.47,words:2241},t:"艺术家和时装设计师利用最先进的AI技术为NVIDIA GTC巴黎画廊创作",y:"a"}}],["/zh/posts/reprints/ai-art-newsletter-jan-25.html",{loader:()=>a.e(9803).then(a.bind(a,47881)),meta:{e:'\n<h3>创刊号 🎉</h3>\n<p>AI 领域的发展速度令人惊叹，回想一年前我们还在为生成正确手指数量的人像而苦苦挣扎的场景，恍如隔世 😂。</p>\n<p>过去两年对开源模型和艺术创作工具而言具有里程碑意义。创意表达的 AI 工具从未像现在这般触手可及，然而这仅仅是冰山一角。让我们共同回顾 2024 年 AI 艺术领域的关键突破与创新工具，并展望 2025 年的发展趋势。</p>\n<h2>目录</h2>\n<ul>\n<li><a href="#2024-%E9%87%8D%E5%A4%A7%E5%8F%91%E5%B8%83">2024 重大发布</a></li>\n<li><a href="#%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90">图像生成</a>\n<ul>\n<li><a href="#%E6%96%87%E7%94%9F%E5%9B%BE">文生图</a></li>\n<li><a href="#%E4%B8%AA%E6%80%A7%E5%8C%96%E4%B8%8E%E9%A3%8E%E6%A0%BC%E5%8C%96">个性化与风格化</a></li>\n</ul>\n</li>\n<li><a href="#%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90">视频生成</a></li>\n<li><a href="#2024-%E9%97%AA%E8%80%80%E5%88%9B%E6%84%8F%E5%B7%A5%E5%85%B7">2024 闪耀创意工具</a></li>\n<li><a href="#2025-%E5%B9%B4-AI-%E8%89%BA%E6%9C%AF%E8%B6%8B%E5%8A%BF%E5%B1%95%E6%9C%9B">2025 年 AI 艺术趋势展望</a></li>\n<li><a href="#%E5%BC%BA%E5%8A%BF%E5%BC%80%E5%B1%80-2025-%E5%B9%B4-1-%E6%9C%88%E5%BC%80%E6%BA%90%E6%96%B0%E4%BD%9C">强势开局: 2025 年 1 月开源新作</a></li>\n</ul>',r:{minutes:8.03,words:2408},t:"AI艺术工具通讯 - 第1期",y:"a"}}],["/zh/posts/reprints/announcing-illustrious-text%E2%80%91enhancer-tag-booster-and-mood-enhancer.html",{loader:()=>a.e(5731).then(a.bind(a,86269)),meta:{a:"LivBigStar",d:17479584e5,l:"2025年5月23日",v:"https://neverbiasu.github.io/assets/images/reprints/illustrious/tag-enhancer/cover.jpg",e:"Illustrious 推出 Text‑Enhancer，包含 Tag Booster 和 Mood Enhancer。前者基于 TIPO 框架扩展稀疏 prompt，后者通过 LLM 生成氛围描述，显著提升图像质量。",r:{minutes:13.34,words:4002},t:"发布 Illustrious Text‑Enhancer：Tag Booster 和 Mood Enhancer",y:"a"}}],["/zh/posts/reprints/crody's-model-merge-guide.html",{loader:()=>a.e(312).then(a.bind(a,44416)),meta:{a:"Crody",d:17419104e5,l:"2025年3月14日",g:["resource guide","script","stable diffusion","merge","model"],v:"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/2d3f67d3-4906-46b0-b27c-63ba7376cca7/width=1320/00000_2500974118.jpeg",e:"<p>Hi this is Crody from Team-C: creator of Nova Series</p>\n<p>In this article, I'll write down what kind of merge I use with some knowledge about SDXL models\nFor how I do, please read Merge Scripter Guide first</p>\n<h2>1. Weighted Sum / Sum Twice</h2>\n<p>Weighted Sum (WS) merges 2 models, Sum Twice (ST) merges first 2 and 1 model (which means doing WS twice)\nYou can use Block Merge as well\nUsing alpha (and beta) to determine how much similarity the result have\nHigher value means the results would be similar to latter model</p>",r:{minutes:5.58,words:1674},t:"Crody's Model Merge Guide // Team-C",y:"a"}}],["/zh/posts/reprints/experiments-with-mcp-using-github-copilot.html",{loader:()=>a.e(8275).then(a.bind(a,43865)),meta:{a:"kcon",d:1742256e6,l:"2025年3月18日",g:["编辑器","模型上下文协议","技术"],e:'\n<h2>概述</h2>\n<p>本文描述了如何在 VS Code 中安装 "Copilot MCP" 扩展，并结合 MCP 使用 GitHub Copilot 从 GitHub 获取信息进行测试。<br>\n注意：由于官方 GitHub Copilot 实现似乎也支持 MCP，一旦该功能发布，此扩展可能将不再必要。</p>\n<h2>测试环境</h2>\n<ul>\n<li>Windows 10 Pro</li>\n<li>VS Code</li>\n</ul>\n<h2>准备工作</h2>\n<p>请提前确保以下事项：</p>\n<ul>\n<li>安装 node.js\n<ul>\n<li>必须可以使用 <code>npm</code> 和 <code>npx</code> 命令</li>\n</ul>\n</li>\n<li>创建 GitHub 账户</li>\n</ul>',r:{minutes:2.24,words:672},t:"使用 GitHub Copilot 进行 MCP 实验",y:"a"}}],["/zh/posts/reprints/explaining-tokens-the-language-and-currency-of-ai-nvidia-blog.html",{loader:()=>a.e(2592).then(a.bind(a,37812)),meta:{a:"Dave Salvator",d:17421696e5,l:"2025年3月17日",c:["Explainer","Generative AI"],g:["Artificial Intelligence","Inference"],v:"https://blogs.nvidia.com/wp-content/uploads/2025/03/llm-blog-data-curator-2847806-1280x680-1.png",e:'\n<figure><img src="https://blogs.nvidia.com/wp-content/uploads/2025/03/llm-blog-data-curator-2847806-1280x680-1.png" alt="图1：大型语言模型处理数据的描述" tabindex="0" loading="lazy"><figcaption>图1：大型语言模型处理数据的描述</figcaption></figure>\n<p>在每个AI应用程序的底层，都有算法在以自己的语言处理数据，这种语言基于token词汇。</p>\n<p>token是通过分解更大信息块而来的微小数据单元。AI模型处理token以学习它们之间的关系，并解锁包括预测、生成和推理在内的能力。token处理得越快，模型学习和响应的速度就越快。</p>',r:{minutes:8.16,words:2449},t:"解释token— AI的语言和货币 | NVIDIA博客",y:"a"}}],["/zh/posts/reprints/flux-1-kontext.html",{loader:()=>a.e(4524).then(a.bind(a,59712)),meta:{a:"Black Forest Labs",d:17484768e5,l:"2025年5月29日",c:"新闻",g:["人工智能","图像生成","FLUX","机器学习"],e:'\n<p>今日，我们荣幸地发布  FLUX.1 Kontext ，这是一套创新的生成式流匹配模型，能够帮助用户生成和编辑图像。与现有的文本转图像模型不同， FLUX.1 Kontext  系列模型能够进行所谓  <strong><em>in-context</em></strong>  的图像生成，允许用户同时使用文本和图像作为输入提示，并能无缝提取和修改视觉概念，从而创造出全新的、协调一致的图像作品。</p>\n<figure><img src="https://cdn.sanity.io/images/gsvmb6gz/production/52e38891df903fdee79f6b4ed0fb63f00a43e376-2211x1174.png?rect=0,113,2211,949&amp;w=960&amp;h=412&amp;fit=max&amp;auto=format" alt="Kontext 效果网格图" tabindex="0" loading="lazy"><figcaption>Kontext 效果网格图</figcaption></figure>',r:{minutes:8.12,words:2436},t:"隆重推出 FLUX.1 Kontext 与 BFL Playground",y:"a"}}],["/zh/posts/reprints/flux-kontext-optimization.html",{loader:()=>a.e(6482).then(a.bind(a,54662)),meta:{d:17525376e5,l:"2025年7月15日",g:["FLUX.1","优化","TaylorSeer","Replicate"],e:'\n<figure><img src="https://replicate.com/_content/assets/top-graphic.CLh5lXp7_Z2h1V1F.webp" alt="FLUX.1 Kontext 优化图" tabindex="0" loading="lazy"><figcaption>FLUX.1 Kontext 优化图</figcaption></figure>\n<p>除了让我们的 FLUX.1 Kontext [dev] 实现开源之外，我们还希望提供更多关于如何在不降低质量的前提下优化它的指导。</p>\n<p>在这篇文章中，您将主要了解 TaylorSeer 优化，这是一种通过使用缓存的图像变化（导数）和从 Taylor 级数近似推导出的公式来近似中间图像预测的方法。</p>',r:{minutes:5.93,words:1780},t:"我们如何优化 FLUX.1 Kontext [dev]",y:"a"}}],["/zh/posts/reprints/flux-qlora.html",{loader:()=>a.e(2826).then(a.bind(a,16184)),meta:{a:"Derek Liu, Marc Sun, Sayak Paul, merve, Linoy Tsaban",d:17187552e5,l:"2024年6月19日",c:"AI/ML",g:["FLUX","LoRA","QLoRA","微调","diffusers","量化"],e:'\n<div class="hint-container tip">\n<p class="hint-container-title">提示</p>\n<figure><a href="https://colab.research.google.com/github/DerekLiu35/notebooks/blob/main/flux_lora_quant_blogpost.ipynb" target="_blank" rel="noopener noreferrer"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" tabindex="0" loading="lazy"></a><figcaption>Open In Colab</figcaption></figure>\n</div>',r:{minutes:13.93,words:4179},t:"在消费级硬件上对 FLUX.1-dev 进行（LoRA）微调",y:"a"}}],["/zh/posts/reprints/generative-ai-powered-design.html",{loader:()=>a.e(16).then(a.bind(a,16636)),meta:{a:"Isha Dua & Parth Patel",d:17423424e5,l:"2025年3月19日",c:["转载"],g:["生成式AI","游戏开发","Stable Diffusion","图像生成","AWS","Amazon Bedrock"],u:!1,v:"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/19/Picture1-11.jpg",e:"<p>在竞争激烈的游戏开发世界中，紧跟技术进步至关重要。生成式AI已经成为游戏规则的改变者，为游戏设计师提供了前所未有的机会，使他们能够突破界限并创造身临其境的虚拟世界。在这场革命的前沿是Stability AI的尖端文本到图像AI模型——Stable Diffusion 3.5 Large (SD3.5 Large)，它正在彻底改变我们创建游戏环境的方式。</p>\n<p>SD3.5 Large可在Amazon Bedrock上使用，是Stability AI迄今为止最先进的文本到图像模型。拥有81亿参数，该模型擅长从文本描述生成高质量的百万像素图像，具有卓越的提示符合性，使其成为快速创建详细游戏环境的理想选择。其改进的架构基于多模态扩散变换器(MMDiT)，结合多个预训练文本编码器以增强文本理解能力，并使用QK归一化来提高训练稳定性。</p>",r:{minutes:7.64,words:2291},t:"生成式AI驱动的设计：使用SD3.5 Large创建游戏环境",i:"openmoji:video-game",O:1,y:"a"}}],["/zh/posts/reprints/how-and-when-to-build-multi-agent-systems.html",{loader:()=>a.e(6976).then(a.bind(a,50646)),meta:{d:175008553e4,l:"2025年6月16日",e:'\n<figure><img src="https://blog.langchain.com/content/images/size/w760/format/webp/2025/06/supervisor.png" alt="如何以及何时构建多智能体系统" tabindex="0" loading="lazy"><figcaption>如何以及何时构建多智能体系统</figcaption></figure>\n<p>上周末，两篇看似标题相反的精彩博文相继发布。一篇是 Cognition 团队的《不要构建多智能体》，另一篇是 Anthropic 团队的《我们如何构建多智能体研究系统》。</p>\n<p>尽管标题相反，我认为它们实际上有很多共同点，并提供了关于如何以及何时构建多智能体系统的一些见解：</p>',r:{minutes:10.3,words:3089},t:"如何以及何时构建多智能体系统",y:"a"}}],["/zh/posts/reprints/illustrious-lu-v0.03.html",{loader:()=>a.e(8808).then(a.bind(a,10540)),meta:{a:"Angelbottomless",d:17449344e5,l:"2025年4月18日",c:["模型开发","转载"],g:["Illustrious","LU","Lumina","AI模型","图像生成","训练"],v:"https://illustrious-prod.s3.ap-northeast-2.amazonaws.com/blog/2025-04-11T07:16:56.712Z/2025-04-11%20Thumbnail.png",e:'\n<p>SD XL 一直受到 CLIP 的困扰--我认为至少这部分是事实。最近的模型在自然语言方面显示出一些潜力，比如理解"左边是红色，右边是蓝色"。然而，由于CLIP没有使用自然语言句子进行训练，基础SD XL及其微调变体在处理自然语言方面受到了显著限制。</p>\n<p>Flux和SD3等DiT模型与T5结合表现出更好的能力。特别是，已经证明T5在处理自然语言信息以正确生成文本或组合方面非常重要。然而，T5 非常大而且仍然有限，因此有人尝试直接使用 LLM 作为文本编码器。此外，DiT模型也非常庞大。即使没有T5，12B参数的模型也不太实用，这一点与SD XL非常相似。</p>\n<p>流匹配（Flow matching）也很有趣。然而，DiT 的直观结构似乎不可避免地促成了许多有用的研究。不幸的是，与SD XL结合的流匹配并未显示支持这一点的证据；相反，它引发了更多关于SD XL的VAE问题的疑问。</p>',r:{minutes:4.08,words:1225},t:"Illustrious-LU v0.03",i:"fa-solid:microscope",y:"a"}}],["/zh/posts/reprints/illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.html",{loader:()=>a.e(4829).then(a.bind(a,71118)),meta:{a:"Angelbottomless",d:17426016e5,l:"2025年3月22日",c:["Model Development","Model Training","Image Generation","Anime Style"],g:["SDXL","2048分辨率","Illustrious","vpred","epsilon预测"],v:"/assets/images/reprints/illustrious/v3.0-3.5/thumbnail.webp",e:'\n<p>Illustrious XL 3.0-3.5-vpred 标志着 Stable Diffusion XL（SD XL）模型的一项重大进展，显著支持从 256 到 2048 分辨率的无缝扩展。特别是 v3.5-vpred 变体，在自然语言理解能力上达到了类似于迷你大型语言模型（LLMs）的精细程度，这是通过对 CLIP 与 UNet 组件的广泛同时训练实现的。</p>\n<h2>训练目标与概述：eps 与 vpred</h2>\n<p>Illustrious v3.0-v3.5 系列设计了两种不同的训练目标以探索行为差异：</p>\n<ul>\n<li>\n<p><strong>V3.0-epsilon</strong> 使用 epsilon 预测（噪声预测），确立了作为未来训练任务（尤其是与 LoRA 训练兼容）稳定"基底"模型的地位。该模型在默认状态下输出的风格较 vpred 变体更具特色，在某些美学评分中有时表现最佳。</p>\n</li>\n<li>\n<p><strong>V3.0-vpred</strong> 则采用 velocity 预测（v 参数化），展示出更强的组合理解能力，但最初伴随着严重问题，包括灾难性遗忘、领域偏移、颜色过饱和以及因零终端 SNR（Zero Terminal SNR）实现失误而导致的色板崩溃。</p>\n</li>\n<li>\n<p><strong>V3.5-vpred</strong> 则在实验性设置下训练，试图缓解上述问题。该模型显示出颜色更稳定，但并不天然生成鲜艳色彩，其功能已转移至特定的控制令牌（controlling tokens）。</p>\n</li>\n</ul>',r:{minutes:13.4,words:4019},t:"Illustrious XL 3.0-3.5-vpred: 2048分辨率与自然语言",i:"mdi:paint-outline",y:"a"}}],["/zh/posts/reprints/illustrious-xl-v2.0-the-best-training-base-model-in-1536-age.html",{loader:()=>a.e(5404).then(a.bind(a,84602)),meta:{a:"Angelbottomless",d:17419968e5,l:"2025年3月15日",c:["模型研发","模型训练","图像生成","动漫风格"],g:["SDXL","动漫","Illustrious","基础模型","图像生成"],v:"/assets/images/reprints/illustrious/v2.0-2.0a/thumbnail.webp",e:'\n<h2>简介</h2>\n<p>Illustrious XL 1.0-2.0系列旨在稳定1536分辨率的原生生成，同时显著提高自然语言理解能力。</p>\n<p>虽然用户有时会观察到在1024x1536分辨率下能成功生成，但这些并不稳定。同样，512x512分辨率的生成偶尔也会产生不必要的伪影。</p>\n<h2>早期版本为何不稳定？</h2>\n<p>这些不一致的根本原因很简单：模型未在这些分辨率上进行有效泛化或训练。使用小数据集填补这些空白往往会导致在某些分辨率上过拟合。这意味着模型会将特定分辨率与特定概念关联起来，使其在多样化生成时变得不可靠。</p>\n<p>一个有用的比喻是"广角效果"。如果数据集通常包含广角镜头，当给定广角分辨率时，模型自然会生成更小的人物，因为这是它学习泛化的方式。</p>',r:{minutes:5.3,words:1589},t:"Illustrious XL v2.0：1536分辨率时代最佳的训练基础模型",i:"mdi:paint-outline",y:"a"}}],["/zh/posts/reprints/image-recognition.html",{loader:()=>a.e(3820).then(a.bind(a,83591)),meta:{a:"Timothy M.",d:1749577901e3,l:"2025年6月10日",g:["计算机视觉","图像识别"],e:'\n<figure><img src="https://blog.roboflow.com/content/images/size/w1200/2025/06/Screenshot-2025-06-10-at-10.46.38---AM.png" alt="什么是图像识别" tabindex="0" loading="lazy"><figcaption>什么是图像识别</figcaption></figure>\n<p>想象一下，一个名叫 Emma 的小女孩对鸟类着迷。每个周末，她都会和爷爷一起去附近的公园观鸟。久而久之，Emma 学会了通过颜色、大小、形状甚至叫声来识别不同的鸟类。一天下午，在翻书时，她毫不费力地指着一张图片说：“看，爷爷！是知更鸟！”她没有测量翼展或分析羽毛类型；她的大脑立即将图像与她在公园里对知更鸟的经历和记忆联系起来。</p>',r:{minutes:22.45,words:6736},t:"什么是图像识别？算法和应用",y:"a"}}],["/zh/posts/reprints/introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression.html",{loader:()=>a.e(1987).then(a.bind(a,85527)),meta:{d:1733895946e3,l:"2024年12月11日",c:["初学者"],g:["Prompt","text2image"],e:'\n<p>本指南涵盖AI插画生成的prompt，重点介绍camera angle、composition和facial expression，以提升您的创作过程。</p>\n<h2>Camera angle和composition的prompt</h2>\n<h3>Camera angle</h3>\n<table>\n<thead>\n<tr>\n<th>Prompt</th>\n<th>描述</th>\n<th>图片</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>from the front</code>, <code>front view</code></td>\n<td>从正面拍摄。</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-from_the_front.jpg" alt="Front view" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>from above</code>, <code>high angle</code></td>\n<td>从上方拍摄。</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-from_above.jpg" alt="High angle" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>from below</code>, <code>low angle</code></td>\n<td>从下方拍摄。</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-from_below.jpg" alt="Low angle" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>side view</code>, <code>side angle</code></td>\n<td>从侧面拍摄。</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-side_view.jpg" alt="Side view" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>back shot</code>, <code>back view</code></td>\n<td>从后方拍摄。使用<code>looking back</code>可实现转身姿势。</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-back_shot.jpg" alt="Back view" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>overhead shot</code>, <code>bird\'s-eye view</code></td>\n<td>从上方俯拍，通常用于躺着的主体。</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-overhead_shot.jpg" alt="Bird\'s-eye view" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>pov</code></td>\n<td>第一人称主观视角。结合<code>pov hand</code>可实现手部视角。</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-pov.png" alt="POV" loading="lazy"></td>\n</tr>\n<tr>\n<td><code>dutch angle</code></td>\n<td>倾斜拍摄以实现引人注目的构图。</td>\n<td><img src="https://neverbiasu.github.io/assets/images/reprints/digitalcreativeai/prompt-samples-dutch_angle.jpg" alt="Dutch angle" loading="lazy"></td>\n</tr>\n</tbody>\n</table>',r:{minutes:4.87,words:1461},t:"AI插画生成中的prompt介绍【composition/camera angle/facial expression】",y:"a"}}],["/zh/posts/reprints/mcp-flash-in-the-pan-or-future-standard.html",{loader:()=>a.e(8627).then(a.bind(a,27822)),meta:{a:"Harrison Chase & Nuno Campos",d:17145216e5,l:"2024年5月1日",c:["转载"],g:["AI","LLM","协议","辩论"],u:!1,v:"/assets/images/reprint/mcp-debate-cover.jpeg",e:"\n<p>Model Context Protocol (MCP) 在Twitter上引起了轩然大波——但它真的有用，还是只是噪音？在这场辩论中，Harrison Chase（LangChain CEO）和Nuno Campos（LangGraph负责人）讨论了MCP是否名副其实。</p>\n<h2>Harrison的观点：MCP确实有用</h2>\n<p>我一开始对MCP持怀疑态度，但我已经开始看到它的价值。本质上：<strong>当你想为自己无法控制的agent添加工具时，MCP就很有用</strong>。</p>\n<p>举个例子。对于Claude Desktop、Cursor、Windsurf这些应用，作为用户，我无法控制底层agent。这些agent默认只能访问几个内置工具。</p>",r:{minutes:6.47,words:1942},t:"MCP：昙花一现还是未来标准？",i:"openmoji:code-editor",O:1,y:"a"}}],["/zh/posts/reprints/niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything.html",{loader:()=>a.e(2869).then(a.bind(a,74037)),meta:{a:"niji・journey",d:16898976e5,l:"2023年7月21日",c:["reprints"],g:["Art","Drawing","Fundamentals","niji"],u:!1,v:"/assets/images/reprints/nijijourney/lesson1/thumb.webp",e:'\n<p>在本课程中，我们不会专注于如何绘制特定主题，而是教你如何在niji的帮助下自学成才。</p>\n<p>相关练习请参见 <a href="https://nijijourney.com/blog/niji-study-1-Measuring-With-Your-Eyes" target="_blank" rel="noopener noreferrer">📏 练习1：用眼睛测量</a>。</p>\n<h2>美的理论基础</h2>\n<p>我人生的大部分时间都困在这两类图像之间的鸿沟里。</p>\n<figure><img src="/assets/images/reprints/nijijourney/lesson1/4942dc04-f4f6-415f-b85f-91ecf5703d9a.jpeg" alt="动漫风格的女孩" tabindex="0" loading="lazy"><figcaption>动漫风格的女孩</figcaption></figure>',r:{minutes:8.03,words:2410},t:"第一课：测量与抽象的基础：绘画的普遍理论",i:"palette",O:1,y:"a"}}],["/zh/posts/reprints/niji-lesson-2-the-terminator-line.html",{loader:()=>a.e(7910).then(a.bind(a,75122)),meta:{e:'<p>title: "课程 2：终结者（线）"\nicon: palette\ncover: /assets/images/reprints/nijijourney/lesson2/thumb.webp\norder: 2\nauthor: niji・journey\ndate: 2023-07-25\ncategories:</p>\n<ul>\n<li>转载\ntags:</li>\n<li>艺术课程</li>\n<li>视觉层次\ndescription: "探索终结者线的概念及其在视觉层次和艺术构图中的作用。"\nsticky: false\nstar: false\nfooter: "转载自 niji・journey"\ncopyright: 转载</li>\n</ul>',r:{minutes:5.97,words:1790},t:"课程 2：终结者（线）",y:"a"}}],["/zh/posts/reprints/niji-study-1-measuring-with-your-eyes.html",{loader:()=>a.e(7597).then(a.bind(a,80191)),meta:{a:"niji・journey",d:16898976e5,l:"2023年7月21日",c:["reprints"],g:["Art","Drawing","Fundamentals","niji"],u:!1,v:"https://neverbiasu.github.io/assets/images/reprints/nijijourney/study1/thumb.webp",e:'\n<p><a href="/zh/posts/reprints/niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything">上一课：测量与抽象的基础：绘画的普遍理论</a> | <a href="https://nijijourney.com/blog/niji-study-2-notan" target="_blank" rel="noopener noreferrer">下一课：练习2：浓淡</a></p>\n<p>你知道可以通过 AI 来提高自己的绘画能力吗？Niji Academy 是一个实验性项目，旨在帮助你使用 Niji 更快地学习绘画。我们每周都有讲座，但我们也在这里发布练习，这样如果你不能参加讲座，也可以参考它们！理解图像的关键是用眼睛进行准确的测量。这是一个很好的练习，可以训练你的手眼协调能力，并加深对所绘主题的理解。</p>',r:{minutes:4.77,words:1431},t:"练习1：用眼睛测量",i:"ruler",O:2,y:"a"}}],["/zh/posts/reprints/niji-study-2-notan.html",{loader:()=>a.e(711).then(a.bind(a,62647)),meta:{a:"niji・journey",d:16902432e5,l:"2023年7月25日",c:["论文"],g:["艺术课程","Notan"],u:!1,v:"/assets/images/reprints/nijijourney/study2/thumb.webp",e:'\n<h2>"Notan" 在艺术中的含义是什么？</h2>\n<figure><img src="/assets/images/reprints/nijijourney/study2/f7577f87-812e-46b6-83ff-98a24eb59a3e.jpeg" alt="Siddarth Chaturvedi 的公鸡与老鼠插图" tabindex="0" loading="lazy"><figcaption>Siddarth Chaturvedi 的公鸡与老鼠插图</figcaption></figure>\n<p>"Notan" 是日语中表示“明暗和谐”的词汇。艺术家使用 "Notan 研究" 来尝试在绘画中不同的明暗排列，而无需担心颜色、纹理或小细节。</p>',r:{minutes:2.58,words:773},t:"研究 2：Notan",i:"contrast",O:3,y:"a"}}],["/zh/posts/reprints/niji-video.html",{loader:()=>a.e(2657).then(a.bind(a,56455)),meta:{d:17501184e5,l:"2025年6月17日",g:["功能发布","使用指南"],e:"\n<p>好久不见！我们确实很久没有更新了。</p>\n<p>这次的更新真的令人印象深刻。我们制作了一个动画片头！！！呃，我是说，Niji 现在可以生成视频了！！！！！！！！！</p>\n<p>虽然我对我们制作的每一个功能都很兴奋，但我可以肯定地说，将你的图片视频化是我们近期推出的最棒的功能。我真的希望你会喜欢它！</p>\n<p>要看到视频模型的实际效果，你可以查看我们使用视频模型制作的这个动画片头！！！！！</p>\n<p>（我的二次元梦想成真了！！😭）</p>\n<p>这个功能制作了很长时间，所以我们超级兴奋能够最终发布它！在这篇文章中，我将给你一些关于如何使用它的简单说明，然后讲述一些获得最佳质量视频的有用技巧 ✨</p>",r:{minutes:4.84,words:1452},t:"如何使用 niji・journey 制作视频",y:"a"}}],["/zh/posts/reprints/original-character-lora-sdxl-character-training.html",{loader:()=>a.e(8436).then(a.bind(a,4010)),meta:{d:17470333e5,l:"2025年5月12日",c:["高级"],g:["Kohya SS GUI","LoRA","SDXL"],e:'\n<figure><img src="http://digitalcreativeai.net/_next/image?url=https%3A%2F%2Fdca.data-hub-center.com%2Fcontent%2Fuploads%2F2025%2F05%2Feye_catch_original-character-lora-sdxl-character-training-en.jpg&amp;w=3840&amp;q=80" alt="如何创建原创角色 LoRA [SDXL 训练] SDXL 角色训练封面图片" tabindex="0" loading="lazy"><figcaption>如何创建原创角色 LoRA [SDXL 训练] SDXL 角色训练封面图片</figcaption></figure>',r:{minutes:11.53,words:3460},t:"如何创建原创角色 LoRA [SDXL 训练] SDXL 角色训练",y:"a"}}],["/zh/posts/reprints/qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview.html",{loader:()=>a.e(266).then(a.bind(a,60931)),meta:{a:"Jainil Prajapati",d:17458848e5,l:"2025年4月29日",c:["reprint"],g:["Qwen","Qwen3","阿里巴巴AI研究","阿里巴巴AI","大语言模型","LLMs","LLM基准测试","多语言AI","多模态AI","AI推理","MCP"],u:!1,v:"https://altctrlai.com/content/images/size/w2000/2025/04/834B968B-9CED-4EF4-B81F-845F8241AC0A.webp",e:'\n<figure><img src="https://altctrlai.com/content/images/size/w2000/2025/04/834B968B-9CED-4EF4-B81F-845F8241AC0A.webp" alt="Qwen3: 下一代具备混合思维和多语言精通能力的AI模型" tabindex="0" loading="lazy"><figcaption>Qwen3: 下一代具备混合思维和多语言精通能力的AI模型</figcaption></figure>\n<p>Qwen3代表了人工智能领域的重大进步，提供了改进的推理能力、多语言支持以及各种基准测试的增强性能。作为Qwen大语言模型家族的最新成员，这一版本引入了创新功能和架构改进，使其成为主要AI实验室领先模型的有力竞争者。以下全面分析探讨了Qwen3的能力、技术规格和实际应用。</p>',r:{minutes:9.81,words:2944},t:"Qwen3: 下一代具备混合思维和多语言精通能力的AI模型 | 2025年概览",i:"fa-solid:robot",y:"a"}}],["/zh/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.html",{loader:()=>a.e(5113).then(a.bind(a,41058)),meta:{a:"Kemal Erdem",g:["机器学习","扩散模型"],e:"\n<blockquote>\n<p>本文为原文《Step by Step visual introduction to Diffusion Models》的中文翻译，原作者 Kemal Erdem。</p>\n</blockquote>\n<h2>什么是扩散模型？</h2>\n<p>扩散模型的概念并不久远。2015 年的论文《Deep Unsupervised Learning using Nonequilibrium Thermodynamics》中，作者这样描述：</p>\n<blockquote>\n<p>其核心思想，受非平衡统计物理学启发，是通过<strong>逐步的前向扩散过程</strong>系统性地、缓慢地破坏数据分布中的结构。随后我们学习一个<strong>反向扩散过程</strong>，恢复数据结构，从而获得对数据高度灵活且易于处理的生成模型。</p>\n</blockquote>",r:{minutes:10.98,words:3294},t:"Diffusion Models 可视化逐步入门",y:"a"}}],["/zh/posts/sci/conda.html",{loader:()=>a.e(1204).then(a.bind(a,97685)),meta:{e:'\n<hr>\n<h2>摘要</h2>\n<p>Conda 是科研工作者、开发者管理代码环境的利器，支持多语言、环境隔离和依赖管理。本文介绍了 Conda 的安装方法、核心命令及其在科研中的实际应用案例，包括创建独立环境、配置镜像源和管理包。通过 Conda，我们可以高效地管理项目依赖，避免冲突，提升工作效率。</p>\n<hr>\n<h2>目录</h2>\n<ol>\n<li><a href="#%E4%BB%80%E4%B9%88%E6%98%AF-conda">什么是 Conda？</a></li>\n<li><a href="#%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85-conda">如何安装 Conda？</a></li>\n<li><a href="#%E6%A1%88%E4%BE%8B%E7%94%A8-conda-%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%BB%88%E7%AB%AF%E5%8A%A8%E7%94%BB%E7%8E%AF%E5%A2%83">案例：用 Conda 创建一个终端动画环境</a></li>\n<li><a href="#conda-%E7%9A%84%E6%A0%B8%E5%BF%83%E5%91%BD%E4%BB%A4">Conda 的核心命令</a></li>\n</ol>',r:{minutes:4.44,words:1331},t:"【高效科研】Conda：科研代码环境管理的利器",y:"a"}}],["/zh/posts/templates/ai-weekly.html",{loader:()=>a.e(144).then(a.bind(a,48869)),meta:{e:'\n<figure><img src="/assets/images/placeholder.png" alt="封面图" tabindex="0" loading="lazy"><figcaption>封面图</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：[亮点1简述]，[亮点2简述]，[亮点3简述]。详见正文，相关参考链接请见文末。</p>\n<hr>\n<h2>目录</h2>\n<ol>\n<li><a href="#%E6%9D%A1%E7%9B%AE1">条目1</a></li>\n<li><a href="#%E6%9D%A1%E7%9B%AE2">条目2</a></li>\n<li><a href="#%E6%9D%A1%E7%9B%AE3">条目3</a></li>\n</ol>',r:{minutes:.97,words:291},t:"[本期亮点关键词1] | [本期亮点关键词2] | [本期亮点关键词3]【AI周报】",y:"a"}}],["/zh/posts/templates/hf-weekly.html",{loader:()=>a.e(3679).then(a.bind(a,8301)),meta:{e:'\n<figure><img src="/assets/images/placeholder.png" alt="封面图" tabindex="0" loading="lazy"><figcaption>封面图</figcaption></figure>\n<h2>摘要</h2>\n<p>本周亮点：[简要描述本期Huggingface相关的主要动态、模型、工具或社区事件]。详细内容见下文，相关参考链接请见文末。</p>\n<hr>\n<h2>目录</h2>\n<ol>\n<li><a href="#%E6%9D%A1%E7%9B%AE1">条目1</a></li>\n<li><a href="#%E6%9D%A1%E7%9B%AE2">条目2</a></li>\n<li><a href="#%E6%9D%A1%E7%9B%AE3">条目3</a></li>\n</ol>',r:{minutes:1.3,words:390},t:"[本期亮点关键词1] | [本期亮点关键词2] | [本期亮点关键词3]【HF周报】",y:"a"}}],["/zh/posts/templates/papers.html",{loader:()=>a.e(1998).then(a.bind(a,67975)),meta:{e:'\n<p>!可选：论文/方法相关图片或Logo</p>\n<h2>摘要</h2>\n<p>简要介绍论文背景、核心创新、主要贡献和实验亮点（100-120字，突出作者、模型/方法、适用场景等关键信息）。</p>\n<hr>\n<h2>目录</h2>\n<ol>\n<li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li>\n<li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li>\n<li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li>\n<li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li>\n<li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li>\n</ol>',r:{minutes:1.61,words:483},t:"【论文精读】论文标题（中英文对照可选）",y:"a"}}],["/zh/posts/templates/repos.html",{loader:()=>a.e(2052).then(a.bind(a,70951)),meta:{d:17492544e5,l:"2025年6月7日",c:["教程指南"],g:["技术教程","GitHub","开源项目"],v:"/path/to/cover-image.jpg",e:'\n<p>!可选：项目Logo或相关截图</p>\n<h2>摘要</h2>\n<p>简要介绍本篇教程的目标、适用读者以及预期收获（80-100字，突出项目特点和教程价值）。</p>\n<hr>\n<h2>目录</h2>\n<ol>\n<li>项目简介</li>\n<li>环境准备</li>\n<li>快速开始</li>\n<li>配置项目</li>\n<li>运行项目</li>\n<li>常见问题解决</li>\n<li>进阶技巧</li>\n<li>参与贡献</li>\n<li>总结</li>\n</ol>\n<hr>\n<h2>项目简介</h2>\n<blockquote>\n<p>🔗 GitHub仓库：<a href="https://github.com/%5B%E7%94%A8%E6%88%B7%E5%90%8D%5D/%5B%E4%BB%93%E5%BA%93%E5%90%8D%5D" target="_blank" rel="noopener noreferrer">链接</a><br>\n📚 官方文档：[链接]</p>\n</blockquote>',r:{minutes:2.59,words:778},t:"GitHub仓库跑通教程：[项目名称]",y:"a"}}],["/zh/posts/thoughts/platform-operation-thoughts-after-comfycon.html",{loader:()=>a.e(7316).then(a.bind(a,23797)),meta:{a:"Faych",d:17436384e5,l:"2025年4月3日",c:["思考"],g:["平台","运营","内容","创新"],u:!1,v:"https://faych.notion.site/image/attachment%3Ac369ffa5-3a8d-4efd-bf62-3811710bc286%3Aimage.png?table=block&id=1ca5f3c4-a139-8050-9757-ff079e04ea98&spaceId=021ded55-a224-419c-939c-70c6888912f7&width=2000&userId=&cache=v2",e:"\n<h2>一、引言：为何探究多平台运营？</h2>\n<p>在 ComfyCon 上，我见到了不少在不同平台上的领军人物，他们为什么能成为顶流，我可以吗？引发了我的思考：</p>\n<ol>\n<li>我的内容创作能力是否有待提升？</li>\n<li>为什么在数十个平台均运营近一年后，仍未能成为真正的“Top”？</li>\n</ol>\n<p>本文将基于我各平台的数据，直面问题，剖析优劣得失，并探索未来可执行的改进方向，为自己带来更明确的运营策略和目标。</p>\n<hr>\n<h2>二、平台分类与核心数据</h2>\n<table>\n<thead>\n<tr>\n<th>平台</th>\n<th>平台分类</th>\n<th>描述</th>\n<th>创作开始时间</th>\n<th>最近更新时间</th>\n<th>成就</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Bilibili</td>\n<td>内容传播</td>\n<td>国内长视频内容发布和互动平台</td>\n<td>Dec. 5, 2022</td>\n<td>Jan. 6, 2025</td>\n<td>孙笑川与邻居友好互动日版6.9万播放量，总播放21.4万，总获赞2593，粉丝34</td>\n</tr>\n<tr>\n<td>CivitAI</td>\n<td>AI 社区</td>\n<td>最大AI绘画模型社区</td>\n<td>Mar. 2, 2025</td>\n<td>Apr. 2, 2025</td>\n<td>FeMix_HassakuXL获得2.6K下载，35书签，138点赞，总下载2.7K，总获赞183，粉丝58；Leaderboard新创作者第14，Base Model Creators第16</td>\n</tr>\n<tr>\n<td>CodeWithGPU</td>\n<td>镜像</td>\n<td>autodl的算法镜像社区</td>\n<td>Jan. 1, 2024</td>\n<td>Mar. 3, 2025</td>\n<td>ChatTTS全站最高排名第49，总下载2.4K</td>\n</tr>\n<tr>\n<td>CompShare</td>\n<td>镜像</td>\n<td>CompShare的算法镜像社区</td>\n<td>Nov. 7, 2024</td>\n<td>Mar. 20, 2025</td>\n<td>lora-scripts全站最高排名第2</td>\n</tr>\n<tr>\n<td>GitHub</td>\n<td>代码</td>\n<td>代码开源平台</td>\n<td>Oct. 29, 2023</td>\n<td>Apr. 2, 2025</td>\n<td>ComfyUI-SAM2获144 Star，总Star233，开发者评级A</td>\n</tr>\n<tr>\n<td>Liblib</td>\n<td>AI 社区</td>\n<td>国内最大AI绘画模型/工作流平台</td>\n<td>Jan. 27, 2025</td>\n<td>Mar. 26, 2025</td>\n<td>总下载3.2K</td>\n</tr>\n<tr>\n<td>ModelScope</td>\n<td>AI 社区</td>\n<td>国内最大AI模型平台（huggingface平替）</td>\n<td>June 3, 2025</td>\n<td>Mar. 27, 2025</td>\n<td>总下载7945</td>\n</tr>\n<tr>\n<td>OpenArt</td>\n<td>AI 社区</td>\n<td>最大的AI工作流开源社区</td>\n<td>Mar. 3, 2025</td>\n<td>Mar. 7, 2025</td>\n<td>总下载441，观看1.5K</td>\n</tr>\n<tr>\n<td>微信公众号</td>\n<td>内容传播</td>\n<td>微信自带的信息发布和互动平台</td>\n<td>Sept. 1, 2025</td>\n<td>Mar. 31, 2025</td>\n<td>【高效科研】李沐和吴恩达推荐的论文散步精度法获517阅读，121转发，粉丝171</td>\n</tr>\n<tr>\n<td>知乎</td>\n<td>内容传播</td>\n<td>问答、专业知识分享和讨论平台</td>\n<td>Mar. 29, 2024</td>\n<td>Feb. 26, 2025</td>\n<td>总获赞447，被关注49</td>\n</tr>\n</tbody>\n</table>",r:{minutes:6.59,words:1977},t:"参加 ComfyCon 有感 —— 数据剖析与自我反思",i:"ic:round-published-with-changes",y:"a"}}],["/zh/posts/tutorials/qwen-code.html",{loader:()=>a.e(7238).then(a.bind(a,73622)),meta:{e:'\n<h2>摘要</h2>\n<p>本教程将指导如何安装和配置 Qwen-Code，包括 Node.js 的安装、环境变量的设置以及 Qwen-Code 的初始化和测试。通过本教程，将能够快速上手并成功调用 Qwen-Code。</p>\n<h2>目录</h2>\n<ol>\n<li><a href="#%E5%AE%89%E8%A3%85-nodejs">安装 Node.js</a></li>\n<li><a href="#%E5%AE%89%E8%A3%85-qwen-code">安装 Qwen-Code</a></li>\n<li><a href="#%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE">环境变量配置</a></li>\n<li><a href="#%E5%88%9D%E5%A7%8B%E5%8C%96-qwen-code">初始化 Qwen-Code</a></li>\n<li><a href="#%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E">使用说明</a></li>\n</ol>',r:{minutes:2.99,words:896},t:"【项目实战】Claude-Code开源平替——Qwen-Code零成本安装及使用教程",y:"a"}}],["/zh/posts/web/vue-1.html",{loader:()=>a.e(7710).then(a.bind(a,27319)),meta:{e:"\n",r:{minutes:.02,words:5},t:"Vue3 快速开始",y:"a"}}],["/zh/posts/ielts/usage-of-collocations-in-speaking/ielts-collocations.html",{loader:()=>a.e(6771).then(a.bind(a,96354)),meta:{e:"\n<h1>附录1：全书词伙分类列表</h1>\n<h1>AcademicSkills</h1>\n<p>researchskills研究能力<br>\nanalytical skills分析能力<br>\nsearchthewebforinformation上网找资料<br>\ndoresearch做调查<br>\nleadershipskills领导能力<br>\nleadershipqualities领导的素质<br>\norganisational skills组织能力<br>\nacademicskills学术能力<br>\nproblem-solvingskills解决问题的能力<br>\ngatherinformation收集信息<br>\ncritical thinkingskills思辨能力<br>\ncomputerliteracy电脑能力<br>\npracticalskills实际工作的能力<br>\nsharpenskills/addtotheskill set丰富技能</p>",r:{minutes:26.03,words:7808},t:"附录",y:"a"}}],["/zh/posts/tutorials/comfyui/flux-kontext-beginner.html",{loader:()=>a.e(6158).then(a.bind(a,25989)),meta:{e:'\n<figure><img src="https://neverbiasu.github.io/assets/images/tutorials/comfyui/flux-kontext/beginner/flux-kontext-teaser.png" alt="Hello Kontext" tabindex="0" loading="lazy"><figcaption>Hello Kontext</figcaption></figure>\n<h2>摘要</h2>\n<p>本教程介绍FLUX.1 Kontext的核心功能，包括局部编辑、角色一致性、风格参考和迭代编辑。通过ComfyUI实现环境搭建，提供硬件需求、模型下载及工作流配置指南。适用于设计创作、摄影后期及电商营销等场景，附提示词技巧和参数优化建议。</p>',r:{minutes:6.86,words:2057},t:"【ComfyUI 实战指南】FLUX.1 Kontext 入门教程：从零开始的图像编辑之旅",y:"a"}}],["/zh/posts/workflows/hf-weekly/checklist.html",{loader:()=>a.e(9033).then(a.bind(a,89231)),meta:{e:'\n<ul class="task-list-container">\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-0" disabled="disabled"><label class="task-list-item-label" for="task-item-0"> 主标题是否包含3个核心模型/事件全称，64字以内，结尾有【HF周报】</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-1" disabled="disabled"><label class="task-list-item-label" for="task-item-1"> 摘要是否100-120字，首句“本周亮点：”，末句“详见正文，相关参考链接请见文末。”</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-2" disabled="disabled"><label class="task-list-item-label" for="task-item-2"> 目录与正文条目顺序一致，标题一致</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-3" disabled="disabled"><label class="task-list-item-label" for="task-item-3"> 每条目有配图、概要、唯一标签、权威链接</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-4" disabled="disabled"><label class="task-list-item-label" for="task-item-4"> 标签无重复，突出独特性</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-5" disabled="disabled"><label class="task-list-item-label" for="task-item-5"> 参考链接权威、可访问</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-6" disabled="disabled"><label class="task-list-item-label" for="task-item-6"> 全文结构、格式、字数、逻辑流畅</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-7" disabled="disabled"><label class="task-list-item-label" for="task-item-7"> 封面图prompt已单独存放，不混入正文</label></li>\n</ul>',r:{minutes:.48,words:143},t:"HF周报撰写检查清单（Checklist）",y:"a"}}],["/zh/posts/workflows/hf-weekly/workflow.html",{loader:()=>a.e(8264).then(a.bind(a,52548)),meta:{e:"\n<ol>\n<li>收集本周Huggingface及AI社区最新模型、工具、事件，筛选8-10个最具代表性条目</li>\n<li>设计主标题，优先突出热搜模型/关键词，控制字数在64字以内</li>\n<li>编写摘要，确保覆盖本期主线与亮点，字数100-120字，首句“本周亮点：”，末句“详见正文，相关参考链接请见文末。”</li>\n<li>梳理目录，确保条目顺序与正文一致，标题一致</li>\n<li>按模板撰写每个条目，确保配图、概要、标签、链接齐全</li>\n<li>检查所有条目标签唯一性、链接有效性、图片可用性</li>\n<li>复查全文结构、格式、字数、逻辑流畅性</li>\n<li>最后补充/更新参考链接，确保权威性与可访问性</li>\n<li>封面图prompt单独存放，不混入正文</li>\n</ol>",r:{minutes:.75,words:225},t:"HF周报撰写工作流（Workflow）",y:"a"}}],["/zh/posts/workflows/papers/checklist.html",{loader:()=>a.e(5654).then(a.bind(a,4888)),meta:{e:'\n<p>在发布论文精读前，请逐一核对以下各项，确保内容的准确性、完整性和可读性。</p>\n<h2>内容准确性检查</h2>\n<ul class="task-list-container">\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-0" disabled="disabled"><label class="task-list-item-label" for="task-item-0"> <strong>数据真实性</strong>: 所有数字、数据和结果均来自原论文，无虚构或猜测内容</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-1" disabled="disabled"><label class="task-list-item-label" for="task-item-1"> <strong>公式准确性</strong>: 所有公式与原论文一致，包括符号和参数的正确表示</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-2" disabled="disabled"><label class="task-list-item-label" for="task-item-2"> <strong>算法描述</strong>: 算法流程与原论文完全一致，无简化导致的错误</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-3" disabled="disabled"><label class="task-list-item-label" for="task-item-3"> <strong>结果引用</strong>: 所有结果和表现指标正确引用原论文，无误导性表述</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-4" disabled="disabled"><label class="task-list-item-label" for="task-item-4"> <strong>引用明确</strong>: 清晰区分原论文内容与个人解释和观点</label></li>\n</ul>',r:{minutes:2.9,words:870},t:"论文精读质量检查清单",y:"a"}}],["/zh/posts/workflows/papers/workflow.html",{loader:()=>a.e(3561).then(a.bind(a,36814)),meta:{e:'\n<figure><img src="https://path-to-workflow-image.png" alt="论文阅读工作流程图" tabindex="0" loading="lazy"><figcaption>论文阅读工作流程图</figcaption></figure>\n<h2>引言</h2>\n<p>高效的论文阅读和精读是研究者和实践者的重要技能。本文总结了一套系统化的论文精读工作流，从初次接触论文到产出高质量的论文解读，帮助读者在有限时间内最大化吸收论文价值并形成自己的见解。</p>\n<p>本工作流结合了李沐与吴恩达的"三步法"阅读方法，以及多位资深研究者的实践经验，适用于计算机科学、人工智能等技术论文的阅读与解读。</p>',r:{minutes:13.74,words:4122},t:"论文精读工作流：从阅读到产出高质量解读",y:"a"}}],["/zh/posts/workflows/repos/checklist.html",{loader:()=>a.e(938).then(a.bind(a,5954)),meta:{e:'\n<h2>准备阶段</h2>\n<h3>项目评估</h3>\n<ul class="task-list-container">\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-0" disabled="disabled"><label class="task-list-item-label" for="task-item-0"> 确认项目活跃度（最近更新时间、贡献者数量、Star数量）</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-1" disabled="disabled"><label class="task-list-item-label" for="task-item-1"> 确认项目文档完整性（README、文档、示例）</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-2" disabled="disabled"><label class="task-list-item-label" for="task-item-2"> 阅读官方文档，确认项目功能和用途</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-3" disabled="disabled"><label class="task-list-item-label" for="task-item-3"> 在本地环境成功运行项目</label></li>\n<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" id="task-item-4" disabled="disabled"><label class="task-list-item-label" for="task-item-4"> 记录可能的问题点和解决方案</label></li>\n</ul>',r:{minutes:3.17,words:950},t:"GitHub项目教程制作检查清单",y:"a"}}],["/zh/posts/workflows/repos/material.html",{loader:()=>a.e(7061).then(a.bind(a,95043)),meta:{e:"\n<h2>核心材料来源</h2>\n<h3>GitHub原始资料</h3>\n<ul>\n<li>GitHub仓库主页及项目结构</li>\n<li>README.md（项目说明文档）</li>\n<li>项目Wiki页面</li>\n<li>Issues和Pull Requests</li>\n<li>Releases发布记录</li>\n<li>贡献指南（CONTRIBUTING.md）</li>\n<li>项目许可证（LICENSE）</li>\n</ul>\n<h3>官方文档资料</h3>\n<ul>\n<li>官方技术文档</li>\n<li>API参考文档</li>\n<li>入门指南和教程</li>\n<li>示例项目和代码</li>\n<li>官方博客文章</li>\n<li>常见问题解答（FAQ）</li>\n<li>视频教程和演示</li>\n</ul>",r:{minutes:10.22,words:3067},t:"GitHub项目解读教程材料来源与分类",y:"a"}}],["/zh/posts/workflows/repos/workflow.html",{loader:()=>a.e(3933).then(a.bind(a,26221)),meta:{e:"\n<h2>准备阶段</h2>\n<h3>1. 项目筛选与评估</h3>\n<ol>\n<li>\n<p><strong>选择标准</strong></p>\n<ul>\n<li>项目活跃度（星标数、最近更新时间）</li>\n<li>文档完整性（README质量、Wiki情况）</li>\n<li>技术价值与学习意义</li>\n<li>跑通难度（环境要求、依赖复杂度）</li>\n</ul>\n</li>\n<li>\n<p><strong>初步调研</strong></p>\n<ul>\n<li>浏览GitHub仓库基本信息</li>\n<li>阅读README和官方快速入门</li>\n<li>检查Issues中的常见问题</li>\n<li>确认项目许可证类型</li>\n<li>评估目标读者群体</li>\n</ul>\n</li>\n</ol>",r:{minutes:5.07,words:1521},t:"GitHub项目跑通教程制作工作流程",y:"a"}}],["/404.html",{loader:()=>a.e(7490).then(a.bind(a,81489)),meta:{t:""}}],["/posts/reprints/",{loader:()=>a.e(7046).then(a.bind(a,54860)),meta:{t:"Reprints"}}],["/posts/",{loader:()=>a.e(8666).then(a.bind(a,18063)),meta:{t:"Posts"}}],["/zh/posts/ai-impls/",{loader:()=>a.e(5770).then(a.bind(a,61799)),meta:{t:"Ai Impls"}}],["/zh/posts/",{loader:()=>a.e(9773).then(a.bind(a,12269)),meta:{t:"Posts"}}],["/zh/posts/ai-weekly/",{loader:()=>a.e(7488).then(a.bind(a,20785)),meta:{t:"Ai Weekly"}}],["/zh/posts/dairys/",{loader:()=>a.e(1426).then(a.bind(a,5355)),meta:{t:"Dairys"}}],["/zh/posts/hf-weekly/",{loader:()=>a.e(1332).then(a.bind(a,12367)),meta:{t:"Hf Weekly"}}],["/zh/posts/ielts/",{loader:()=>a.e(2625).then(a.bind(a,79356)),meta:{t:"Ielts"}}],["/zh/posts/papers/",{loader:()=>a.e(4601).then(a.bind(a,21332)),meta:{t:"Papers"}}],["/zh/posts/prompts/",{loader:()=>a.e(7469).then(a.bind(a,48430)),meta:{t:"Prompts"}}],["/zh/posts/repos/",{loader:()=>a.e(7919).then(a.bind(a,47104)),meta:{t:"Repos"}}],["/zh/posts/reprints/",{loader:()=>a.e(4423).then(a.bind(a,61060)),meta:{t:"Reprints"}}],["/zh/posts/sci/",{loader:()=>a.e(4567).then(a.bind(a,9673)),meta:{t:"Sci"}}],["/zh/posts/templates/",{loader:()=>a.e(5313).then(a.bind(a,23913)),meta:{t:"Templates"}}],["/zh/posts/thoughts/",{loader:()=>a.e(6824).then(a.bind(a,99044)),meta:{t:"Thoughts"}}],["/zh/posts/tutorials/",{loader:()=>a.e(9889).then(a.bind(a,89237)),meta:{t:"Tutorials"}}],["/zh/posts/web/",{loader:()=>a.e(5452).then(a.bind(a,79892)),meta:{t:"Web"}}],["/zh/posts/ielts/usage-of-collocations-in-speaking/",{loader:()=>a.e(467).then(a.bind(a,59973)),meta:{t:"Usage of Collocations in Speaking"}}],["/zh/posts/tutorials/comfyui/",{loader:()=>a.e(8444).then(a.bind(a,18e3)),meta:{t:"Comfyui"}}],["/zh/posts/workflows/hf-weekly/",{loader:()=>a.e(1013).then(a.bind(a,41726)),meta:{t:"Hf Weekly"}}],["/zh/posts/workflows/",{loader:()=>a.e(9680).then(a.bind(a,1734)),meta:{t:"Workflows"}}],["/zh/posts/workflows/papers/",{loader:()=>a.e(6162).then(a.bind(a,96284)),meta:{t:"Papers"}}],["/zh/posts/workflows/repos/",{loader:()=>a.e(4966).then(a.bind(a,48048)),meta:{t:"Repos"}}],["/category/",{loader:()=>a.e(3583).then(a.bind(a,3233)),meta:{t:"Category",I:!1}}],["/category/guide/",{loader:()=>a.e(3468).then(a.bind(a,96487)),meta:{t:"Guide Category",I:!1}}],["/category/generative-ai/",{loader:()=>a.e(1367).then(a.bind(a,83719)),meta:{t:"Generative AI Category",I:!1}}],["/category/explainer/",{loader:()=>a.e(2580).then(a.bind(a,90723)),meta:{t:"Explainer Category",I:!1}}],["/category/news/",{loader:()=>a.e(7715).then(a.bind(a,74570)),meta:{t:"News Category",I:!1}}],["/category/aiml/",{loader:()=>a.e(3623).then(a.bind(a,29825)),meta:{t:"AI/ML Category",I:!1}}],["/category/reprints/",{loader:()=>a.e(9609).then(a.bind(a,21828)),meta:{t:"Reprints Category",I:!1}}],["/category/model-development/",{loader:()=>a.e(1881).then(a.bind(a,95603)),meta:{t:"Model Development Category",I:!1}}],["/category/reprint/",{loader:()=>a.e(8688).then(a.bind(a,29829)),meta:{t:"reprint Category",I:!1}}],["/category/model-training/",{loader:()=>a.e(9986).then(a.bind(a,52772)),meta:{t:"Model Training Category",I:!1}}],["/category/image-generation/",{loader:()=>a.e(8390).then(a.bind(a,1654)),meta:{t:"Image Generation Category",I:!1}}],["/category/anime-style/",{loader:()=>a.e(1712).then(a.bind(a,41828)),meta:{t:"Anime Style Category",I:!1}}],["/category/novice/",{loader:()=>a.e(4854).then(a.bind(a,70024)),meta:{t:"Novice Category",I:!1}}],["/category/ai-tools/",{loader:()=>a.e(1376).then(a.bind(a,78369)),meta:{t:"AI Tools Category",I:!1}}],["/category/reprints/",{loader:()=>a.e(9609).then(a.bind(a,21828)),meta:{t:"reprints Category",I:!1}}],["/category/papers/",{loader:()=>a.e(3611).then(a.bind(a,13333)),meta:{t:"Papers Category",I:!1}}],["/category/advanced/",{loader:()=>a.e(3550).then(a.bind(a,10230)),meta:{t:"Advanced Category",I:!1}}],["/zh/category/",{loader:()=>a.e(86).then(a.bind(a,70888)),meta:{t:"分类",I:!1}}],["/zh/category/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/",{loader:()=>a.e(1206).then(a.bind(a,41145)),meta:{t:"使用指南 分类",I:!1}}],["/zh/category/%E6%8C%87%E5%8D%97/",{loader:()=>a.e(8693).then(a.bind(a,4413)),meta:{t:"指南 分类",I:!1}}],["/zh/category/%E6%97%A5%E8%AE%B0/",{loader:()=>a.e(2830).then(a.bind(a,33193)),meta:{t:"日记 分类",I:!1}}],["/zh/category/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/",{loader:()=>a.e(3775).then(a.bind(a,75920)),meta:{t:"论文精读 分类",I:!1}}],["/zh/category/%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90/",{loader:()=>a.e(2929).then(a.bind(a,80591)),meta:{t:"视频生成 分类",I:!1}}],["/zh/category/%E5%BC%A0%E5%90%95%E6%95%8F/",{loader:()=>a.e(9461).then(a.bind(a,32752)),meta:{t:"张吕敏 分类",I:!1}}],["/zh/category/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/",{loader:()=>a.e(2827).then(a.bind(a,55951)),meta:{t:"图像生成 分类",I:!1}}],["/zh/category/%E5%88%9B%E9%80%A0%E8%80%85%E5%B7%A5%E5%9D%8A/",{loader:()=>a.e(8164).then(a.bind(a,65805)),meta:{t:"创造者工坊 分类",I:!1}}],["/zh/category/%E7%94%9F%E6%88%90%E5%BC%8Fai/",{loader:()=>a.e(8199).then(a.bind(a,43952)),meta:{t:"生成式AI 分类",I:!1}}],["/zh/category/explainer/",{loader:()=>a.e(5133).then(a.bind(a,82293)),meta:{t:"Explainer 分类",I:!1}}],["/zh/category/generative-ai/",{loader:()=>a.e(2778).then(a.bind(a,71280)),meta:{t:"Generative AI 分类",I:!1}}],["/zh/category/%E6%96%B0%E9%97%BB/",{loader:()=>a.e(1946).then(a.bind(a,89490)),meta:{t:"新闻 分类",I:!1}}],["/zh/category/aiml/",{loader:()=>a.e(912).then(a.bind(a,20666)),meta:{t:"AI/ML 分类",I:!1}}],["/zh/category/%E8%BD%AC%E8%BD%BD/",{loader:()=>a.e(420).then(a.bind(a,38389)),meta:{t:"转载 分类",I:!1}}],["/zh/category/%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91/",{loader:()=>a.e(5212).then(a.bind(a,81280)),meta:{t:"模型开发 分类",I:!1}}],["/zh/category/model-development/",{loader:()=>a.e(1628).then(a.bind(a,98760)),meta:{t:"Model Development 分类",I:!1}}],["/zh/category/model-training/",{loader:()=>a.e(2153).then(a.bind(a,85769)),meta:{t:"Model Training 分类",I:!1}}],["/zh/category/image-generation/",{loader:()=>a.e(1129).then(a.bind(a,8133)),meta:{t:"Image Generation 分类",I:!1}}],["/zh/category/anime-style/",{loader:()=>a.e(5421).then(a.bind(a,55967)),meta:{t:"Anime Style 分类",I:!1}}],["/zh/category/%E6%A8%A1%E5%9E%8B%E7%A0%94%E5%8F%91/",{loader:()=>a.e(9288).then(a.bind(a,24915)),meta:{t:"模型研发 分类",I:!1}}],["/zh/category/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/",{loader:()=>a.e(3935).then(a.bind(a,62900)),meta:{t:"模型训练 分类",I:!1}}],["/zh/category/%E5%8A%A8%E6%BC%AB%E9%A3%8E%E6%A0%BC/",{loader:()=>a.e(840).then(a.bind(a,53387)),meta:{t:"动漫风格 分类",I:!1}}],["/zh/category/%E5%88%9D%E5%AD%A6%E8%80%85/",{loader:()=>a.e(7339).then(a.bind(a,59893)),meta:{t:"初学者 分类",I:!1}}],["/zh/category/reprints/",{loader:()=>a.e(4426).then(a.bind(a,44544)),meta:{t:"reprints 分类",I:!1}}],["/zh/category/%E8%AE%BA%E6%96%87/",{loader:()=>a.e(7760).then(a.bind(a,99664)),meta:{t:"论文 分类",I:!1}}],["/zh/category/%E9%AB%98%E7%BA%A7/",{loader:()=>a.e(8974).then(a.bind(a,83067)),meta:{t:"高级 分类",I:!1}}],["/zh/category/reprint/",{loader:()=>a.e(1545).then(a.bind(a,18191)),meta:{t:"reprint 分类",I:!1}}],["/zh/category/%E6%95%99%E7%A8%8B%E6%8C%87%E5%8D%97/",{loader:()=>a.e(4989).then(a.bind(a,22960)),meta:{t:"教程指南 分类",I:!1}}],["/zh/category/%E6%80%9D%E8%80%83/",{loader:()=>a.e(6143).then(a.bind(a,5628)),meta:{t:"思考 分类",I:!1}}],["/tag/",{loader:()=>a.e(1797).then(a.bind(a,60951)),meta:{t:"Tag",I:!1}}],["/tag/disable/",{loader:()=>a.e(702).then(a.bind(a,26216)),meta:{t:"Tag: disable",I:!1}}],["/tag/encryption/",{loader:()=>a.e(6803).then(a.bind(a,45476)),meta:{t:"Tag: encryption",I:!1}}],["/tag/layout/",{loader:()=>a.e(1180).then(a.bind(a,26413)),meta:{t:"Tag: Layout",I:!1}}],["/tag/markdown/",{loader:()=>a.e(7931).then(a.bind(a,8216)),meta:{t:"Tag: Markdown",I:!1}}],["/tag/page-config/",{loader:()=>a.e(8782).then(a.bind(a,55844)),meta:{t:"Tag: Page config",I:!1}}],["/tag/guide/",{loader:()=>a.e(6210).then(a.bind(a,62351)),meta:{t:"Tag: Guide",I:!1}}],["/tag/art/",{loader:()=>a.e(7975).then(a.bind(a,30149)),meta:{t:"Tag: Art",I:!1}}],["/tag/artificial-intelligence/",{loader:()=>a.e(4498).then(a.bind(a,72735)),meta:{t:"Tag: Artificial Intelligence",I:!1}}],["/tag/creators/",{loader:()=>a.e(9235).then(a.bind(a,90431)),meta:{t:"Tag: Creators",I:!1}}],["/tag/gtc/",{loader:()=>a.e(890).then(a.bind(a,12899)),meta:{t:"Tag: GTC",I:!1}}],["/tag/resource-guide/",{loader:()=>a.e(7451).then(a.bind(a,51474)),meta:{t:"Tag: resource guide",I:!1}}],["/tag/script/",{loader:()=>a.e(9969).then(a.bind(a,80696)),meta:{t:"Tag: script",I:!1}}],["/tag/stable-diffusion/",{loader:()=>a.e(5175).then(a.bind(a,22149)),meta:{t:"Tag: stable diffusion",I:!1}}],["/tag/merge/",{loader:()=>a.e(7734).then(a.bind(a,19497)),meta:{t:"Tag: merge",I:!1}}],["/tag/model/",{loader:()=>a.e(1647).then(a.bind(a,24530)),meta:{t:"Tag: model",I:!1}}],["/tag/editor/",{loader:()=>a.e(619).then(a.bind(a,19427)),meta:{t:"Tag: editor",I:!1}}],["/tag/model-context-protocol/",{loader:()=>a.e(2808).then(a.bind(a,76562)),meta:{t:"Tag: Model Context Protocol",I:!1}}],["/tag/tech/",{loader:()=>a.e(2354).then(a.bind(a,86510)),meta:{t:"Tag: tech",I:!1}}],["/tag/inference/",{loader:()=>a.e(7461).then(a.bind(a,55371)),meta:{t:"Tag: Inference",I:!1}}],["/tag/ai/",{loader:()=>a.e(5268).then(a.bind(a,19082)),meta:{t:"Tag: AI",I:!1}}],["/tag/image-generation/",{loader:()=>a.e(1988).then(a.bind(a,54200)),meta:{t:"Tag: Image Generation",I:!1}}],["/tag/flux/",{loader:()=>a.e(4973).then(a.bind(a,55703)),meta:{t:"Tag: FLUX",I:!1}}],["/tag/machine-learning/",{loader:()=>a.e(1088).then(a.bind(a,47107)),meta:{t:"Tag: Machine Learning",I:!1}}],["/tag/flux.1/",{loader:()=>a.e(9548).then(a.bind(a,74198)),meta:{t:"Tag: FLUX.1",I:!1}}],["/tag/optimization/",{loader:()=>a.e(5983).then(a.bind(a,8942)),meta:{t:"Tag: Optimization",I:!1}}],["/tag/taylorseer/",{loader:()=>a.e(554).then(a.bind(a,67163)),meta:{t:"Tag: TaylorSeer",I:!1}}],["/tag/replicate/",{loader:()=>a.e(2213).then(a.bind(a,67118)),meta:{t:"Tag: Replicate",I:!1}}],["/tag/lora/",{loader:()=>a.e(3770).then(a.bind(a,14938)),meta:{t:"Tag: LoRA",I:!1}}],["/tag/qlora/",{loader:()=>a.e(3877).then(a.bind(a,83871)),meta:{t:"Tag: QLoRA",I:!1}}],["/tag/fine-tuning/",{loader:()=>a.e(1008).then(a.bind(a,46286)),meta:{t:"Tag: Fine-tuning",I:!1}}],["/tag/diffusers/",{loader:()=>a.e(4903).then(a.bind(a,18455)),meta:{t:"Tag: diffusers",I:!1}}],["/tag/quantization/",{loader:()=>a.e(6667).then(a.bind(a,40937)),meta:{t:"Tag: quantization",I:!1}}],["/tag/generative-ai/",{loader:()=>a.e(2777).then(a.bind(a,80649)),meta:{t:"Tag: Generative AI",I:!1}}],["/tag/game-development/",{loader:()=>a.e(6024).then(a.bind(a,88186)),meta:{t:"Tag: Game Development",I:!1}}],["/tag/stable-diffusion/",{loader:()=>a.e(5175).then(a.bind(a,22149)),meta:{t:"Tag: Stable Diffusion",I:!1}}],["/tag/aws/",{loader:()=>a.e(4079).then(a.bind(a,49317)),meta:{t:"Tag: AWS",I:!1}}],["/tag/amazon-bedrock/",{loader:()=>a.e(6641).then(a.bind(a,92305)),meta:{t:"Tag: Amazon Bedrock",I:!1}}],["/tag/illustrious/",{loader:()=>a.e(6815).then(a.bind(a,7442)),meta:{t:"Tag: Illustrious",I:!1}}],["/tag/lu/",{loader:()=>a.e(313).then(a.bind(a,93366)),meta:{t:"Tag: LU",I:!1}}],["/tag/lumina/",{loader:()=>a.e(9630).then(a.bind(a,40903)),meta:{t:"Tag: Lumina",I:!1}}],["/tag/ai-model/",{loader:()=>a.e(1092).then(a.bind(a,84851)),meta:{t:"Tag: AI Model",I:!1}}],["/tag/training/",{loader:()=>a.e(1188).then(a.bind(a,50718)),meta:{t:"Tag: Training",I:!1}}],["/tag/sdxl/",{loader:()=>a.e(1625).then(a.bind(a,32001)),meta:{t:"Tag: SDXL",I:!1}}],["/tag/2048-resolution/",{loader:()=>a.e(7057).then(a.bind(a,70731)),meta:{t:"Tag: 2048 Resolution",I:!1}}],["/tag/vpred/",{loader:()=>a.e(5599).then(a.bind(a,52487)),meta:{t:"Tag: vpred",I:!1}}],["/tag/epsilon-prediction/",{loader:()=>a.e(1094).then(a.bind(a,52483)),meta:{t:"Tag: epsilon prediction",I:!1}}],["/tag/anime/",{loader:()=>a.e(8738).then(a.bind(a,24506)),meta:{t:"Tag: Anime",I:!1}}],["/tag/base-model/",{loader:()=>a.e(5503).then(a.bind(a,42486)),meta:{t:"Tag: Base model",I:!1}}],["/tag/image-generation/",{loader:()=>a.e(1988).then(a.bind(a,54200)),meta:{t:"Tag: Image generation",I:!1}}],["/tag/computer-vision/",{loader:()=>a.e(8534).then(a.bind(a,95998)),meta:{t:"Tag: Computer Vision",I:!1}}],["/tag/image-recognition/",{loader:()=>a.e(6485).then(a.bind(a,94328)),meta:{t:"Tag: Image Recognition",I:!1}}],["/tag/prompt/",{loader:()=>a.e(6724).then(a.bind(a,87827)),meta:{t:"Tag: Prompt",I:!1}}],["/tag/text2image/",{loader:()=>a.e(9414).then(a.bind(a,57031)),meta:{t:"Tag: text2image",I:!1}}],["/tag/llm/",{loader:()=>a.e(6725).then(a.bind(a,77412)),meta:{t:"Tag: LLM",I:!1}}],["/tag/protocol/",{loader:()=>a.e(9870).then(a.bind(a,69947)),meta:{t:"Tag: Protocol",I:!1}}],["/tag/debate/",{loader:()=>a.e(7763).then(a.bind(a,33376)),meta:{t:"Tag: Debate",I:!1}}],["/tag/stablediffusion/",{loader:()=>a.e(4168).then(a.bind(a,47004)),meta:{t:"Tag: StableDiffusion",I:!1}}],["/tag/modelmerge/",{loader:()=>a.e(2109).then(a.bind(a,86646)),meta:{t:"Tag: ModelMerge",I:!1}}],["/tag/automatic1111/",{loader:()=>a.e(7817).then(a.bind(a,44196)),meta:{t:"Tag: AUTOMATIC1111",I:!1}}],["/tag/mcp/",{loader:()=>a.e(6294).then(a.bind(a,66883)),meta:{t:"Tag: MCP",I:!1}}],["/tag/news-agents/",{loader:()=>a.e(9426).then(a.bind(a,45149)),meta:{t:"Tag: News Agents",I:!1}}],["/tag/tmux/",{loader:()=>a.e(7676).then(a.bind(a,30009)),meta:{t:"Tag: tmux",I:!1}}],["/tag/amazon-q/",{loader:()=>a.e(7116).then(a.bind(a,43281)),meta:{t:"Tag: Amazon Q",I:!1}}],["/tag/agent-systems/",{loader:()=>a.e(8392).then(a.bind(a,3714)),meta:{t:"Tag: Agent Systems",I:!1}}],["/tag/drawing/",{loader:()=>a.e(4236).then(a.bind(a,7157)),meta:{t:"Tag: Drawing",I:!1}}],["/tag/fundamentals/",{loader:()=>a.e(1286).then(a.bind(a,8782)),meta:{t:"Tag: Fundamentals",I:!1}}],["/tag/niji/",{loader:()=>a.e(6474).then(a.bind(a,6127)),meta:{t:"Tag: niji",I:!1}}],["/tag/art-lesson/",{loader:()=>a.e(2430).then(a.bind(a,75433)),meta:{t:"Tag: Art Lesson",I:!1}}],["/tag/visual-hierarchy/",{loader:()=>a.e(6196).then(a.bind(a,83801)),meta:{t:"Tag: Visual Hierarchy",I:!1}}],["/tag/notan/",{loader:()=>a.e(360).then(a.bind(a,61753)),meta:{t:"Tag: Notan",I:!1}}],["/tag/feature-announcement/",{loader:()=>a.e(4372).then(a.bind(a,10890)),meta:{t:"Tag: Feature Announcement",I:!1}}],["/tag/usage-guide/",{loader:()=>a.e(5124).then(a.bind(a,36507)),meta:{t:"Tag: Usage Guide",I:!1}}],["/tag/kohya-ss-gui/",{loader:()=>a.e(6277).then(a.bind(a,73226)),meta:{t:"Tag: Kohya SS GUI",I:!1}}],["/tag/qwen/",{loader:()=>a.e(2493).then(a.bind(a,28025)),meta:{t:"Tag: Qwen",I:!1}}],["/tag/qwen3/",{loader:()=>a.e(9606).then(a.bind(a,10468)),meta:{t:"Tag: Qwen3",I:!1}}],["/tag/alibaba-ai-research/",{loader:()=>a.e(5043).then(a.bind(a,66647)),meta:{t:"Tag: Alibaba AI research",I:!1}}],["/tag/alibaba-ai/",{loader:()=>a.e(8779).then(a.bind(a,32374)),meta:{t:"Tag: Alibaba AI",I:!1}}],["/tag/large-language-models/",{loader:()=>a.e(811).then(a.bind(a,76982)),meta:{t:"Tag: large language models",I:!1}}],["/tag/llms/",{loader:()=>a.e(5406).then(a.bind(a,83768)),meta:{t:"Tag: LLMs",I:!1}}],["/tag/llm-benchmarks/",{loader:()=>a.e(1016).then(a.bind(a,65495)),meta:{t:"Tag: LLM benchmarks",I:!1}}],["/tag/multilingual-ai/",{loader:()=>a.e(3220).then(a.bind(a,88673)),meta:{t:"Tag: Multilingual AI",I:!1}}],["/tag/multimodal-ai/",{loader:()=>a.e(6585).then(a.bind(a,40700)),meta:{t:"Tag: Multimodal AI",I:!1}}],["/tag/ai-reasoning/",{loader:()=>a.e(1441).then(a.bind(a,6644)),meta:{t:"Tag: AI reasoning",I:!1}}],["/tag/diffusion-models/",{loader:()=>a.e(1318).then(a.bind(a,693)),meta:{t:"Tag: Diffusion Models",I:!1}}],["/tag/modelmerging/",{loader:()=>a.e(810).then(a.bind(a,3977)),meta:{t:"Tag: ModelMerging",I:!1}}],["/zh/tag/",{loader:()=>a.e(5230).then(a.bind(a,683)),meta:{t:"标签",I:!1}}],["/zh/tag/%E7%A6%81%E7%94%A8/",{loader:()=>a.e(4210).then(a.bind(a,67770)),meta:{t:"标签: 禁用",I:!1}}],["/zh/tag/%E5%8A%A0%E5%AF%86/",{loader:()=>a.e(8773).then(a.bind(a,44722)),meta:{t:"标签: 加密",I:!1}}],["/zh/tag/%E5%B8%83%E5%B1%80/",{loader:()=>a.e(400).then(a.bind(a,76657)),meta:{t:"标签: 布局",I:!1}}],["/zh/tag/markdown/",{loader:()=>a.e(8754).then(a.bind(a,26411)),meta:{t:"标签: Markdown",I:!1}}],["/zh/tag/%E9%A1%B5%E9%9D%A2%E9%85%8D%E7%BD%AE/",{loader:()=>a.e(627).then(a.bind(a,74927)),meta:{t:"标签: 页面配置",I:!1}}],["/zh/tag/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/",{loader:()=>a.e(7249).then(a.bind(a,36747)),meta:{t:"标签: 使用指南",I:!1}}],["/zh/tag/%E9%9A%8F%E6%83%B3/",{loader:()=>a.e(6209).then(a.bind(a,25924)),meta:{t:"标签: 随想",I:!1}}],["/zh/tag/%E5%BC%80%E6%BA%90/",{loader:()=>a.e(8131).then(a.bind(a,98972)),meta:{t:"标签: 开源",I:!1}}],["/zh/tag/%E5%A4%A7%E4%BC%9A/",{loader:()=>a.e(1562).then(a.bind(a,24442)),meta:{t:"标签: 大会",I:!1}}],["/zh/tag/%E5%AE%9E%E4%B9%A0/",{loader:()=>a.e(285).then(a.bind(a,52424)),meta:{t:"标签: 实习",I:!1}}],["/zh/tag/%E5%81%9A%E9%A5%AD/",{loader:()=>a.e(8614).then(a.bind(a,79412)),meta:{t:"标签: 做饭",I:!1}}],["/zh/tag/%E5%85%AC%E4%BC%97%E5%8F%B7/",{loader:()=>a.e(1775).then(a.bind(a,53344)),meta:{t:"标签: 公众号",I:!1}}],["/zh/tag/comfyui/",{loader:()=>a.e(5319).then(a.bind(a,9243)),meta:{t:"标签: ComfyUI",I:!1}}],["/zh/tag/aigc/",{loader:()=>a.e(9463).then(a.bind(a,32449)),meta:{t:"标签: AIGC",I:!1}}],["/zh/tag/llm/",{loader:()=>a.e(8030).then(a.bind(a,87807)),meta:{t:"标签: LLM",I:!1}}],["/zh/tag/workflow-generation/",{loader:()=>a.e(3291).then(a.bind(a,27987)),meta:{t:"标签: Workflow Generation",I:!1}}],["/zh/tag/reasoning-model/",{loader:()=>a.e(7685).then(a.bind(a,20818)),meta:{t:"标签: Reasoning Model",I:!1}}],["/zh/tag/framepack/",{loader:()=>a.e(4297).then(a.bind(a,17227)),meta:{t:"标签: FramePack",I:!1}}],["/zh/tag/%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90/",{loader:()=>a.e(7945).then(a.bind(a,49800)),meta:{t:"标签: 视频生成",I:!1}}],["/zh/tag/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/",{loader:()=>a.e(5099).then(a.bind(a,13002)),meta:{t:"标签: 扩散模型",I:!1}}],["/zh/tag/%E8%BE%93%E5%85%A5%E9%A2%84%E5%A4%84%E7%90%86/",{loader:()=>a.e(9039).then(a.bind(a,32090)),meta:{t:"标签: 输入预处理",I:!1}}],["/zh/tag/%E5%A4%9A%E6%A8%A1%E6%80%81/",{loader:()=>a.e(6953).then(a.bind(a,19355)),meta:{t:"标签: 多模态",I:!1}}],["/zh/tag/ovis-u1/",{loader:()=>a.e(6427).then(a.bind(a,72059)),meta:{t:"标签: Ovis-U1",I:!1}}],["/zh/tag/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4/",{loader:()=>a.e(8e3).then(a.bind(a,68869)),meta:{t:"标签: 阿里巴巴",I:!1}}],["/zh/tag/sdo/",{loader:()=>a.e(3041).then(a.bind(a,71221)),meta:{t:"标签: SDO",I:!1}}],["/zh/tag/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/",{loader:()=>a.e(2330).then(a.bind(a,93040)),meta:{t:"标签: 反向传播",I:!1}}],["/zh/tag/%E8%AE%A1%E7%AE%97%E4%BC%98%E5%8C%96/",{loader:()=>a.e(9425).then(a.bind(a,75770)),meta:{t:"标签: 计算优化",I:!1}}],["/zh/tag/%E5%8F%AF%E6%8E%A7%E7%94%9F%E6%88%90/",{loader:()=>a.e(4234).then(a.bind(a,89865)),meta:{t:"标签: 可控生成",I:!1}}],["/zh/tag/%E6%8A%80%E6%9C%AF%E6%95%99%E7%A8%8B/",{loader:()=>a.e(4422).then(a.bind(a,94207)),meta:{t:"标签: 技术教程",I:!1}}],["/zh/tag/ai%E7%94%9F%E6%88%90/",{loader:()=>a.e(4694).then(a.bind(a,34188)),meta:{t:"标签: AI生成",I:!1}}],["/zh/tag/comfymind/",{loader:()=>a.e(4813).then(a.bind(a,19330)),meta:{t:"标签: ComfyMind",I:!1}}],["/zh/tag/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/",{loader:()=>a.e(6048).then(a.bind(a,46338)),meta:{t:"标签: 开源项目",I:!1}}],["/zh/tag/%E8%89%BA%E6%9C%AF/",{loader:()=>a.e(6512).then(a.bind(a,22240)),meta:{t:"标签: 艺术",I:!1}}],["/zh/tag/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/",{loader:()=>a.e(8823).then(a.bind(a,68355)),meta:{t:"标签: 人工智能",I:!1}}],["/zh/tag/%E5%88%9B%E4%BD%9C%E8%80%85/",{loader:()=>a.e(9899).then(a.bind(a,79913)),meta:{t:"标签: 创作者",I:!1}}],["/zh/tag/gtc/",{loader:()=>a.e(3753).then(a.bind(a,24272)),meta:{t:"标签: GTC",I:!1}}],["/zh/tag/resource-guide/",{loader:()=>a.e(2678).then(a.bind(a,87645)),meta:{t:"标签: resource guide",I:!1}}],["/zh/tag/script/",{loader:()=>a.e(8052).then(a.bind(a,19772)),meta:{t:"标签: script",I:!1}}],["/zh/tag/stable-diffusion/",{loader:()=>a.e(9102).then(a.bind(a,62983)),meta:{t:"标签: stable diffusion",I:!1}}],["/zh/tag/merge/",{loader:()=>a.e(8721).then(a.bind(a,84539)),meta:{t:"标签: merge",I:!1}}],["/zh/tag/model/",{loader:()=>a.e(1300).then(a.bind(a,87003)),meta:{t:"标签: model",I:!1}}],["/zh/tag/%E7%BC%96%E8%BE%91%E5%99%A8/",{loader:()=>a.e(8438).then(a.bind(a,35226)),meta:{t:"标签: 编辑器",I:!1}}],["/zh/tag/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AE/",{loader:()=>a.e(2404).then(a.bind(a,63921)),meta:{t:"标签: 模型上下文协议",I:!1}}],["/zh/tag/%E6%8A%80%E6%9C%AF/",{loader:()=>a.e(7366).then(a.bind(a,20827)),meta:{t:"标签: 技术",I:!1}}],["/zh/tag/artificial-intelligence/",{loader:()=>a.e(7077).then(a.bind(a,7239)),meta:{t:"标签: Artificial Intelligence",I:!1}}],["/zh/tag/inference/",{loader:()=>a.e(1706).then(a.bind(a,11063)),meta:{t:"标签: Inference",I:!1}}],["/zh/tag/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/",{loader:()=>a.e(7443).then(a.bind(a,97350)),meta:{t:"标签: 图像生成",I:!1}}],["/zh/tag/flux/",{loader:()=>a.e(1276).then(a.bind(a,38307)),meta:{t:"标签: FLUX",I:!1}}],["/zh/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/",{loader:()=>a.e(8311).then(a.bind(a,157)),meta:{t:"标签: 机器学习",I:!1}}],["/zh/tag/flux.1/",{loader:()=>a.e(3481).then(a.bind(a,57812)),meta:{t:"标签: FLUX.1",I:!1}}],["/zh/tag/%E4%BC%98%E5%8C%96/",{loader:()=>a.e(7053).then(a.bind(a,48396)),meta:{t:"标签: 优化",I:!1}}],["/zh/tag/taylorseer/",{loader:()=>a.e(523).then(a.bind(a,71091)),meta:{t:"标签: TaylorSeer",I:!1}}],["/zh/tag/replicate/",{loader:()=>a.e(2666).then(a.bind(a,96257)),meta:{t:"标签: Replicate",I:!1}}],["/zh/tag/lora/",{loader:()=>a.e(583).then(a.bind(a,79342)),meta:{t:"标签: LoRA",I:!1}}],["/zh/tag/qlora/",{loader:()=>a.e(6778).then(a.bind(a,75085)),meta:{t:"标签: QLoRA",I:!1}}],["/zh/tag/%E5%BE%AE%E8%B0%83/",{loader:()=>a.e(8768).then(a.bind(a,31784)),meta:{t:"标签: 微调",I:!1}}],["/zh/tag/diffusers/",{loader:()=>a.e(980).then(a.bind(a,68490)),meta:{t:"标签: diffusers",I:!1}}],["/zh/tag/%E9%87%8F%E5%8C%96/",{loader:()=>a.e(798).then(a.bind(a,3813)),meta:{t:"标签: 量化",I:!1}}],["/zh/tag/%E7%94%9F%E6%88%90%E5%BC%8Fai/",{loader:()=>a.e(8255).then(a.bind(a,7576)),meta:{t:"标签: 生成式AI",I:!1}}],["/zh/tag/%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/",{loader:()=>a.e(1061).then(a.bind(a,76184)),meta:{t:"标签: 游戏开发",I:!1}}],["/zh/tag/stable-diffusion/",{loader:()=>a.e(9102).then(a.bind(a,62983)),meta:{t:"标签: Stable Diffusion",I:!1}}],["/zh/tag/aws/",{loader:()=>a.e(1232).then(a.bind(a,97529)),meta:{t:"标签: AWS",I:!1}}],["/zh/tag/amazon-bedrock/",{loader:()=>a.e(1872).then(a.bind(a,82732)),meta:{t:"标签: Amazon Bedrock",I:!1}}],["/zh/tag/illustrious/",{loader:()=>a.e(9492).then(a.bind(a,63986)),meta:{t:"标签: Illustrious",I:!1}}],["/zh/tag/lu/",{loader:()=>a.e(1991).then(a.bind(a,28963)),meta:{t:"标签: LU",I:!1}}],["/zh/tag/lumina/",{loader:()=>a.e(9347).then(a.bind(a,29468)),meta:{t:"标签: Lumina",I:!1}}],["/zh/tag/ai%E6%A8%A1%E5%9E%8B/",{loader:()=>a.e(4749).then(a.bind(a,12422)),meta:{t:"标签: AI模型",I:!1}}],["/zh/tag/%E8%AE%AD%E7%BB%83/",{loader:()=>a.e(4967).then(a.bind(a,3120)),meta:{t:"标签: 训练",I:!1}}],["/zh/tag/sdxl/",{loader:()=>a.e(320).then(a.bind(a,55243)),meta:{t:"标签: SDXL",I:!1}}],["/zh/tag/2048%E5%88%86%E8%BE%A8%E7%8E%87/",{loader:()=>a.e(7848).then(a.bind(a,60686)),meta:{t:"标签: 2048分辨率",I:!1}}],["/zh/tag/vpred/",{loader:()=>a.e(4996).then(a.bind(a,10875)),meta:{t:"标签: vpred",I:!1}}],["/zh/tag/epsilon%E9%A2%84%E6%B5%8B/",{loader:()=>a.e(4198).then(a.bind(a,96556)),meta:{t:"标签: epsilon预测",I:!1}}],["/zh/tag/%E5%8A%A8%E6%BC%AB/",{loader:()=>a.e(506).then(a.bind(a,99923)),meta:{t:"标签: 动漫",I:!1}}],["/zh/tag/%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B/",{loader:()=>a.e(3621).then(a.bind(a,14355)),meta:{t:"标签: 基础模型",I:!1}}],["/zh/tag/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/",{loader:()=>a.e(7162).then(a.bind(a,24434)),meta:{t:"标签: 计算机视觉",I:!1}}],["/zh/tag/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/",{loader:()=>a.e(2259).then(a.bind(a,58087)),meta:{t:"标签: 图像识别",I:!1}}],["/zh/tag/prompt/",{loader:()=>a.e(1941).then(a.bind(a,19743)),meta:{t:"标签: Prompt",I:!1}}],["/zh/tag/text2image/",{loader:()=>a.e(7095).then(a.bind(a,55959)),meta:{t:"标签: text2image",I:!1}}],["/zh/tag/ai/",{loader:()=>a.e(361).then(a.bind(a,20235)),meta:{t:"标签: AI",I:!1}}],["/zh/tag/%E5%8D%8F%E8%AE%AE/",{loader:()=>a.e(1702).then(a.bind(a,71741)),meta:{t:"标签: 协议",I:!1}}],["/zh/tag/%E8%BE%A9%E8%AE%BA/",{loader:()=>a.e(1540).then(a.bind(a,547)),meta:{t:"标签: 辩论",I:!1}}],["/zh/tag/art/",{loader:()=>a.e(7012).then(a.bind(a,11134)),meta:{t:"标签: Art",I:!1}}],["/zh/tag/drawing/",{loader:()=>a.e(9147).then(a.bind(a,66962)),meta:{t:"标签: Drawing",I:!1}}],["/zh/tag/fundamentals/",{loader:()=>a.e(863).then(a.bind(a,87349)),meta:{t:"标签: Fundamentals",I:!1}}],["/zh/tag/niji/",{loader:()=>a.e(2643).then(a.bind(a,58370)),meta:{t:"标签: niji",I:!1}}],["/zh/tag/%E8%89%BA%E6%9C%AF%E8%AF%BE%E7%A8%8B/",{loader:()=>a.e(1535).then(a.bind(a,61125)),meta:{t:"标签: 艺术课程",I:!1}}],["/zh/tag/notan/",{loader:()=>a.e(6560).then(a.bind(a,85052)),meta:{t:"标签: Notan",I:!1}}],["/zh/tag/%E5%8A%9F%E8%83%BD%E5%8F%91%E5%B8%83/",{loader:()=>a.e(2183).then(a.bind(a,68670)),meta:{t:"标签: 功能发布",I:!1}}],["/zh/tag/kohya-ss-gui/",{loader:()=>a.e(3496).then(a.bind(a,32331)),meta:{t:"标签: Kohya SS GUI",I:!1}}],["/zh/tag/qwen/",{loader:()=>a.e(6016).then(a.bind(a,856)),meta:{t:"标签: Qwen",I:!1}}],["/zh/tag/qwen3/",{loader:()=>a.e(6937).then(a.bind(a,18614)),meta:{t:"标签: Qwen3",I:!1}}],["/zh/tag/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4ai%E7%A0%94%E7%A9%B6/",{loader:()=>a.e(220).then(a.bind(a,72375)),meta:{t:"标签: 阿里巴巴AI研究",I:!1}}],["/zh/tag/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4ai/",{loader:()=>a.e(7466).then(a.bind(a,63402)),meta:{t:"标签: 阿里巴巴AI",I:!1}}],["/zh/tag/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/",{loader:()=>a.e(5073).then(a.bind(a,18850)),meta:{t:"标签: 大语言模型",I:!1}}],["/zh/tag/llms/",{loader:()=>a.e(5047).then(a.bind(a,14814)),meta:{t:"标签: LLMs",I:!1}}],["/zh/tag/llm%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95/",{loader:()=>a.e(6322).then(a.bind(a,93357)),meta:{t:"标签: LLM基准测试",I:!1}}],["/zh/tag/%E5%A4%9A%E8%AF%AD%E8%A8%80ai/",{loader:()=>a.e(1992).then(a.bind(a,60760)),meta:{t:"标签: 多语言AI",I:!1}}],["/zh/tag/%E5%A4%9A%E6%A8%A1%E6%80%81ai/",{loader:()=>a.e(3135).then(a.bind(a,86270)),meta:{t:"标签: 多模态AI",I:!1}}],["/zh/tag/ai%E6%8E%A8%E7%90%86/",{loader:()=>a.e(1823).then(a.bind(a,85981)),meta:{t:"标签: AI推理",I:!1}}],["/zh/tag/mcp/",{loader:()=>a.e(8669).then(a.bind(a,40886)),meta:{t:"标签: MCP",I:!1}}],["/zh/tag/github/",{loader:()=>a.e(4735).then(a.bind(a,67292)),meta:{t:"标签: GitHub",I:!1}}],["/zh/tag/%E5%B9%B3%E5%8F%B0/",{loader:()=>a.e(6528).then(a.bind(a,77447)),meta:{t:"标签: 平台",I:!1}}],["/zh/tag/%E8%BF%90%E8%90%A5/",{loader:()=>a.e(5912).then(a.bind(a,78442)),meta:{t:"标签: 运营",I:!1}}],["/zh/tag/%E5%86%85%E5%AE%B9/",{loader:()=>a.e(8021).then(a.bind(a,20481)),meta:{t:"标签: 内容",I:!1}}],["/zh/tag/%E5%88%9B%E6%96%B0/",{loader:()=>a.e(5416).then(a.bind(a,51284)),meta:{t:"标签: 创新",I:!1}}],["/article/",{loader:()=>a.e(7511).then(a.bind(a,2772)),meta:{t:"Articles",I:!1}}],["/zh/article/",{loader:()=>a.e(716).then(a.bind(a,86654)),meta:{t:"文章",I:!1}}],["/star/",{loader:()=>a.e(7199).then(a.bind(a,70353)),meta:{t:"Star",I:!1}}],["/zh/star/",{loader:()=>a.e(7774).then(a.bind(a,85588)),meta:{t:"星标",I:!1}}],["/timeline/",{loader:()=>a.e(5464).then(a.bind(a,14695)),meta:{t:"Timeline",I:!1}}],["/zh/timeline/",{loader:()=>a.e(8229).then(a.bind(a,90903)),meta:{t:"时间轴",I:!1}}]])},58761:(e,t,a)=>{a.d(t,{U:()=>n});const n=JSON.parse('{"base":"/","lang":"en-US","title":"","description":"","head":[],"locales":{"/":{"lang":"en-US","title":"Nlog","description":"A blog of neverbiasu"},"/zh/":{"lang":"zh-CN","title":"Nlog","description":"neverbiasu 的博客"}}}')},26653:(e,t,a)=>{a.d(t,{K:()=>n});const n=JSON.parse('{"encrypt":{"config":{"/demo/encrypt.html":["$2a$10$hjZrjfyuENvDp.CsVFyrqO.NIyMix5AlfoDpe84AE5SygzDfAJmmu"],"/zh/demo/encrypt.html":["$2a$10$T8kpTE6KA0FVfYkvPLovle2UV2B0TT9zzl0.9Tt8soeW22SMpKITe"]}},"author":{"name":"neverbiasu","url":"https://neverbiasu.github.io"},"logo":"logo.svg","repo":"vuepress-theme-hope/vuepress-theme-hope","docsDir":"src","blog":{"medias":{"BiliBili":"https://space.bilibili.com/342773888","Email":"neverbiasu@gmail.com","GitHub":"https://github.com/neverbiasu","Gmail":"neverbiasu@gmail.com","Instagram":"https://instagram.com","CodeWithGPU":{"icon":"https://raw.githubusercontent.com/neverbiasu/blog/refs/heads/main/src/.vuepress/public/assets/icon/codewithgpu.png","link":"https://www.codewithgpu.com/u/fayche"},"ModelScope":{"icon":"https://raw.githubusercontent.com/lobehub/lobe-icons/7dabb828beb43f56512b054447753491366a4197/packages/static-svg/icons/modelscope-color.svg","link":"https://modelscope.cn/profile/ModelE"},"Civitai":{"icon":"https://raw.githubusercontent.com/lobehub/lobe-icons/7dabb828beb43f56512b054447753491366a4197/packages/static-svg/icons/civitai-color.svg","link":"https://civitai.com/user/Fetch267"},"HuggingFace":{"icon":"https://huggingface.co/front/assets/huggingface_logo.svg","link":"https://huggingface.co/6chan"}}},"locales":{"/zh/":{"lang":"zh-CN","navbarLocales":{"langName":"简体中文","selectLangAriaLabel":"选择语言"},"metaLocales":{"author":"作者","date":"写作日期","origin":"原创","views":"访问量","category":"分类","tag":"标签","readingTime":"阅读时间","words":"字数","toc":"此页内容","prev":"上一页","next":"下一页","lastUpdated":"上次编辑于","contributors":"贡献者","editLink":"在 GitHub 上编辑此页","print":"打印"},"blogLocales":{"article":"文章","articleList":"文章列表","category":"分类","tag":"标签","timeline":"时间轴","timelineTitle":"昨日不在","all":"全部","intro":"个人介绍","star":"星标","empty":"$text 为空"},"paginationLocales":{"prev":"上一页","next":"下一页","navigate":"跳转到","action":"前往","errorText":"请输入 1 到 $page 之前的页码！"},"outlookLocales":{"themeColor":"主题色","darkmode":"外观","fullscreen":"全屏"},"encryptLocales":{"iconLabel":"文章已加密","placeholder":"输入密码","remember":"记住密码","errorHint":"请输入正确的密码"},"routeLocales":{"skipToContent":"跳至主要內容","notFoundTitle":"页面不存在","notFoundMsg":["这里什么也没有","我们是怎么来到这儿的？","这 是 四 零 四 !","看起来你访问了一个失效的链接"],"back":"返回上一页","home":"带我回家"},"navbar":["/zh/","/zh/demo/",{"text":"博客","icon":"pen-to-square","prefix":"/zh/posts/","children":[{"text":"ai-impls","icon":"pen-to-square","prefix":"ai-impls/","children":[{"text":"yolov9","icon":"pen-to-square","link":"yolov9"}]},{"text":"ai-weekly","icon":"pen-to-square","prefix":"ai-weekly/","children":[{"text":"001","icon":"pen-to-square","link":"001"},{"text":"002","icon":"pen-to-square","link":"002"},{"text":"003","icon":"pen-to-square","link":"003"},{"text":"004","icon":"pen-to-square","link":"004"},{"text":"005","icon":"pen-to-square","link":"005"},{"text":"006","icon":"pen-to-square","link":"006"},{"text":"007","icon":"pen-to-square","link":"007"},{"text":"008","icon":"pen-to-square","link":"008"},{"text":"009","icon":"pen-to-square","link":"009"},{"text":"010","icon":"pen-to-square","link":"010"},{"text":"011","icon":"pen-to-square","link":"011"},{"text":"012","icon":"pen-to-square","link":"012"},{"text":"013","icon":"pen-to-square","link":"013"},{"text":"014","icon":"pen-to-square","link":"014"},{"text":"015","icon":"pen-to-square","link":"015"},{"text":"016","icon":"pen-to-square","link":"016"},{"text":"017","icon":"pen-to-square","link":"017"},{"text":"018","icon":"pen-to-square","link":"018"},{"text":"019","icon":"pen-to-square","link":"019"},{"text":"020","icon":"pen-to-square","link":"020"},{"text":"021","icon":"pen-to-square","link":"021"},{"text":"022","icon":"pen-to-square","link":"022"},{"text":"023","icon":"pen-to-square","link":"023"},{"text":"024","icon":"pen-to-square","link":"024"},{"text":"025","icon":"pen-to-square","link":"025"},{"text":"026","icon":"pen-to-square","link":"026"},{"text":"027","icon":"pen-to-square","link":"027"},{"text":"028","icon":"pen-to-square","link":"028"},{"text":"029","icon":"pen-to-square","link":"029"},{"text":"030","icon":"pen-to-square","link":"030"},{"text":"031","icon":"pen-to-square","link":"031"},{"text":"032","icon":"pen-to-square","link":"032"},{"text":"033","icon":"pen-to-square","link":"033"},{"text":"034","icon":"pen-to-square","link":"034"},{"text":"035","icon":"pen-to-square","link":"035"},{"text":"036","icon":"pen-to-square","link":"036"},{"text":"037","icon":"pen-to-square","link":"037"},{"text":"038","icon":"pen-to-square","link":"038"},{"text":"039","icon":"pen-to-square","link":"039"},{"text":"040","icon":"pen-to-square","link":"040"},{"text":"041","icon":"pen-to-square","link":"041"},{"text":"042","icon":"pen-to-square","link":"042"},{"text":"043","icon":"pen-to-square","link":"043"},{"text":"044","icon":"pen-to-square","link":"044"},{"text":"045","icon":"pen-to-square","link":"045"},{"text":"046","icon":"pen-to-square","link":"046"},{"text":"047","icon":"pen-to-square","link":"047"},{"text":"X01","icon":"pen-to-square","link":"X01"}]},{"text":"dairys","icon":"pen-to-square","prefix":"dairys/","children":[{"text":"250222","icon":"pen-to-square","link":"250222"},{"text":"250223","icon":"pen-to-square","link":"250223"}]},{"text":"hf-weekly","icon":"pen-to-square","prefix":"hf-weekly/","children":[{"text":"001","icon":"pen-to-square","link":"001"},{"text":"002","icon":"pen-to-square","link":"002"},{"text":"003","icon":"pen-to-square","link":"003"},{"text":"004","icon":"pen-to-square","link":"004"}]},{"text":"ielts","icon":"pen-to-square","prefix":"ielts/","children":[{"text":"simon-task1","icon":"pen-to-square","link":"simon-task1"},{"text":"simon-task2","icon":"pen-to-square","link":"simon-task2"}]},{"text":"papers","icon":"pen-to-square","prefix":"papers/","children":[{"text":"3steps-paper-reading","icon":"pen-to-square","link":"3steps-paper-reading"},{"text":"alexnet","icon":"pen-to-square","link":"alexnet"},{"text":"bagel","icon":"pen-to-square","link":"bagel"},{"text":"blip-3o","icon":"pen-to-square","link":"blip-3o"},{"text":"colorizediffusion","icon":"pen-to-square","link":"colorizediffusion"},{"text":"comfyui-r1","icon":"pen-to-square","link":"comfyui-r1"},{"text":"dgpst","icon":"pen-to-square","link":"dgpst"},{"text":"ecomimic-v3","icon":"pen-to-square","link":"ecomimic-v3"},{"text":"flux-kontext","icon":"pen-to-square","link":"flux-kontext"},{"text":"framepack","icon":"pen-to-square","link":"framepack"},{"text":"hunyuancustom","icon":"pen-to-square","link":"hunyuancustom"},{"text":"icedit","icon":"pen-to-square","link":"icedit"},{"text":"ming-omni","icon":"pen-to-square","link":"ming-omni"},{"text":"nhr","icon":"pen-to-square","link":"nhr"},{"text":"omniconsistency","icon":"pen-to-square","link":"omniconsistency"},{"text":"omnigen2","icon":"pen-to-square","link":"omnigen2"},{"text":"ovis-u1","icon":"pen-to-square","link":"ovis-u1"},{"text":"qr-lora","icon":"pen-to-square","link":"qr-lora"},{"text":"reptext","icon":"pen-to-square","link":"reptext"},{"text":"resnet","icon":"pen-to-square","link":"resnet"},{"text":"sdo","icon":"pen-to-square","link":"sdo"},{"text":"show-o2","icon":"pen-to-square","link":"show-o2"},{"text":"transformer","icon":"pen-to-square","link":"transformer"},{"text":"vlv","icon":"pen-to-square","link":"vlv"}]},{"text":"prompts","icon":"pen-to-square","prefix":"prompts/","children":[{"text":"ai-weekly.prompt","icon":"pen-to-square","link":"ai-weekly.prompt"},{"text":"cover.prompt","icon":"pen-to-square","link":"cover.prompt"},{"text":"hf-weekly.prompt","icon":"pen-to-square","link":"hf-weekly.prompt"},{"text":"image-extract.prompt","icon":"pen-to-square","link":"image-extract.prompt"},{"text":"papers.prompt","icon":"pen-to-square","link":"papers.prompt"},{"text":"translate.prompt","icon":"pen-to-square","link":"translate.prompt"}]},{"text":"repos","icon":"pen-to-square","prefix":"repos/","children":[{"text":"comfy-mind","icon":"pen-to-square","link":"comfy-mind"}]},{"text":"reprints","icon":"pen-to-square","prefix":"reprints/","children":[{"text":"ai-art-gtc-paris-2025","icon":"pen-to-square","link":"ai-art-gtc-paris-2025"},{"text":"ai-art-newsletter-jan-25","icon":"pen-to-square","link":"ai-art-newsletter-jan-25"},{"text":"announcing-illustrious-text‑enhancer-tag-booster-and-mood-enhancer","icon":"pen-to-square","link":"announcing-illustrious-text‑enhancer-tag-booster-and-mood-enhancer"},{"text":"crody\'s-model-merge-guide","icon":"pen-to-square","link":"crody\'s-model-merge-guide"},{"text":"experiments-with-mcp-using-github-copilot","icon":"pen-to-square","link":"experiments-with-mcp-using-github-copilot"},{"text":"explaining-tokens-the-language-and-currency-of-ai-nvidia-blog","icon":"pen-to-square","link":"explaining-tokens-the-language-and-currency-of-ai-nvidia-blog"},{"text":"flux-1-kontext","icon":"pen-to-square","link":"flux-1-kontext"},{"text":"flux-kontext-optimization","icon":"pen-to-square","link":"flux-kontext-optimization"},{"text":"flux-qlora","icon":"pen-to-square","link":"flux-qlora"},{"text":"generative-ai-powered-design","icon":"pen-to-square","link":"generative-ai-powered-design"},{"text":"how-and-when-to-build-multi-agent-systems","icon":"pen-to-square","link":"how-and-when-to-build-multi-agent-systems"},{"text":"illustrious-lu-v0.03","icon":"pen-to-square","link":"illustrious-lu-v0.03"},{"text":"illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language","icon":"pen-to-square","link":"illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language"},{"text":"illustrious-xl-v2.0-the-best-training-base-model-in-1536-age","icon":"pen-to-square","link":"illustrious-xl-v2.0-the-best-training-base-model-in-1536-age"},{"text":"image-recognition","icon":"pen-to-square","link":"image-recognition"},{"text":"introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression","icon":"pen-to-square","link":"introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression"},{"text":"mcp-flash-in-the-pan-or-future-standard","icon":"pen-to-square","link":"mcp-flash-in-the-pan-or-future-standard"},{"text":"niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything","icon":"pen-to-square","link":"niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything"},{"text":"niji-lesson-2-the-terminator-line","icon":"pen-to-square","link":"niji-lesson-2-the-terminator-line"},{"text":"niji-study-1-measuring-with-your-eyes","icon":"pen-to-square","link":"niji-study-1-measuring-with-your-eyes"},{"text":"niji-study-2-notan","icon":"pen-to-square","link":"niji-study-2-notan"},{"text":"niji-video","icon":"pen-to-square","link":"niji-video"},{"text":"original-character-lora-sdxl-character-training","icon":"pen-to-square","link":"original-character-lora-sdxl-character-training"},{"text":"qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview","icon":"pen-to-square","link":"qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview"},{"text":"step-by-step-visual-introduction-to-diffusion-models","icon":"pen-to-square","link":"step-by-step-visual-introduction-to-diffusion-models"}]},{"text":"sci","icon":"pen-to-square","prefix":"sci/","children":[{"text":"conda","icon":"pen-to-square","link":"conda"}]},{"text":"templates","icon":"pen-to-square","prefix":"templates/","children":[{"text":"ai-weekly","icon":"pen-to-square","link":"ai-weekly"},{"text":"hf-weekly","icon":"pen-to-square","link":"hf-weekly"},{"text":"papers","icon":"pen-to-square","link":"papers"},{"text":"repos","icon":"pen-to-square","link":"repos"}]},{"text":"thoughts","icon":"pen-to-square","prefix":"thoughts/","children":[{"text":"platform-operation-thoughts-after-comfycon","icon":"pen-to-square","link":"platform-operation-thoughts-after-comfycon"}]},{"text":"tutorials","icon":"pen-to-square","prefix":"tutorials/","children":[{"text":"qwen-code","icon":"pen-to-square","link":"qwen-code"}]},{"text":"web","icon":"pen-to-square","prefix":"web/","children":[{"text":"vue-1","icon":"pen-to-square","link":"vue-1"}]},{"text":"workflows","icon":"pen-to-square","prefix":"workflows/","children":[]}]}],"sidebar":{"/zh/":["",{"text":"如何使用","icon":"laptop-code","prefix":"demo/","link":"demo/","children":"structure"},{"text":"文章","icon":"book","prefix":"posts/","children":"structure"},"intro",{"text":"幻灯片","icon":"person-chalkboard","link":"https://ecosystem.vuejs.press/zh/plugins/markdown/revealjs/demo.html"}]},"footer":"我的页脚","displayFooter":true,"blog":{"description":"一个人工智能学习者，前端开发者","intro":"/zh/intro.html"}},"/":{"lang":"en-US","navbarLocales":{"langName":"English","selectLangAriaLabel":"Select language"},"metaLocales":{"author":"Author","date":"Writing Date","origin":"Original","views":"Page views","category":"Category","tag":"Tag","readingTime":"Reading Time","words":"Words","toc":"On This Page","prev":"Prev","next":"Next","lastUpdated":"Last update","contributors":"Contributors","editLink":"Edit this page on GitHub","print":"Print"},"blogLocales":{"article":"Articles","articleList":"Article List","category":"Category","tag":"Tag","timeline":"Timeline","timelineTitle":"Yesterday Once More!","all":"All","intro":"Personal Intro","star":"Star","empty":"No $text"},"paginationLocales":{"prev":"Prev","next":"Next","navigate":"Jump to","action":"Go","errorText":"Please enter a number between 1 and $page !"},"outlookLocales":{"themeColor":"Theme Color","darkmode":"Theme Mode","fullscreen":"Full Screen"},"encryptLocales":{"iconLabel":"Page Encrypted","placeholder":"Enter password","remember":"Remember password","errorHint":"Please enter the correct password!"},"routeLocales":{"skipToContent":"Skip to main content","notFoundTitle":"Page not found","notFoundMsg":["There’s nothing here.","How did we get here?","That’s a Four-Oh-Four.","Looks like we\'ve got some broken links."],"back":"Go back","home":"Take me home"},"navbar":[{"text":"home","link":"/home"},"/demo/",{"text":"Posts","icon":"pen-to-square","prefix":"/posts/","children":[{"text":"Apple","icon":"pen-to-square","prefix":"apple/","children":[{"text":"Apple1","icon":"pen-to-square","link":"1"},{"text":"Apple2","icon":"pen-to-square","link":"2"},"3","4"]},{"text":"Banana","icon":"pen-to-square","prefix":"banana/","children":[{"text":"Banana 1","icon":"pen-to-square","link":"1"},{"text":"Banana 2","icon":"pen-to-square","link":"2"},"3","4"]},{"text":"Cherry","icon":"pen-to-square","link":"cherry"},{"text":"Dragon Fruit","icon":"pen-to-square","link":"dragonfruit"},"tomato","strawberry"]}],"sidebar":{"/":["",{"text":"Demo","icon":"laptop-code","prefix":"demo/","link":"demo/","children":"structure"},{"text":"Articles","icon":"book","prefix":"posts/","children":"structure"},"intro",{"text":"Slides","icon":"person-chalkboard","link":"https://ecosystem.vuejs.press/plugins/markdown/revealjs/demo.html"}]},"footer":"My own footer","displayFooter":true,"blog":{"description":"An AI student && A FrontEnd programmer","intro":"/intro.html"}}}}')},74923:(e,t,a)=>{},38123:(e,t,a)=>{a.d(t,{v:()=>n});const n={"/zh/demo/":["markdown","layout","page","disable","encrypt"],"/zh/posts/":[{text:"Ai Impls",prefix:"ai-impls/",collapsible:!0,children:["yolov9"]},{text:"Ai Weekly",prefix:"ai-weekly/",collapsible:!0,children:["X01","001","017","028","020","038","023","036","031","022","044","016","010","035","047","045","034","005","013","025","014","006","024","042","021","033","039","040","027","037","004","018","030","003","009","026","019","046","012","032","002","041","043","029","015","011","007","008"]},{text:"Dairys",prefix:"dairys/",collapsible:!0,children:["250222","250223"]},{text:"Hf Weekly",prefix:"hf-weekly/",collapsible:!0,children:["001","002","004","003"]},{text:"Ielts",prefix:"ielts/",collapsible:!0,children:["simon-task2",{text:"Usage of Collocations in Speaking",prefix:"usage-of-collocations-in-speaking/",collapsible:!0,children:["ielts-collocations"]},"simon-task1"]},{text:"Papers",prefix:"papers/",collapsible:!0,children:["alexnet","bagel","blip-3o","colorizediffusion","comfyui-r1","dgpst","ecomimic-v3","flux-kontext","framepack","hunyuancustom","icedit","ming-omni","nhr","omniconsistency","omnigen2","ovis-u1","qr-lora","reptext","resnet","sdo","show-o2","transformer","vlv","3steps-paper-reading"]},{text:"Prompts",prefix:"prompts/",collapsible:!0,children:["ai-weekly.prompt","cover.prompt","hf-weekly.prompt","image-extract.prompt","papers.prompt","translate.prompt"]},{text:"Repos",prefix:"repos/",collapsible:!0,children:["comfy-mind"]},{text:"Reprints",prefix:"reprints/",collapsible:!0,children:["mcp-flash-in-the-pan-or-future-standard","generative-ai-powered-design","niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything","niji-study-1-measuring-with-your-eyes","niji-study-2-notan","introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression","ai-art-newsletter-jan-25","crody's-model-merge-guide","step-by-step-visual-introduction-to-diffusion-models","illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language","illustrious-xl-v2.0-the-best-training-base-model-in-1536-age","illustrious-lu-v0.03","qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview","image-recognition","experiments-with-mcp-using-github-copilot","announcing-illustrious-text‑enhancer-tag-booster-and-mood-enhancer","flux-qlora","how-and-when-to-build-multi-agent-systems","niji-video","original-character-lora-sdxl-character-training","flux-kontext-optimization","ai-art-gtc-paris-2025","explaining-tokens-the-language-and-currency-of-ai-nvidia-blog","niji-lesson-2-the-terminator-line","flux-1-kontext"]},{text:"Sci",prefix:"sci/",collapsible:!0,children:["conda"]},{text:"Templates",prefix:"templates/",collapsible:!0,children:["ai-weekly","hf-weekly","papers","repos"]},{text:"Thoughts",prefix:"thoughts/",collapsible:!0,children:["platform-operation-thoughts-after-comfycon"]},{text:"Tutorials",prefix:"tutorials/",collapsible:!0,children:["qwen-code",{text:"Comfyui",prefix:"comfyui/",collapsible:!0,children:["flux-kontext-beginner"]}]},{text:"Web",prefix:"web/",collapsible:!0,children:["vue-1"]},{text:"Workflows",prefix:"workflows/",collapsible:!0,children:[{text:"Hf Weekly",prefix:"hf-weekly/",collapsible:!0,children:["workflow","checklist"]},{text:"Papers",prefix:"papers/",collapsible:!0,children:["workflow","checklist"]},{text:"Repos",prefix:"repos/",collapsible:!0,children:["checklist","material","workflow"]}]}],"/demo/":["layout","markdown","page","disable","encrypt"],"/posts/":[{text:"Reprints",prefix:"reprints/",collapsible:!0,children:["generative-ai-powered-design","niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything","mcp-flash-in-the-pan-or-future-standard","niji-lesson-2-the-terminator-line","niji-study-1-measuring-with-your-eyes","niji-study-2-notan","flux-qlora","model-block-merge-1","model-block-merge-2","announcing-illustrious-text‑enhancer-tag-booster-and-mood-enhancer","ai-art-gtc-paris-2025","blog-images","news-agents-daily-recap","crody's-model-merge-guide","experimenting-with-mcp-using-github-copilot","explaining-tokens-the-language-and-currency-of-ai-nvidia-blog","how-and-when-to-build-multi-agent-systems","original-character-lora-sdxl-character-training","niji-video","flux-kontext-optimization","illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language","illustrious-xl-v2.0-the-best-training-base-model-in-1536-age","illustrious-lu-v0.03","flux-1-kontext","introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression","qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview","step-by-step-visual-introduction-to-diffusion-models","ai-art-newsletter-jan-25","what-is-block-merging","image-recognition"]}]}},32878:(e,t,a)=>{a.d(t,{P:()=>n});const n={BiliBili:'<svg xmlns="http://www.w3.org/2000/svg" class="icon bilibili-icon" viewBox="0 0 1024 1024"><circle cx="512" cy="512" r="512" fill="#1296db"/><path fill="#fff" d="M745.363 177.725a47 47 0 0 1 0 66.3L702.5 286.85h44A141 141 0 0 1 887 427.512v281.25a141 141 0 0 1-141 140.626H277.25A141 141 0 0 1 137 708.763v-281.25a141 141 0 0 1 141-141h43.725l-42.788-42.825a47 47 0 1 1 66.263-66.3l99.45 99.45c2.963 2.962 5.438 6.187 7.425 9.637h120.487c1.988-3.45 4.5-6.75 7.463-9.675l99.413-99.45a47 47 0 0 1 66.3 0zm1.012 203.25h-468.75a47 47 0 0 0-46.763 43.388l-.112 3.525v281.25c0 24.712 19.125 44.962 43.387 46.724l3.488.15h468.75a47 47 0 0 0 46.763-43.387l.112-3.487v-281.25c0-26-21-47-47-46.876zm-375 93.75c26 0 47 21 47 47v47a47 47 0 1 1-93.75 0V521.6c0-26 21-47 47-47zm281.25 0c26 0 47 21 47 47v47a47 47 0 1 1-93.75 0V521.6c0-26 21-47 47-47z"/></svg>',Email:'<svg xmlns="http://www.w3.org/2000/svg" class="icon email-icon" viewBox="0 0 1024 1024"><circle cx="512" cy="512" r="512" fill="#1384FF"/><path fill="#fff" d="M270.077 286.233H751.99c32.933 0 59.86 24.855 60.274 55.51l-301.023 157L210.217 341.88c.207-30.723 26.927-55.717 59.86-55.717zm-59.929 115.714-.276 277.756c0 30.931 27.134 56.2 60.205 56.2H751.99c33.14 0 60.274-25.269 60.274-56.2V401.81L518.283 551.492a15.88 15.88 0 0 1-14.43 0L210.148 401.947z"/></svg>',GitHub:'<svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024"><circle cx="512" cy="512" r="512" fill="#171515"/><path fill="#fff" d="M509.423 146.442c-200.317 0-362.756 162.42-362.756 362.8 0 160.266 103.936 296.24 248.109 344.217 18.139 3.327 24.76-7.872 24.76-17.486 0-8.613-.313-31.427-.49-61.702-100.912 21.923-122.205-48.63-122.205-48.63-16.495-41.91-40.28-53.067-40.28-53.067-32.937-22.51 2.492-22.053 2.492-22.053 36.407 2.566 55.568 37.386 55.568 37.386 32.362 55.438 84.907 39.43 105.58 30.143 3.296-23.444 12.667-39.43 23.032-48.498-80.557-9.156-165.246-40.28-165.246-179.297 0-39.604 14.135-71.988 37.342-97.348-3.731-9.178-16.18-46.063 3.556-96.009 0 0 30.46-9.754 99.76 37.19 28.937-8.048 59.97-12.071 90.823-12.211 30.807.14 61.843 4.165 90.822 12.21 69.26-46.944 99.663-37.189 99.663-37.189 19.792 49.946 7.34 86.831 3.61 96.01 23.25 25.359 37.29 57.742 37.29 97.347 0 139.366-84.82 170.033-165.637 179.013 13.026 11.2 24.628 33.342 24.628 67.182 0 48.498-.445 87.627-.445 99.521 0 9.702 6.535 20.988 24.945 17.444 144.03-48.067 247.881-183.95 247.881-344.175 0-200.378-162.442-362.798-362.802-362.798z"/></svg>',Gmail:'<svg xmlns="http://www.w3.org/2000/svg" class="icon gmail-icon" viewBox="0 0 1024 1024"><circle cx="512" cy="512" r="512" fill="#DB4437"/><path fill="#E67C73" d="M277.48 285.567h465.767v441.362H277.48V285.567z"/><path fill="#FFF" d="M282.543 285.567h-10.645c-25.962 0-47.122 21.808-47.122 48.705v343.952c0 26.897 21.08 48.705 47.122 48.705h24.976V407.954l213.49 169.95 213.489-169.95V726.93h24.975c26.04 0 47.123-21.809 47.123-48.705V334.272c0-26.897-21.134-48.705-47.123-48.705h-10.644L510.364 480.44 282.542 285.567z"/></svg>',Instagram:'<svg xmlns="http://www.w3.org/2000/svg" class="icon instagram-icon" viewBox="0 0 1024 1024"><circle cx="512" cy="512" r="512" fill="#181818"/><path fill="#fff" d="M512 348.16c-88.222 0-163.84 71.417-163.84 163.84 0 88.222 71.417 163.84 163.84 163.84 88.222 0 163.84-71.417 163.84-163.84 0-88.222-75.618-163.84-163.84-163.84zm0 268.866c-58.814 0-105.026-46.212-105.026-105.026S453.186 406.974 512 406.974 617 453.186 617 512s-46.186 105-105 105zM680.041 306.15c-21 0-37.81 16.804-37.81 37.809s16.805 37.81 37.81 37.81 37.81-16.805 37.81-37.81-16.805-37.81-37.81-37.81z"/><path fill="#FFF" d="M659.036 196.923h-16.804c-50.413-4.2-210.051-4.2-260.464 0-96.623-4.2-180.644 71.418-184.845 168.041v16.804c-4.2 50.413-4.2 210.051 0 260.464-4.2 96.623 71.418 180.644 168.041 184.845h16.804c50.413 4.2 210.051 4.2 260.464 0 96.623 4.2 180.644-71.418 184.845-168.041V381.768c4.2-96.623-71.418-180.644-168.041-184.845zM759.86 696.845c-12.604 29.407-33.609 50.412-58.815 58.814-121.83 16.805-247.86 16.805-373.891 0-29.407-12.603-50.412-33.608-58.814-58.814-12.604-63.015-16.805-126-12.604-184.845-4.2-63.015 0-126 12.604-184.845 12.603-29.407 33.608-50.412 58.814-58.814 121.83-16.805 247.86-16.805 373.891 0 29.407 12.603 50.412 33.608 58.815 58.814 12.603 63.015 16.804 126 12.603 184.845 4.2 63.015 0 126-12.603 184.845z"/></svg>'}}},e=>{e.O(0,[6332,9156],(()=>e(e.s=38731))),e.O()}]);