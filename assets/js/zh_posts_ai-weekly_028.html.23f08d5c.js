"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[7622],{66262:(t,i)=>{i.A=(t,i)=>{const e=t.__vccOpts||t;for(const[t,n]of i)e[t]=n;return e}},34382:(t,i,e)=>{e.r(i),e.d(i,{comp:()=>a,data:()=>r});var n=e(20641);const o={},a=(0,e(66262).A)(o,[["render",function(t,i){return(0,n.uX)(),(0,n.CE)("div",null,i[0]||(i[0]=[(0,n.Fv)('<h1 id="attention-distillation精准风格迁移-phi-4数学推理登顶-egolife自我视角ai助手【ai周报】" tabindex="-1"><a class="header-anchor" href="#attention-distillation精准风格迁移-phi-4数学推理登顶-egolife自我视角ai助手【ai周报】"><span>Attention Distillation精准风格迁移 | Phi-4数学推理登顶 | EgoLife自我视角AI助手【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/ba745a8d-475e-4060-9590-870302076a56/original=true,quality=90/60528420.jpeg" alt="封面源自C站作者Qvoheu" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Qvoheu</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：Attention Distillation基于扩散模型注意力机制实现精准风格迁移；微软Phi-4数学推理远超同类；EgoLife提出EgoGPT/EgoRAG，推进自我视角AI助手；HunyuanVideo-I2V优化图转视频。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#attentiondistillation%E5%9F%BA%E4%BA%8E%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%92%B8%E9%A6%8F">Attention Distillation：基于扩散模型的注意力蒸馏 </a></li><li><a href="#hunyuanvideo-i2v%E5%8F%AF%E5%AE%9A%E5%88%B6%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%B0%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">HunyuanVideo-I2V：可定制的图像到视频生成模型</a></li><li><a href="#phi-4%E5%BE%AE%E8%BD%AF%E5%8F%91%E5%B8%83%E7%9A%84%E4%B8%8B%E4%B8%80%E4%BB%A3phi%E7%B3%BB%E5%88%97%E5%A4%84%E7%90%86%E5%99%A8">Phi-4：微软发布的下一代 Phi 系列处理器</a></li><li><a href="#egolife%E8%BF%88%E5%90%91%E8%87%AA%E6%88%91%E8%A7%86%E8%A7%92%E7%94%9F%E6%B4%BB%E5%8A%A9%E6%89%8B">EgoLife：迈向自我视角生活助手</a></li><li><a href="#olmOCR%E8%A7%A3%E9%94%81pdf%E5%86%85%E5%AE%B9%E7%9A%84%E6%96%B0%E5%B7%A5%E5%85%B7">olmOCR：解锁 PDF 内容的新工具</a></li><li><a href="#owl%E4%BC%98%E5%8C%96%E5%8A%B3%E5%8A%A8%E5%8A%9B%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%8F%E4%BD%9C%E6%A1%86%E6%9E%B6">OWL：优化劳动力学习的多智能体协作框架</a></li></ol><hr><h2 id="attention-distillation-基于扩散模型的注意力蒸馏" tabindex="-1"><a class="header-anchor" href="#attention-distillation-基于扩散模型的注意力蒸馏"><span>Attention Distillation：基于扩散模型的注意力蒸馏</span></a></h2><figure><img src="https://github.com/xugao97/AttentionDistillation/raw/main/assets/1.jpg" alt="Attention Distillation Teaser 图" tabindex="0" loading="lazy"><figcaption>Attention Distillation Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>Attention Distillation</strong> 是由 <strong>新加坡国立大学</strong> 提出的创新方法，利用预训练扩散模型的自注意力特征，在生成图像时精确转移参考图像的风格和语义特征。不同于以往仅将这些特征作为插件使用的方式，Attention Distillation 提出了全新的蒸馏损失函数，通过计算理想风格化结果与当前结果之间的注意力损失，在潜空间进行反向传播优化。此外，该方法提出了改进的分类器引导（Classifier Guidance），将注意力蒸馏损失直接整合到去噪采样过程中，从而加速生成并支持更广泛的图像合成任务。实验表明，该方法在风格迁移、外观保持和纹理合成方面优于现有技术。</p><p><strong>标签</strong>：#扩散模型 #注意力蒸馏 #风格迁移 #图像生成 #CFG</p><hr><h2 id="hunyuanvideo-i2v-可定制的图像到视频生成模型" tabindex="-1"><a class="header-anchor" href="#hunyuanvideo-i2v-可定制的图像到视频生成模型"><span>HunyuanVideo-I2V：可定制的图像到视频生成模型</span></a></h2><figure><img src="https://github.com/Tencent/HunyuanVideo-I2V/raw/main/assets/backbone.png" alt="HunyuanVideo-I2V Backbone 图" tabindex="0" loading="lazy"><figcaption>HunyuanVideo-I2V Backbone 图</figcaption></figure><p><strong>概要</strong>：<strong>HunyuanVideo-I2V</strong> 是由 <strong>腾讯</strong> 开发的可定制图像到视频生成模型，基于此前开源的 HunyuanVideo 模型。该模型允许用户输入单张图像，并生成与之风格和内容一致的动态视频。其设计旨在为开源社区提供灵活的工具，促进图像到视频生成领域的探索和应用。</p><p><strong>标签</strong>：#图像到视频 #视频生成 #深度学习 #计算机视觉 #腾讯</p><hr><h2 id="phi-4-微软发布的下一代-phi-系列处理器" tabindex="-1"><a class="header-anchor" href="#phi-4-微软发布的下一代-phi-系列处理器"><span>Phi-4：微软发布的下一代 Phi 系列处理器</span></a></h2><figure><img src="https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/02/F3.webp" alt="Phi-4 Vision Benchmarks 图" tabindex="0" loading="lazy"><figcaption>Phi-4 Vision Benchmarks 图</figcaption></figure><p><strong>概要</strong>：微软正式推出 Phi 系列最新语言模型 <strong>Phi-4</strong>，这是一款参数规模达 140 亿的小型模型，通过以 <strong>数据质量为核心</strong> 的训练策略实现性能突破。与多数依赖网页、代码等有机数据预训练的模型不同，Phi-4 在训练中 <strong>全程战略性融入合成数据</strong>，并结合严格筛选的高质量数据，显著提升了复杂推理能力。 Phi-4 在STEM 领域问答表现尤为突出，其数学推理能力超越了前代模型所依赖的教师模型（GPT-4），证明其数据生成与后训练技术已突破传统蒸馏范式。尽管架构与 Phi-3 相比仅作微调，Phi-4 凭借 <strong>优化的训练课程</strong> 和 <strong>创新的后训练方案</strong>，在推理类基准测试中实现了 <strong>远超同量级模型的性能</strong>。</p><p><strong>标签</strong>：#微软 #Phi-4 #语言模型 #数学推理 #人工智能 #STEM #数据质量</p><hr><h2 id="egolife-迈向自我视角生活助手" tabindex="-1"><a class="header-anchor" href="#egolife-迈向自我视角生活助手"><span>EgoLife：迈向自我视角生活助手</span></a></h2><figure><img src="https://github.com/EvolvingLMMs-Lab/EgoLife/raw/main/assets/egolife_teaser.png" alt="EgoLife Teaser 图" tabindex="0" loading="lazy"><figcaption>EgoLife Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>EgoLife</strong> 是首个聚焦 <strong>自我视角生活助手</strong> 的研究项目，通过 AI 驱动的可穿戴眼镜记录并分析日常生活，旨在提升个人效率。项目通过为期一周的六人共同生活研究，采集了包含 <strong>300 小时多模态数据</strong>（视频、音频、多视角）的EgoLife 数据集，覆盖讨论、购物、烹饪等场景，并提供详细注释。基于此，团队提出EgoLifeQA，一套面向长时间上下文的问答任务，支持生活场景中的事件回忆、健康监测与个性化建议。 为应对跨模态理解、身份识别和超长上下文推理等挑战，研究团队开发了集成系统 <strong>EgoButler</strong>，包含：<strong>EgoGPT</strong>：在自我视角数据上训练的全模态模型，刷新视频理解 SOTA；<strong>EgoRAG</strong>：基于检索的增强组件，支持超长上下文问答。实验验证了系统有效性，并揭示技术瓶颈与改进方向。项目已开源数据集、模型及基准，推动自我视角 AI 助手领域研究。</p><p><strong>标签</strong>：#EgoLife #自我视角AI #多模态数据集 #EgoGPT #EgoRAG #生活助手 #长时间上下文</p><hr><h2 id="olmocr-解锁-pdf-内容的新工具" tabindex="-1"><a class="header-anchor" href="#olmocr-解锁-pdf-内容的新工具"><span>olmOCR：解锁 PDF 内容的新工具</span></a></h2><figure><img src="https://olmocr.allenai.org/olmocr-v3-light-crf8wznq.png" alt="olmOCR Demo 图" tabindex="0" loading="lazy"><figcaption>olmOCR Demo 图</figcaption></figure><p><strong>概要</strong>：由 Allen Institute for AI 开发的 <strong>olmOCR</strong> 是一个用于将 PDF 文档线性化为纯文本的工具包，旨在为大型语言模型的训练提供高质量的数据。 olmOCR 利用视觉语言模型，从多样化的 PDF 文档中提取结构化内容，如章节、表格、列表和公式等。 该工具包优化了大规模批处理过程，能够以低成本高效地处理大量 PDF 页面。 通过发布模型权重、数据和训练代码，olmOCR 为研究人员提供了一个强大的工具，以解锁 PDF 文档中的海量信息。</p><p><strong>标签</strong>：#olmOCR #PDF处理 #视觉语言模型 #人工智能 #AllenInstituteForAI</p><hr><h2 id="owl-优化劳动力学习的多智能体协作框架" tabindex="-1"><a class="header-anchor" href="#owl-优化劳动力学习的多智能体协作框架"><span>OWL：优化劳动力学习的多智能体协作框架</span></a></h2><figure><img src="https://github.com/camel-ai/owl/raw/main/assets/owl_architecture.png" alt="OWL 框架示意图" tabindex="0" loading="lazy"><figcaption>OWL 框架示意图</figcaption></figure><p><strong>概要</strong>：<strong>OWL</strong>（Optimized Workforce Learning）是由 <strong>CAMEL-AI</strong> 开发的前沿多智能体协作框架，旨在推动任务自动化的边界。 该框架建立在 CAMEL-AI Framework 之上，旨在彻底变革 AI 智能体协作解决现实任务的方式。 通过利用动态智能体交互，OWL 实现了跨多领域更自然、高效且稳健的任务自动化。 在 GAIA 基准测试中，OWL 取得了 58.18 的平均分，在开源框架中排名第一。</p><p><strong>标签</strong>：#OWL #多智能体协作 #任务自动化 #CAMEL-AI #GAIA</p><hr><h3 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span><strong>参考文献</strong></span></a></h3><ol><li><a href="https://xugao97.github.io/AttentionDistillation/" target="_blank" rel="noopener noreferrer">Attention Distillation 主页</a></li><li><a href="https://github.com/xugao97/AttentionDistillation" target="_blank" rel="noopener noreferrer">Attention Distillation GitHub</a></li><li><a href="https://github.com/zichongc/ComfyUI-Attention-Distillation" target="_blank" rel="noopener noreferrer">ComfyUI-Attention-Distillation GitHub</a></li><li><a href="https://arxiv.org/pdf/2502.20235" target="_blank" rel="noopener noreferrer">Attention Distillation 论文</a></li><li><a href="https://huggingface.co/tencent/HunyuanVideo-I2V" target="_blank" rel="noopener noreferrer">HunyuanVideo-I2V Hugging Face</a></li><li><a href="https://github.com/tencent/HunyuanVideo-I2V" target="_blank" rel="noopener noreferrer">HunyuanVideo-I2V GitHub</a></li><li><a href="https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/" target="_blank" rel="noopener noreferrer">微软 Phi-4 官方博客</a></li><li><a href="https://arxiv.org/html/2503.01743" target="_blank" rel="noopener noreferrer">Phi-4 论文</a></li><li><a href="https://egolife-ai.github.io/" target="_blank" rel="noopener noreferrer">EgoLife 官网</a></li><li><a href="https://github.com/EvolvingLMMs-Lab/EgoLife" target="_blank" rel="noopener noreferrer">EgoLife GitHub</a></li><li><a href="https://arxiv.org/html/2503.03803v1" target="_blank" rel="noopener noreferrer">EgoLife 论文</a></li><li><a href="https://github.com/allenai/olmocr" target="_blank" rel="noopener noreferrer">olmOCR GitHub</a></li><li><a href="https://olmocr.allenai.org/papers/olmocr.pdf" target="_blank" rel="noopener noreferrer">olmOCR 论文</a></li><li><a href="https://github.com/camel-ai/owl" target="_blank" rel="noopener noreferrer">OWL GitHub</a></li></ol>',40)]))}]]),r=JSON.parse('{"path":"/zh/posts/ai-weekly/028.html","title":"Attention Distillation精准风格迁移 | Phi-4数学推理登顶 | EgoLife自我视角AI助手【AI周报】","lang":"zh-CN","frontmatter":{"description":"Attention Distillation精准风格迁移 | Phi-4数学推理登顶 | EgoLife自我视角AI助手【AI周报】 封面源自C站作者Qvoheu封面源自C站作者Qvoheu 摘要 本周亮点：Attention Distillation基于扩散模型注意力机制实现精准风格迁移；微软Phi-4数学推理远超同类；EgoLife提出EgoGPT...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/028.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"Attention Distillation精准风格迁移 | Phi-4数学推理登顶 | EgoLife自我视角AI助手【AI周报】"}],["meta",{"property":"og:description","content":"Attention Distillation精准风格迁移 | Phi-4数学推理登顶 | EgoLife自我视角AI助手【AI周报】 封面源自C站作者Qvoheu封面源自C站作者Qvoheu 摘要 本周亮点：Attention Distillation基于扩散模型注意力机制实现精准风格迁移；微软Phi-4数学推理远超同类；EgoLife提出EgoGPT..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/ba745a8d-475e-4060-9590-870302076a56/original=true,quality=90/60528420.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Attention Distillation精准风格迁移 | Phi-4数学推理登顶 | EgoLife自我视角AI助手【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/ba745a8d-475e-4060-9590-870302076a56/original=true,quality=90/60528420.jpeg\\",\\"https://github.com/xugao97/AttentionDistillation/raw/main/assets/1.jpg\\",\\"https://github.com/Tencent/HunyuanVideo-I2V/raw/main/assets/backbone.png\\",\\"https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/02/F3.webp\\",\\"https://github.com/EvolvingLMMs-Lab/EgoLife/raw/main/assets/egolife_teaser.png\\",\\"https://olmocr.allenai.org/olmocr-v3-light-crf8wznq.png\\",\\"https://github.com/camel-ai/owl/raw/main/assets/owl_architecture.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"Attention Distillation：基于扩散模型的注意力蒸馏","slug":"attention-distillation-基于扩散模型的注意力蒸馏","link":"#attention-distillation-基于扩散模型的注意力蒸馏","children":[]},{"level":2,"title":"HunyuanVideo-I2V：可定制的图像到视频生成模型","slug":"hunyuanvideo-i2v-可定制的图像到视频生成模型","link":"#hunyuanvideo-i2v-可定制的图像到视频生成模型","children":[]},{"level":2,"title":"Phi-4：微软发布的下一代 Phi 系列处理器","slug":"phi-4-微软发布的下一代-phi-系列处理器","link":"#phi-4-微软发布的下一代-phi-系列处理器","children":[]},{"level":2,"title":"EgoLife：迈向自我视角生活助手","slug":"egolife-迈向自我视角生活助手","link":"#egolife-迈向自我视角生活助手","children":[]},{"level":2,"title":"olmOCR：解锁 PDF 内容的新工具","slug":"olmocr-解锁-pdf-内容的新工具","link":"#olmocr-解锁-pdf-内容的新工具","children":[]},{"level":2,"title":"OWL：优化劳动力学习的多智能体协作框架","slug":"owl-优化劳动力学习的多智能体协作框架","link":"#owl-优化劳动力学习的多智能体协作框架","children":[{"level":3,"title":"参考文献","slug":"参考文献","link":"#参考文献","children":[]}]}],"readingTime":{"minutes":5.85,"words":1756},"filePathRelative":"zh/posts/ai-weekly/028.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/ba745a8d-475e-4060-9590-870302076a56/original=true,quality=90/60528420.jpeg\\" alt=\\"封面源自C站作者Qvoheu\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面源自C站作者Qvoheu</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：Attention Distillation基于扩散模型注意力机制实现精准风格迁移；微软Phi-4数学推理远超同类；EgoLife提出EgoGPT/EgoRAG，推进自我视角AI助手；HunyuanVideo-I2V优化图转视频。</p>","autoDesc":true}')}}]);