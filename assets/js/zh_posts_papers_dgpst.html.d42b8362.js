"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[5145],{66262:(t,e)=>{e.A=(t,e)=>{const l=t.__vccOpts||t;for(const[t,a]of e)l[t]=a;return l}},30745:(t,e,l)=>{l.r(e),l.d(e,{comp:()=>r,data:()=>n});var a=l(20641);const i={},r=(0,l(66262).A)(i,[["render",function(t,e){return(0,a.uX)(),(0,a.CE)("div",null,e[0]||(e[0]=[(0,a.Fv)('<h1 id="【论文精读】dgpst-精通百变风格的通用肖像画师" tabindex="-1"><a class="header-anchor" href="#【论文精读】dgpst-精通百变风格的通用肖像画师"><span>【论文精读】DGPST：精通百变风格的通用肖像画师</span></a></h1><table><thead><tr><th style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/contenttext.png" alt="" loading="lazy"></th><th style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/style0.png" alt="" loading="lazy"></th><th style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/style3.png" alt="" loading="lazy"></th><th style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/sketch.png" alt="" loading="lazy"></th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/contenttext.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/style5.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/style3.5.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/sketch.png" alt="" loading="lazy"></td></tr></tbody></table><p align="center">DGPST方法效果展示。该方法可实现照片、卡通、素描、动画等不同领域间的高质量风格迁移。</p><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>该论文提出了一种通用肖像风格迁移方法DGPST，旨在解决跨领域（如照片、卡通、素描）风格迁移中保留身份与传递风格的难题。通过结合扩散模型、语义对齐和创新的小波变换，DGPST能精准传递风格，同时保留人物细节。实验证明，该方法在多个数据集上均展现了超越SOTA的泛化能力与生成质量。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li><li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li><li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li><li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li></ol><hr><h2 id="背景与研究目标" tabindex="-1"><a class="header-anchor" href="#背景与研究目标"><span>背景与研究目标</span></a></h2><p>肖像风格迁移是图像编辑领域一个备受关注且极具挑战性的任务。其目标是将一张参考图像的艺术风格（如色彩、笔触、光影）应用到一张内容图像上，同时完整保留内容图像中的人物身份和面部结构。</p><p>现有方法在该领域取得了显著进展，但普遍存在以下局限性：</p><ul><li><strong>领域泛化能力弱</strong>：多数方法在处理来自不同领域（例如，将真实照片转换为卡通或素描风格）的图像时效果不佳，因为源域和目标域之间存在巨大的结构和纹理差异。</li><li><strong>内容与风格解耦不佳</strong>：许多模型难以在保留人物关键身份特征（如脸型、五官）的同时，充分学习参考图像的风格，导致生成结果要么“风格不足”，要么“面目全非”。</li><li><strong>依赖精确的语义对齐</strong>：对于肖像这一精细任务，风格迁移需要在语义对应的区域（如头发对头发，眼睛对眼睛）进行，否则会导致“风格错位”的现象。</li></ul><p>为解决上述问题，本文提出了一种具有强大领域泛化能力的肖像风格迁移框架（DGPST）。其核心研究目标是：</p><ol><li><strong>实现高质量的跨领域风格迁移</strong>，即使输入和参考肖像来自照片、动画、素描等完全不同的领域。</li><li><strong>建立精确的语义对应关系</strong>，确保风格在面部各区域得到正确迁移。</li><li><strong>设计精巧的平衡机制</strong>，在内容保留和风格化之间取得最佳平衡，生成视觉效果自然且身份信息明确的结果。</li></ol><hr><h2 id="方法与创新点" tabindex="-1"><a class="header-anchor" href="#方法与创新点"><span>方法与创新点</span></a></h2><p>DGPST的整体框架基于预训练的稳定扩散模型（Stable Diffusion），并引入了三大核心创新模块：语义感知风格对齐、双条件扩散模型和初始隐变量的AdaIN-小波变换。</p><figure><img src="https://www.arxiv.org/html/2507.04243v2/x1.png" alt="DGPST方法整体流程图" tabindex="0" loading="lazy"><figcaption>DGPST方法整体流程图</figcaption></figure><p align="center">DGPST方法整体流程图</p><h3 id="_1-语义感知风格对齐-semantic-aware-style-alignment" tabindex="-1"><a class="header-anchor" href="#_1-语义感知风格对齐-semantic-aware-style-alignment"><span>1. 语义感知风格对齐 (Semantic-Aware Style Alignment)</span></a></h3><p>为了实现精准的风格迁移，首先需要将参考图的风格与内容图的结构对齐。DGPST通过一个精巧的“语义适配器”来实现这一点。</p><ul><li><strong>特征提取</strong>：利用预训练的CLIP图像编码器和Stable Diffusion的U-Net提取内容图像和参考图像的深层特征。这些特征蕴含了丰富的语义信息。</li><li><strong>稠密对应计算</strong>：通过计算两组特征之间的相关性矩阵（Correlation Matrix），模型能够找到内容图每个像素点在参考图中的语义对应点。</li><li><strong>图像扭曲 (Warping)</strong>：基于计算出的对应关系，对参考图像进行可微调的扭曲操作，生成一个“扭曲后的参考图”（Warped Reference）。这个新生成的图像在风格上与原始参考图一致，但在结构和姿态上已经与内容图对齐，为后续的风格注入提供了理想的“弹药”。</li></ul><h3 id="_2-双条件扩散模型-dual-conditional-diffusion-model" tabindex="-1"><a class="header-anchor" href="#_2-双条件扩散模型-dual-conditional-diffusion-model"><span>2. 双条件扩散模型 (Dual Conditional Diffusion Model)</span></a></h3><p>在生成最终图像时，模型需要同时考虑内容结构和目标风格。DGPST采用了一个双条件控制的扩散模型，通过两个并行的“指导员”来精确控制生成过程。</p><ul><li><strong>结构指导 (Structure Guidance)</strong>：为了保留内容图像的身份细节（如皮肤纹理、面部轮廓），模型首先通过哈尔离散小波变换（Haar DWT）提取出内容图像的高频信息。这些信息代表了图像的结构和边缘细节，随后被送入一个ControlNet中，用于在生成过程中牢牢“抓住”人物的原始结构。</li><li><strong>风格指导 (Style Guidance)</strong>：风格信息则来源于前一步生成的“扭曲后的参考图”。该图像被送入一个“风格适配器”（Style Adapter）中，该适配器将风格信息注入到扩散模型的交叉注意力层，从而引导生成图像的色彩、光影和整体氛围向参考风格靠拢。</li></ul><p>通过这种结构与风格的双重控制，DGPST能够生成既保留了人物身份又充满了艺术风格的高质量肖像。</p><h3 id="_3-初始隐变量的adain-小波变换-initial-latent-adain-wavelet-transform" tabindex="-1"><a class="header-anchor" href="#_3-初始隐变量的adain-小波变换-initial-latent-adain-wavelet-transform"><span>3. 初始隐变量的AdaIN-小波变换 (Initial Latent AdaIN-Wavelet Transform)</span></a></h3><p>扩散模型的生成起点（即初始隐变量）对最终结果有巨大影响。若直接使用内容图的隐变量，会导致风格学习不足；若直接使用参考图的隐变量，则会丢失内容细节。 为解决此问题，DGPST提出了一种新颖的初始隐变量混合策略：</p><ol><li><strong>分别编码</strong>：首先通过DDIM Inversion技术，分别得到内容图和扭曲参考图的初始隐变量。</li><li><strong>风格混合</strong>：使用AdaIN（Adaptive Instance Normalization）将扭曲参考图的风格信息（均值和方差）应用到内容图的隐变量上，得到一个初步混合的隐变量。</li><li><strong>小波融合</strong>：最关键的一步，对初步混合后的隐变量和扭曲参考图的隐变量进行小波变换，将参考图的低频信息（决定整体色调和氛围）与内容图的高频信息（决定细节和结构）进行融合，再通过逆小波变换（IDWT）得到最终的初始隐变量。</li></ol><p>这种方法巧妙地结合了参考图的“灵魂”（低频风格）和内容图的“骨架”（高频细节），为扩散模型提供了一个完美的起点，极大地提升了最终生成图像的质量。</p><hr><h2 id="实验与结果分析" tabindex="-1"><a class="header-anchor" href="#实验与结果分析"><span>实验与结果分析</span></a></h2><h3 id="实验设置" tabindex="-1"><a class="header-anchor" href="#实验设置"><span>实验设置</span></a></h3><ul><li><strong>数据集</strong>：模型仅在包含2.8万张真实人像的CelebAMask-HQ数据集上进行训练。为了验证其泛化能力，测试在多个数据集上进行，包括FFHQ（高质量真实人像）和AAHQ（高质量艺术人像），以及各种卡通、素描等跨领域图像。</li><li><strong>评估指标</strong>： <ul><li><strong>Gram Loss</strong>：衡量风格迁移的保真度。</li><li><strong>LPIPS</strong>：衡量内容保留的相似度。</li><li><strong>ID Distance</strong>：衡量人物身份信息的一致性。</li></ul></li><li><strong>对比方法</strong>：与两类SOTA方法进行比较：(i) 传统的肖像风格迁移方法（如Shih et al.）；(ii) 基于扩散模型的方法（如IP-Adapter, StyleID, InstantStyle+）。</li></ul><h3 id="主要实验结果" tabindex="-1"><a class="header-anchor" href="#主要实验结果"><span>主要实验结果</span></a></h3><h4 id="定量比较" tabindex="-1"><a class="header-anchor" href="#定量比较"><span>定量比较</span></a></h4><p>如下表所示，无论是在训练所用的CelebAMask-HQ数据集，还是在包含多领域的混合数据集上，DGPST在关键指标上均表现出色。</p><table><thead><tr><th style="text-align:left;">Method</th><th style="text-align:left;">Gram loss↓</th><th style="text-align:left;">LPIPS↓</th><th style="text-align:left;">ID↓</th></tr></thead><tbody><tr><td style="text-align:left;">Shih et al. [27]</td><td style="text-align:left;">0.376</td><td style="text-align:left;">0.187</td><td style="text-align:left;">0.093</td></tr><tr><td style="text-align:left;">Wang et al. [37]</td><td style="text-align:left;"><strong>0.208</strong></td><td style="text-align:left;">0.181</td><td style="text-align:left;">0.106</td></tr><tr><td style="text-align:left;">IP-A[41] + C.N.[43]</td><td style="text-align:left;">2.835</td><td style="text-align:left;">0.245</td><td style="text-align:left;">0.774</td></tr><tr><td style="text-align:left;">StyleID[3]</td><td style="text-align:left;">0.505</td><td style="text-align:left;">0.198</td><td style="text-align:left;">0.222</td></tr><tr><td style="text-align:left;">InstantStyle+ [35]</td><td style="text-align:left;">0.557</td><td style="text-align:left;">0.294</td><td style="text-align:left;">0.272</td></tr><tr><td style="text-align:left;"><strong>Ours</strong></td><td style="text-align:left;">0.274</td><td style="text-align:left;"><strong>0.116</strong></td><td style="text-align:left;"><strong>0.057</strong></td></tr></tbody></table><p align="center">在CelebAMask-HQ数据集上的定量比较</p><table><thead><tr><th style="text-align:left;">Method</th><th style="text-align:left;">Gram loss↓</th><th style="text-align:left;">LPIPS↓</th><th style="text-align:left;">ID↓</th></tr></thead><tbody><tr><td style="text-align:left;">Shih et al. [27]</td><td style="text-align:left;">0.802</td><td style="text-align:left;">0.156</td><td style="text-align:left;">0.105</td></tr><tr><td style="text-align:left;">Wang et al. [37]</td><td style="text-align:left;">1.488</td><td style="text-align:left;">0.119</td><td style="text-align:left;">0.096</td></tr><tr><td style="text-align:left;">IP-A[41] + C.N.[43]</td><td style="text-align:left;">4.211</td><td style="text-align:left;">0.375</td><td style="text-align:left;">0.763</td></tr><tr><td style="text-align:left;">StyleID[3]</td><td style="text-align:left;">1.343</td><td style="text-align:left;">0.149</td><td style="text-align:left;">0.165</td></tr><tr><td style="text-align:left;">InstantStyle+ [35]</td><td style="text-align:left;">0.723</td><td style="text-align:left;">0.192</td><td style="text-align:left;">0.203</td></tr><tr><td style="text-align:left;"><strong>Ours</strong></td><td style="text-align:left;"><strong>0.657</strong></td><td style="text-align:left;"><strong>0.083</strong></td><td style="text-align:left;"><strong>0.087</strong></td></tr></tbody></table><p align="center">在混合领域数据集上的定量比较</p><p>在混合数据集上，DGPST在所有三个指标上均取得了最优成绩，尤其是在LPIPS（内容保留）和ID（身份保留）上优势明显，充分证明了其强大的领域泛化能力和对内容的保护能力。</p><h4 id="定性比较" tabindex="-1"><a class="header-anchor" href="#定性比较"><span>定性比较</span></a></h4><p>定性结果更加直观地展示了DGPST的优越性。</p><table><thead><tr><th style="text-align:center;">Reference</th><th style="text-align:center;">Input</th><th style="text-align:center;">Ours</th><th style="text-align:center;">Wang et al.</th><th style="text-align:center;">IP-A+C.N.</th><th style="text-align:center;">Deng et al.</th><th style="text-align:center;">StyleID</th><th style="text-align:center;">InstantStyle+</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/style.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/content.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/Ours.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/ppst.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/ipa.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/zxing.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/styleid.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/instant.jpg" alt="" loading="lazy"></td></tr><tr><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/style.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/content.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/Ours.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/ppst.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/ipa.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/zxing.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/styleid.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/instant.jpg" alt="" loading="lazy"></td></tr><tr><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/style.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/content.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/Ours.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/ppst.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/ipa.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/zxing.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/styleid.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/instant.jpg" alt="" loading="lazy"></td></tr></tbody></table><p align="center">DGPST与SOTA方法的定性效果对比</p><p>从上图可以看出，相比其他方法，DGPST的生成结果在风格上更忠实于参考图（如皮肤色泽、头发颜色和高光），同时人物的面部特征和身份信息也得到了最好的保留。其他方法或多或少存在颜色溢出、身份扭曲或风格学习不充分的问题。</p><h4 id="消融实验" tabindex="-1"><a class="header-anchor" href="#消融实验"><span>消融实验</span></a></h4><p>为了验证各模块的有效性，论文进行了详细的消融实验。</p><table><thead><tr><th style="text-align:center;">Reference</th><th style="text-align:center;">Input</th><th style="text-align:center;">w/o Cont.</th><th style="text-align:center;">Canny Cont.</th><th style="text-align:center;">Input Cont.</th><th style="text-align:center;">w/o style adapter</th><th style="text-align:center;">IP-A.(4 tokens)</th><th style="text-align:center;">Ours</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/style.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/content.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/nocont.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/canny.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/inputcont.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/noip.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/4tokens.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/Ours.png" alt="" loading="lazy"></td></tr></tbody></table><p align="center">ControlNet与风格适配器消融实验效果</p><ul><li><strong>ControlNet和Style Adapter的作用</strong>：上图显示，去除ControlNet（w/o Cont.）会导致细节丢失；而去除风格适配器（w/o style adapter）则导致风格学习不足。这证明了双条件控制的必要性。</li></ul><table><thead><tr><th style="text-align:center;">Reference</th><th style="text-align:center;">Input</th><th style="text-align:center;">Input latent</th><th style="text-align:center;">w.r. latent</th><th style="text-align:center;">AdaIN</th><th style="text-align:center;">Ours</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_init/3/style.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_init/3/content.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_init/3/inputlatent.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_init/3/wrlatent.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_init/3/adain.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_init/3/Ours.png" alt="" loading="lazy"></td></tr></tbody></table><p align="center">不同初始隐变量初始化策略的效果对比</p><ul><li><strong>初始隐变量的作用</strong>：上图对比了不同初始化策略的效果。直接用内容图 latent (c) 会保留过多原始颜色；直接用扭曲参考图 latent (d) 会导致模糊；而本文提出的AdaIN-小波变换 (f) 在内容清晰度和风格化程度上取得了最佳平衡。</li></ul><h4 id="领域泛化能力展示" tabindex="-1"><a class="header-anchor" href="#领域泛化能力展示"><span>领域泛化能力展示</span></a></h4><p>DGPST在多种跨领域任务中均表现出色，包括灰度图上色、老照片修复和真人照片转素描。</p><table><thead><tr><th style="text-align:center;">Input</th><th style="text-align:center;">Output</th><th style="text-align:center;">Input</th><th style="text-align:center;">Output</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/colorization/5/content.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/colorization/5/output1.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/colorization/4/input.jpg" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/colorization/4/Style1.png" alt="" loading="lazy"></td></tr></tbody></table><p align="center">灰度图和素描上色效果展示</p><table><thead><tr><th style="text-align:center;">Input</th><th style="text-align:center;">Output</th><th style="text-align:center;">Input</th><th style="text-align:center;">Output</th></tr></thead><tbody><tr><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/oldphotos/1/content.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/oldphotos/1/output.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/oldphotos/5/content.png" alt="" loading="lazy"></td><td style="text-align:center;"><img src="https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/oldphotos/5/Output.png" alt="" loading="lazy"></td></tr></tbody></table><p align="center">老照片修复效果展示</p><p>这些结果进一步验证了模型仅需在真实人像上训练，便能泛化到各种艺术风格和图像修复任务中，具有极高的实用价值。</p><hr><h2 id="模型启发与方法延伸" tabindex="-1"><a class="header-anchor" href="#模型启发与方法延伸"><span>模型启发与方法延伸</span></a></h2><p>DGPST模型的成功为我们提供了诸多有价值的启发：</p><ul><li><strong>融合经典与现代</strong>：该工作巧妙地将经典信号处理技术（小波变换）与现代的深度生成模型（扩散模型、ControlNet）相结合，为解决复杂的生成任务提供了新的思路。</li><li><strong>特征的再利用</strong>：该方法充分挖掘了预训练扩散模型内部特征的价值，将其用于计算语义对应关系，这提示我们预训练模型不仅是强大的生成器，其内部特征同样是宝贵的“信息矿藏”。</li><li><strong>初始化的重要性</strong>：DGPST对初始隐变量的精巧设计再次证明，在生成任务中，一个好的起点至关重要。AdaIN-小波变换的融合策略对于其他需要平衡内容与风格的生成任务（如图像到图像翻译）具有很高的借鉴意义。</li></ul><p>该方法未来可以向以下方向延伸：</p><ul><li><strong>视频风格迁移</strong>：将当前对静态图像的处理能力扩展到视频领域，实现动态、时序一致的肖像视频风格迁移。</li><li><strong>更细粒度的控制</strong>：除了整体风格迁移，未来可以探索对特定属性（如仅改变发型、妆容或光照）进行更精细、可编辑的控制。</li><li><strong>轻量化部署</strong>：目前模型依赖于大型扩散模型，计算开销较大。未来的研究可以探索模型蒸馏或量化技术，以实现更高效的推理和移动端部署。</li></ul><hr><h2 id="结论与未来展望" tabindex="-1"><a class="header-anchor" href="#结论与未来展望"><span>结论与未来展望</span></a></h2><p>本文提出的“Domain Generalizable Portrait Style Transfer (DGPST)”框架，通过结合基于扩散模型的语义对齐、双条件控制以及创新的AdaIN-小波变换初始化策略，成功地解决了跨领域肖像风格迁移中的核心挑战。该方法在仅使用真实人像数据进行训练的情况下，展现出了卓越的领域泛化能力，能够在保留人物身份的同时，生成风格鲜明、细节丰富的高质量艺术肖像。大量的定量和定性实验均证明了其相较于现有SOTA方法的优越性。</p><p>总而言之，DGPST不仅为肖像风格迁移领域提供了一个强大而可靠的解决方案，其在特征利用、多重条件控制和初始状态设计方面的创新思路，也为更广泛的AIGC领域研究开辟了新的可能性。</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h3><ol><li><a href="https://github.com/wangxb29/DGPST" target="_blank" rel="noopener noreferrer">代码仓库</a></li><li><a href="https://arxiv.org/abs/2507.04243" target="_blank" rel="noopener noreferrer">论文原文</a></li><li><a href="https://www.alphaxiv.org/overview/2507.04243" target="_blank" rel="noopener noreferrer">论文博客（Alphaxiv）</a></li></ol>',76)]))}]]),n=JSON.parse('{"path":"/zh/posts/papers/dgpst.html","title":"【论文精读】DGPST：精通百变风格的通用肖像画师","lang":"zh-CN","frontmatter":{"description":"【论文精读】DGPST：精通百变风格的通用肖像画师 DGPST方法效果展示。该方法可实现照片、卡通、素描、动画等不同领域间的高质量风格迁移。 摘要 该论文提出了一种通用肖像风格迁移方法DGPST，旨在解决跨领域（如照片、卡通、素描）风格迁移中保留身份与传递风格的难题。通过结合扩散模型、语义对齐和创新的小波变换，DGPST能精准传递风格，同时保留人物细节...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/papers/dgpst.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"【论文精读】DGPST：精通百变风格的通用肖像画师"}],["meta",{"property":"og:description","content":"【论文精读】DGPST：精通百变风格的通用肖像画师 DGPST方法效果展示。该方法可实现照片、卡通、素描、动画等不同领域间的高质量风格迁移。 摘要 该论文提出了一种通用肖像风格迁移方法DGPST，旨在解决跨领域（如照片、卡通、素描）风格迁移中保留身份与传递风格的难题。通过结合扩散模型、语义对齐和创新的小波变换，DGPST能精准传递风格，同时保留人物细节..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/contenttext.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"【论文精读】DGPST：精通百变风格的通用肖像画师\\",\\"image\\":[\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/contenttext.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/style0.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/style3.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/sketch.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/contenttext.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/style5.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/style3.5.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/sketch.png\\",\\"https://www.arxiv.org/html/2507.04243v2/x1.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/style.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/content.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/Ours.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/ppst.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/ipa.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/zxing.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/styleid.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/11/instant.jpg\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/style.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/content.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/Ours.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/ppst.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/ipa.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/zxing.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/styleid.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/12/instant.jpg\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/style.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/content.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/Ours.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/ppst.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/ipa.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/zxing.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/styleid.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/cmp_diffusion/13/instant.jpg\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/style.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/content.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/nocont.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/canny.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/inputcont.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/noip.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/4tokens.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_ip/8/Ours.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_init/3/style.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_init/3/content.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_init/3/inputlatent.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_init/3/wrlatent.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_init/3/adain.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/ablation_init/3/Ours.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/colorization/5/content.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/colorization/5/output1.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/colorization/4/input.jpg\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/colorization/4/Style1.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/oldphotos/1/content.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/oldphotos/1/output.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/oldphotos/5/content.png\\",\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/oldphotos/5/Output.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"背景与研究目标","slug":"背景与研究目标","link":"#背景与研究目标","children":[]},{"level":2,"title":"方法与创新点","slug":"方法与创新点","link":"#方法与创新点","children":[{"level":3,"title":"1. 语义感知风格对齐 (Semantic-Aware Style Alignment)","slug":"_1-语义感知风格对齐-semantic-aware-style-alignment","link":"#_1-语义感知风格对齐-semantic-aware-style-alignment","children":[]},{"level":3,"title":"2. 双条件扩散模型 (Dual Conditional Diffusion Model)","slug":"_2-双条件扩散模型-dual-conditional-diffusion-model","link":"#_2-双条件扩散模型-dual-conditional-diffusion-model","children":[]},{"level":3,"title":"3. 初始隐变量的AdaIN-小波变换 (Initial Latent AdaIN-Wavelet Transform)","slug":"_3-初始隐变量的adain-小波变换-initial-latent-adain-wavelet-transform","link":"#_3-初始隐变量的adain-小波变换-initial-latent-adain-wavelet-transform","children":[]}]},{"level":2,"title":"实验与结果分析","slug":"实验与结果分析","link":"#实验与结果分析","children":[{"level":3,"title":"实验设置","slug":"实验设置","link":"#实验设置","children":[]},{"level":3,"title":"主要实验结果","slug":"主要实验结果","link":"#主要实验结果","children":[]}]},{"level":2,"title":"模型启发与方法延伸","slug":"模型启发与方法延伸","link":"#模型启发与方法延伸","children":[]},{"level":2,"title":"结论与未来展望","slug":"结论与未来展望","link":"#结论与未来展望","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":11.39,"words":3417},"filePathRelative":"zh/posts/papers/dgpst.md","excerpt":"\\n<table>\\n<thead>\\n<tr>\\n<th style=\\"text-align:center\\"><img src=\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/contenttext.png\\" alt=\\"\\" loading=\\"lazy\\"></th>\\n<th style=\\"text-align:center\\"><img src=\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/style0.png\\" alt=\\"\\" loading=\\"lazy\\"></th>\\n<th style=\\"text-align:center\\"><img src=\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/style3.png\\" alt=\\"\\" loading=\\"lazy\\"></th>\\n<th style=\\"text-align:center\\"><img src=\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/8/sketch.png\\" alt=\\"\\" loading=\\"lazy\\"></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td style=\\"text-align:center\\"><img src=\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/contenttext.png\\" alt=\\"\\" loading=\\"lazy\\"></td>\\n<td style=\\"text-align:center\\"><img src=\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/style5.png\\" alt=\\"\\" loading=\\"lazy\\"></td>\\n<td style=\\"text-align:center\\"><img src=\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/style3.5.png\\" alt=\\"\\" loading=\\"lazy\\"></td>\\n<td style=\\"text-align:center\\"><img src=\\"https://www.arxiv.org/html/2507.04243v2/extracted/6604132/figures/head/10/sketch.png\\" alt=\\"\\" loading=\\"lazy\\"></td>\\n</tr>\\n</tbody>\\n</table>","autoDesc":true}')}}]);