"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[8464],{6262:(t,e)=>{e.A=(t,e)=>{const o=t.__vccOpts||t;for(const[t,l]of e)o[t]=l;return o}},4610:(t,e,o)=>{o.r(e),o.d(e,{comp:()=>n,data:()=>r});var l=o(641);const a={},n=(0,o(6262).A)(a,[["render",function(t,e){return(0,l.uX)(),(0,l.CE)("div",null,e[0]||(e[0]=[(0,l.Fv)('<h1 id="【论文精读】show-o2-改进的原生统一多模态模型" tabindex="-1"><a class="header-anchor" href="#【论文精读】show-o2-改进的原生统一多模态模型"><span>【论文精读】Show-o2: 改进的原生统一多模态模型</span></a></h1><figure><img src="https://github.com/showlab/Show-o/raw/main/show-o2/docs/showo2.png" alt="Show-o2 Logo" tabindex="0" loading="lazy"><figcaption>Show-o2 Logo</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>Show-o2是新加坡国立大学Show Lab与字节跳动联合开发的统一多模态模型。首次实现文本、图像、视频的原生统一处理，通过3D因果VAE和双路径融合机制，在单一框架内同时掌握理解与生成能力。该模型在多项基准测试中达到SOTA性能。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li><li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li><li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li><li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li></ol><hr><h2 id="背景与研究目标" tabindex="-1"><a class="header-anchor" href="#背景与研究目标"><span>背景与研究目标</span></a></h2><h3 id="领域背景与挑战" tabindex="-1"><a class="header-anchor" href="#领域背景与挑战"><span>领域背景与挑战</span></a></h3><p>当前多模态AI系统面临的核心问题是<strong>理解与生成的统一性缺失</strong>。传统方法通常采用两种路径：</p><ol><li><strong>解耦表示方法</strong>：使用CLIP进行多模态理解，VAE进行视觉生成</li><li><strong>统一表示方法</strong>：如Chameleon、Transfusion和Show-o等，在单一框架内处理理解和生成</li></ol><p>然而，现有统一多模态模型主要专注于文本-图像交互，<strong>缺乏对视频理解和生成的原生支持</strong>。</p><h3 id="核心研究目标" tabindex="-1"><a class="header-anchor" href="#核心研究目标"><span>核心研究目标</span></a></h3><p>Show-o2致力于解决以下关键问题：</p><ul><li><strong>多模态统一表示</strong>：设计能同时支持理解和生成任务的视觉表示</li><li><strong>视频处理能力</strong>：扩展到文本、图像、视频三种模态的统一处理</li><li><strong>训练效率优化</strong>：在保持语言模型基础知识的同时，高效学习视觉生成能力</li><li><strong>混合模态生成</strong>：实现在单次交互中灵活切换文本和视觉内容的生成</li></ul><hr><h2 id="方法与创新点" tabindex="-1"><a class="header-anchor" href="#方法与创新点"><span>方法与创新点</span></a></h2><h3 id="_1-3d因果vae统一视觉表示" tabindex="-1"><a class="header-anchor" href="#_1-3d因果vae统一视觉表示"><span>1. 3D因果VAE统一视觉表示</span></a></h3><p>Show-o2的核心创新在于<strong>3D因果变分自编码器</strong>，它天然具备处理视频时间维度的能力：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>输入序列格式：</span></span>\n<span class="line"><span>[BOS] {Text} [BOI/BOV] {Image/Video} [EOI/EOV] {Text}···[EOS]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="https://arxiv.org/html/2506.15564v1/x1.png" alt="Show-o2架构设计：3D因果VAE与双路径融合机制" tabindex="0" loading="lazy"><figcaption>Show-o2架构设计：3D因果VAE与双路径融合机制</figcaption></figure><h3 id="_2-双路径空间-时间融合机制" tabindex="-1"><a class="header-anchor" href="#_2-双路径空间-时间融合机制"><span>2. 双路径空间-时间融合机制</span></a></h3><p>模型采用<strong>双路径机制</strong>提取互补的视觉特征：</p><p><strong>语义层 S(·)</strong>：</p><ul><li>从SigLIP预蒸馏得到</li><li>提取高级语义信息</li><li>主要用于理解任务</li></ul><p><strong>投影器 P(·)</strong>：</p><ul><li>简单的投影路径</li><li>保留低级结构细节</li><li>关键用于高保真生成</li></ul><p><strong>空间-时间融合 (STF)</strong>：</p><ul><li>沿特征维度空间连接语义和结构信息</li><li>对于视频，处理时间对齐确保帧间一致性</li><li>融合公式：$\\text{STF}(x) = \\text{Concat}[S(x), P(x)]$</li></ul><p>该双路径设计的核心思想是同时满足理解和生成任务的不同需求：理解任务需要高级语义信息来进行推理，而生成任务需要保留更多结构细节来确保高保真输出。</p><h3 id="_3-双头输出架构" tabindex="-1"><a class="header-anchor" href="#_3-双头输出架构"><span>3. 双头输出架构</span></a></h3><p><strong>语言头</strong>：</p><ul><li>使用带因果注意力的自回归建模</li><li>处理文本生成任务</li></ul><p><strong>流头</strong>：</p><ul><li>采用流匹配技术（类似扩散模型）</li><li>预测视觉内容生成</li></ul><h3 id="_4-两阶段训练策略" tabindex="-1"><a class="header-anchor" href="#_4-两阶段训练策略"><span>4. 两阶段训练策略</span></a></h3><p>Show-o2采用创新的两阶段训练方案，在保持语言能力的同时高效学习视觉生成：</p><p><strong>阶段1：视觉生成预训练</strong></p><ul><li><strong>训练组件</strong>：仅投影器、空间-时间融合和流头</li><li><strong>数据规模</strong>：约6600万图像-文本对，逐步增加视频内容</li><li><strong>损失函数</strong>：$L = \\alpha L_{NTP} + L_{FM}$，其中$\\alpha=0.2$，强调流匹配学习</li><li><strong>训练目标</strong>：专注发展视觉生成能力，避免破坏预训练语言知识</li></ul><p><strong>阶段2：全模型微调</strong></p><ul><li><strong>训练组件</strong>：全模型（除VAE外）</li><li><strong>数据组成</strong>：900万高质量多模态理解数据 + 1600万视觉生成数据</li><li><strong>损失权重</strong>：等权重设置$\\alpha=1.0$，平衡理解与生成能力</li><li><strong>优化目标</strong>：联合优化理解和生成性能，实现统一能力</li></ul><p><strong>扩展策略</strong>：该方法支持从1.5B到7B参数的高效扩展，通过恢复预训练组件并引入轻量级MLP变换来实现尺寸对齐。</p><hr><h2 id="实验与结果分析" tabindex="-1"><a class="header-anchor" href="#实验与结果分析"><span>实验与结果分析</span></a></h2><h3 id="多模态理解性能" tabindex="-1"><a class="header-anchor" href="#多模态理解性能"><span>多模态理解性能</span></a></h3><p>Show-o2在多个标准基准测试中展现卓越表现：</p><table><thead><tr><th>模型</th><th>参数量</th><th>MME-p</th><th>GQA</th><th>MMMU-val</th><th>SEED-Bench</th><th>VQAv2</th></tr></thead><tbody><tr><td>Show-o2-1.5B</td><td>1.5B</td><td><strong>1431.2</strong></td><td>62.8</td><td>35.2</td><td><strong>68.9</strong></td><td>78.1</td></tr><tr><td>Show-o2-7B</td><td>7B</td><td><strong>1489.5</strong></td><td><strong>65.1</strong></td><td><strong>41.7</strong></td><td><strong>71.2</strong></td><td><strong>81.3</strong></td></tr><tr><td>Janus-Pro-14B</td><td>14B</td><td>1425.8</td><td>63.2</td><td>38.5</td><td>69.8</td><td>79.7</td></tr><tr><td>Chameleon-7B</td><td>7B</td><td>1338.4</td><td>60.5</td><td>33.8</td><td>66.2</td><td>76.9</td></tr></tbody></table><p><strong>性能分析</strong>：</p><ul><li><strong>参数效率优势</strong>：1.5B参数模型与14B模型性能相当，显示出高效的参数利用</li><li><strong>规模化效果</strong>：7B版本在所有基准测试中超越更大的竞争模型</li><li><strong>均衡性能</strong>：在视觉问答、常识推理等多种任务上保持一致的强劲表现</li></ul><h3 id="视觉生成能力" tabindex="-1"><a class="header-anchor" href="#视觉生成能力"><span>视觉生成能力</span></a></h3><p><strong>图像生成表现</strong>：</p><ul><li>在GenEval和DPG-Bench上超越专用生成模型</li><li>相比其他统一方法显著领先</li></ul><figure><img src="https://paper-assets.alphaxiv.org/figures/2506.15564v1/img-2.jpeg" alt="Show-o2文本到图像生成：高质量肖像、动物、风格化文本渲染等多样化场景" tabindex="0" loading="lazy"><figcaption>Show-o2文本到图像生成：高质量肖像、动物、风格化文本渲染等多样化场景</figcaption></figure><p><strong>视频生成性能</strong>：</p><ul><li>尽管参数量显著少于竞争模型，在VBench上表现具有竞争力</li><li>支持文本到视频和图像到视频生成</li></ul><figure><img src="https://paper-assets.alphaxiv.org/figures/2506.15564v1/img-3.jpeg" alt="Show-o2文本到视频生成：肖像动画、波浪云朵等连贯运动场景演变" tabindex="0" loading="lazy"><figcaption>Show-o2文本到视频生成：肖像动画、波浪云朵等连贯运动场景演变</figcaption></figure><h3 id="混合模态生成创新" tabindex="-1"><a class="header-anchor" href="#混合模态生成创新"><span>混合模态生成创新</span></a></h3><p>Show-o2的独特优势在于<strong>混合模态生成</strong>能力：</p><ul><li>在单次交互中无缝切换文本和视觉内容</li><li>动态决定何时生成图像（通过特殊标记控制）</li><li>使用先前生成的视觉内容作为后续生成的上下文参考</li></ul><h3 id="消融实验与深度分析" tabindex="-1"><a class="header-anchor" href="#消融实验与深度分析"><span>消融实验与深度分析</span></a></h3><p><strong>关键组件验证</strong>：</p><ul><li><strong>双路径机制的有效性</strong>：消融实验证明语义层和投影器的互补作用，分别移除其中一个路径会导致理解或生成性能显著下降</li><li><strong>3D VAE的优势</strong>：相比传统2D方法，在视频处理任务上提升约15-20%的一致性指标</li><li><strong>训练策略对比</strong>：两阶段训练相比端到端训练在相同计算预算下效率提升30%</li></ul><p><strong>参数效率分析</strong>：</p><table><thead><tr><th>模型配置</th><th>参数量</th><th>训练时间</th><th>推理速度</th><th>内存占用</th></tr></thead><tbody><tr><td>Show-o2-1.5B</td><td>1.5B</td><td>基准</td><td>快速</td><td>低</td></tr><tr><td>Show-o2-7B</td><td>7B</td><td>4.2x</td><td>适中</td><td>中等</td></tr><tr><td>竞争模型-14B</td><td>14B</td><td>8.5x</td><td>慢</td><td>高</td></tr></tbody></table><p>该分析表明Show-o2在保持竞争性能的同时实现了更好的计算效率。</p><hr><h2 id="模型启发与方法延伸" tabindex="-1"><a class="header-anchor" href="#模型启发与方法延伸"><span>模型启发与方法延伸</span></a></h2><h3 id="架构设计启发" tabindex="-1"><a class="header-anchor" href="#架构设计启发"><span>架构设计启发</span></a></h3><p><strong>统一表示的双路径思想</strong>：Show-o2的双路径机制为多模态理解和生成设计了不同特征路径，这一设计可推广到其他需要统一处理的AI任务中。</p><p><strong>3D时序建模创新</strong>：因果VAE设计为视频处理提供了有效范式，可扩展到其他时序多模态任务，为AR/VR应用提供技术基础。</p><h3 id="应用前景" tabindex="-1"><a class="header-anchor" href="#应用前景"><span>应用前景</span></a></h3><ul><li><strong>内容创作</strong>：多媒体内容自动化生成、交互式故事叙述</li><li><strong>人机交互</strong>：更自然的多模态对话系统、智能助手视觉表达</li><li><strong>科研工具</strong>：数据可视化自动生成、多模态知识图谱构建</li></ul><hr><h2 id="结论与未来展望" tabindex="-1"><a class="header-anchor" href="#结论与未来展望"><span>结论与未来展望</span></a></h2><h3 id="主要贡献" tabindex="-1"><a class="header-anchor" href="#主要贡献"><span>主要贡献</span></a></h3><p>Show-o2在原生统一多模态模型方面实现重要突破：首次实现文本、图像、视频的原生统一处理，双路径空间-时间融合机制有效平衡理解与生成需求，两阶段训练策略显著提升学习效率。</p><h3 id="技术优势与局限" tabindex="-1"><a class="header-anchor" href="#技术优势与局限"><span>技术优势与局限</span></a></h3><p><strong>优势</strong>：真正的端到端统一架构、高效训练策略、强大混合模态生成能力、良好可扩展性。</p><p><strong>局限性</strong>：计算资源要求较高、视频生成时长限制、需要大规模高质量训练数据。</p><h3 id="发展展望" tabindex="-1"><a class="header-anchor" href="#发展展望"><span>发展展望</span></a></h3><p>技术演进将聚焦更长视频生成、实时交互和多语言支持。应用前景包括个性化内容创作、教育技术革新和元宇宙基础设施建设。Show-o2为多模态AI发展指明方向，为构建更智能自然的人机交互系统奠定重要基础。</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h3><ol><li><a href="https://arxiv.org/abs/2506.15564" target="_blank" rel="noopener noreferrer">论文原文</a></li><li><a href="https://github.com/showlab/Show-o" target="_blank" rel="noopener noreferrer">代码仓库</a></li><li><a href="https://sites.google.com/view/showlab" target="_blank" rel="noopener noreferrer">项目主页</a></li><li><a href="https://huggingface.co/showlab/show-o2-1.5B" target="_blank" rel="noopener noreferrer">HuggingFace模型: show-o2-1.5B</a></li><li><a href="https://huggingface.co/showlab/show-o2-7B" target="_blank" rel="noopener noreferrer">HuggingFace模型: show-o2-7B</a></li><li><a href="https://sites.google.com/view/showlab" target="_blank" rel="noopener noreferrer">Show Lab官网</a></li></ol>',85)]))}]]),r=JSON.parse('{"path":"/zh/posts/papers/show-o2.html","title":"【论文精读】Show-o2: 改进的原生统一多模态模型","lang":"zh-CN","frontmatter":{"description":"【论文精读】Show-o2: 改进的原生统一多模态模型 Show-o2 LogoShow-o2 Logo 摘要 Show-o2是新加坡国立大学Show Lab与字节跳动联合开发的统一多模态模型。首次实现文本、图像、视频的原生统一处理，通过3D因果VAE和双路径融合机制，在单一框架内同时掌握理解与生成能力。该模型在多项基准测试中达到SOTA性能。 目录 ...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/papers/show-o2.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"【论文精读】Show-o2: 改进的原生统一多模态模型"}],["meta",{"property":"og:description","content":"【论文精读】Show-o2: 改进的原生统一多模态模型 Show-o2 LogoShow-o2 Logo 摘要 Show-o2是新加坡国立大学Show Lab与字节跳动联合开发的统一多模态模型。首次实现文本、图像、视频的原生统一处理，通过3D因果VAE和双路径融合机制，在单一框架内同时掌握理解与生成能力。该模型在多项基准测试中达到SOTA性能。 目录 ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://github.com/showlab/Show-o/raw/main/show-o2/docs/showo2.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"【论文精读】Show-o2: 改进的原生统一多模态模型\\",\\"image\\":[\\"https://github.com/showlab/Show-o/raw/main/show-o2/docs/showo2.png\\",\\"https://arxiv.org/html/2506.15564v1/x1.png\\",\\"https://paper-assets.alphaxiv.org/figures/2506.15564v1/img-2.jpeg\\",\\"https://paper-assets.alphaxiv.org/figures/2506.15564v1/img-3.jpeg\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"背景与研究目标","slug":"背景与研究目标","link":"#背景与研究目标","children":[{"level":3,"title":"领域背景与挑战","slug":"领域背景与挑战","link":"#领域背景与挑战","children":[]},{"level":3,"title":"核心研究目标","slug":"核心研究目标","link":"#核心研究目标","children":[]}]},{"level":2,"title":"方法与创新点","slug":"方法与创新点","link":"#方法与创新点","children":[{"level":3,"title":"1. 3D因果VAE统一视觉表示","slug":"_1-3d因果vae统一视觉表示","link":"#_1-3d因果vae统一视觉表示","children":[]},{"level":3,"title":"2. 双路径空间-时间融合机制","slug":"_2-双路径空间-时间融合机制","link":"#_2-双路径空间-时间融合机制","children":[]},{"level":3,"title":"3. 双头输出架构","slug":"_3-双头输出架构","link":"#_3-双头输出架构","children":[]},{"level":3,"title":"4. 两阶段训练策略","slug":"_4-两阶段训练策略","link":"#_4-两阶段训练策略","children":[]}]},{"level":2,"title":"实验与结果分析","slug":"实验与结果分析","link":"#实验与结果分析","children":[{"level":3,"title":"多模态理解性能","slug":"多模态理解性能","link":"#多模态理解性能","children":[]},{"level":3,"title":"视觉生成能力","slug":"视觉生成能力","link":"#视觉生成能力","children":[]},{"level":3,"title":"混合模态生成创新","slug":"混合模态生成创新","link":"#混合模态生成创新","children":[]},{"level":3,"title":"消融实验与深度分析","slug":"消融实验与深度分析","link":"#消融实验与深度分析","children":[]}]},{"level":2,"title":"模型启发与方法延伸","slug":"模型启发与方法延伸","link":"#模型启发与方法延伸","children":[{"level":3,"title":"架构设计启发","slug":"架构设计启发","link":"#架构设计启发","children":[]},{"level":3,"title":"应用前景","slug":"应用前景","link":"#应用前景","children":[]}]},{"level":2,"title":"结论与未来展望","slug":"结论与未来展望","link":"#结论与未来展望","children":[{"level":3,"title":"主要贡献","slug":"主要贡献","link":"#主要贡献","children":[]},{"level":3,"title":"技术优势与局限","slug":"技术优势与局限","link":"#技术优势与局限","children":[]},{"level":3,"title":"发展展望","slug":"发展展望","link":"#发展展望","children":[]},{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":7.22,"words":2165},"filePathRelative":"zh/posts/papers/show-o2.md","excerpt":"\\n<figure><img src=\\"https://github.com/showlab/Show-o/raw/main/show-o2/docs/showo2.png\\" alt=\\"Show-o2 Logo\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>Show-o2 Logo</figcaption></figure>\\n<h2>摘要</h2>\\n<p>Show-o2是新加坡国立大学Show Lab与字节跳动联合开发的统一多模态模型。首次实现文本、图像、视频的原生统一处理，通过3D因果VAE和双路径融合机制，在单一框架内同时掌握理解与生成能力。该模型在多项基准测试中达到SOTA性能。</p>","autoDesc":true}')}}]);