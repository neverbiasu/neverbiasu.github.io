"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[7809],{66262:(e,i)=>{i.A=(e,i)=>{const n=e.__vccOpts||e;for(const[e,a]of i)n[e]=a;return n}},45651:(e,i,n)=>{n.r(i),n.d(i,{comp:()=>r,data:()=>g});var a=n(20641);const t={},r=(0,n(66262).A)(t,[["render",function(e,i){return(0,a.uX)(),(0,a.CE)("div",null,i[0]||(i[0]=[(0,a.Fv)('<h1 id="qwen-image顶级文本渲染与编辑-unipic1-5b模型统一多模态生成-longvie超长视频生成【ai周报】" tabindex="-1"><a class="header-anchor" href="#qwen-image顶级文本渲染与编辑-unipic1-5b模型统一多模态生成-longvie超长视频生成【ai周报】"><span>Qwen-Image顶级文本渲染与编辑|UniPic1.5B模型统一多模态生成|LongVie超长视频生成【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg" alt="封面图源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面图源自C站作者Koal2</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：Qwen-Image聚焦复杂文本渲染与高一致性图像编辑，UniPic实现统一架构下的多模态理解、生成与编辑，LongVie突破极长视频的可控生成与一致性。更多精彩内容详见正文，相关参考链接请见文末。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#qwen%E2%80%91image%E9%9D%A2%E5%90%91%E5%A4%8D%E6%9D%82%E6%96%87%E6%9C%AC%E6%8E%92%E7%89%88%E4%B8%8E%E7%B2%BE%E7%A1%AE%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91%E7%9A%84-20b-%E5%9B%BE%E5%83%8F%E5%9F%BA%E5%BA%A7%E6%A8%A1%E5%9E%8B">Qwen‑Image：面向复杂文本排版与精确图像编辑的 20B 图像基座模型</a></li><li><a href="#unipic15b%E5%8F%82%E6%95%B0%E7%BB%9F%E4%B8%80%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3%E7%94%9F%E6%88%90%E4%B8%8E%E7%BC%96%E8%BE%91%E6%A8%A1%E5%9E%8B">UniPic：1.5B参数统一视觉理解、生成与编辑模型</a></li><li><a href="#pixnerd%E7%9B%B4%E6%8E%A5%E5%9C%A8%E5%83%8F%E7%B4%A0%E7%A9%BA%E9%97%B4%E9%AB%98%E6%95%88%E7%94%9F%E6%88%90%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%9B%BE%E5%83%8F%E7%9A%84%E6%96%B0%E8%8C%83%E5%BC%8F">PixNerd：直接在像素空间高效生成高精度图像的新范式</a></li><li><a href="#magenta-realtimegoogle-%E5%BC%80%E6%BA%90%E7%9A%84%E5%AE%9E%E6%97%B6%E4%BA%A4%E4%BA%92%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">Magenta RealTime：Google 开源的实时交互音乐生成模型</a></li><li><a href="#uniedit-i%E9%9B%B6%E8%AE%AD%E7%BB%83%E7%BB%9F%E4%B8%80%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91%E6%A1%86%E6%9E%B6">UniEdit-I：零训练统一视觉语言模型图像编辑框架</a></li><li><a href="#longvie%E5%A4%9A%E6%A8%A1%E6%80%81%E5%BC%95%E5%AF%BC%E7%9A%84%E5%8F%AF%E6%8E%A7%E6%9E%81%E9%95%BF%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E6%A1%86%E6%9E%B6">LongVie：多模态引导的可控极长视频生成框架</a></li></ol><hr><h2 id="qwen‐image-面向复杂文本排版与精确图像编辑的-20b-图像基座模型" tabindex="-1"><a class="header-anchor" href="#qwen‐image-面向复杂文本排版与精确图像编辑的-20b-图像基座模型"><span>Qwen‑Image：面向复杂文本排版与精确图像编辑的 20B 图像基座模型</span></a></h2><figure><img src="https://camo.githubusercontent.com/842722e31238859e5605fb52bb0ae20b431f952346c2cc4f65b8245a601ba49f/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e2d496d6167652f6d65726765332e6a7067" alt="Qwen‑Image Teaser 图" tabindex="0" loading="lazy"><figcaption>Qwen‑Image Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>Qwen‑Image</strong> 是<strong>千问团队</strong>发布的 20B MMDiT 图像基座模型，专注解决「复杂文本在图像中原生渲染」与「高一致性的图像编辑」两大痛点。团队构建了大规模的文本-图像流水线（收集、过滤、注释、合成与均衡），并采用从简单到复杂的渐进训练策略（curriculum learning）以提升段落级、多行多字的本地文本渲染能力；同时通过双编码（将语义编码器与 VAE 重建编码分离）和多任务联合训练，增强编辑模块在保留语义一致性与视觉保真间的平衡。官方报告与基准测试显示 Qwen‑Image 在文本渲染（含中文）与图像编辑多项指标上处于领先水平，并已发布模型权重、技术报告与使用示例。</p><p><strong>标签</strong>：#文本渲染 #图像编辑 #多任务训练 #双编码机制 #大规模数据流水线</p><hr><h2 id="unipic-1-5b参数统一视觉理解、生成与编辑模型" tabindex="-1"><a class="header-anchor" href="#unipic-1-5b参数统一视觉理解、生成与编辑模型"><span>UniPic：1.5B参数统一视觉理解、生成与编辑模型</span></a></h2><figure><img src="https://github.com/SkyworkAI/UniPic/raw/main/teaser.png" alt="UniPic Teaser 图" tabindex="0" loading="lazy"><figcaption>UniPic Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>UniPic</strong> 是<strong>天工AI</strong>团队推出的 1.5B 参数统一自回归多模态模型，首次通过单一架构同时支持图像理解、文本生成图像与图像编辑任务。其创新架构结合被掩蔽的自回归编码器（用于图像合成）与 SigLIP2 编码器（用于理解），并通过共享的自回归解码器实现三任务融合。训练采用了从 256×256 到 1024×1024 的渐进训练策略、分阶段参数解冻以及精心构建的大规模多模态数据集。UniPic 在 GenEval（0.86）、DPG-Bench（85.5）、GEditBench-EN（5.83）、ImgEdit-Bench（3.49）基准上取得了 SOTA 成绩，并能在 RTX 4090 硬件下以低于 15GB 显存生成高分辨率图像。</p><p><strong>标签</strong>：#统一多模态 #自回归模型 #分离编码 #渐进训练 #高效部署</p><hr><h2 id="pixnerd-直接在像素空间高效生成高精度图像的新范式" tabindex="-1"><a class="header-anchor" href="#pixnerd-直接在像素空间高效生成高精度图像的新范式"><span>PixNerd：直接在像素空间高效生成高精度图像的新范式</span></a></h2><figure><img src="https://github.com/MCG-NJU/PixNerd/raw/main/figs/arch.png" alt="PixNerd Architecture 图" tabindex="0" loading="lazy"><figcaption>PixNerd Architecture 图</figcaption></figure><p><strong>概要</strong>：<strong>PixNerd</strong>（Pixel Neural Field Diffusion）由<strong>南京大学 MCG</strong> 团队提出，是一种跳过 VAE、直接在像素空间生成图像的扩散 Transformer。它采用**神经场（Neural Field）**替代传统线性投影，用于处理大 patch 解码，从而实现单阶段、端到端的高效训练流程。模型在 ImageNet 上达到了出色的生成性能：256×256 分辨率 FID 为 2.15，512×512 为 2.84；在文本到图像任务上，PixNerd-XXL/16 在 GenEval 和 DPG 基准上分别取得 0.73 和 80.9 的高分表现，展示了在高保真图像生成及多任务扩展方面的强大能力。</p><p><strong>标签</strong>：#像素扩散模型 #神经场解码 #端到端训练 #高保真生成 #跨任务扩展</p><hr><h2 id="magenta-realtime-google-开源的实时交互音乐生成模型" tabindex="-1"><a class="header-anchor" href="#magenta-realtime-google-开源的实时交互音乐生成模型"><span>Magenta RealTime：Google 开源的实时交互音乐生成模型</span></a></h2><figure><img src="https://arxiv.org/html/2508.04651v1/x1.png" alt="Magenta RealTime Pipeline 图" tabindex="0" loading="lazy"><figcaption>Magenta RealTime Pipeline 图</figcaption></figure><p><strong>概要</strong>：<strong>Magenta RealTime</strong> 是 <strong>Google Magenta</strong> 项目最新发布的开源直播音乐生成模型，由 Lyria 团队开发。它基于约 8 亿参数的自回归 Transformer 网络，并结合 SpectroStream 音频编码器与 MusicCoCa 风格嵌入模型，实现每 2 秒生成一段高质量立体声音轨，具备实时响应文本或音频提示控制音色与风格的能力。在自动评估指标上，它领先于其他开源离线音乐生成模型，同时因模型轻量而适配本地部署，支持 Colab Demo 和未来设备端推理部署。</p><p><strong>标签</strong>：#实时音乐生成 #自回归模型 #交互式控制 #低延迟 #风格融合</p><hr><h2 id="uniedit-i-零训练统一视觉语言模型图像编辑框架" tabindex="-1"><a class="header-anchor" href="#uniedit-i-零训练统一视觉语言模型图像编辑框架"><span>UniEdit-I：零训练统一视觉语言模型图像编辑框架</span></a></h2><figure><img src="https://arxiv.org/html/2508.03142v1/x1.png" alt="UniEdit-I Teaser 图" tabindex="0" loading="lazy"><figcaption>UniEdit-I Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>UniEdit-I</strong> 是<strong>字节跳动</strong>发布的一个无需训练便可为统一视觉语言模型（VLM）赋予图像编辑能力的系统。它采用「理解—编辑—验证」的三步迭代流程：首先对源图像进行语义结构解析生成提示（Understanding），再引入时间自适应偏移执行逐步降噪编辑（Editing），最后通过验证模块评估编辑一致性并决定是否继续或停止（Verifying）。该方法在无需额外训练的前提下，实现了高保真、高一致性的图像编辑，并在 GEdit-Bench 上取得领先表现。</p><p><strong>标签</strong>：#图像编辑 #训练免除 #迭代理解流程 #统一VLM #高保真一致性</p><hr><h2 id="longvie-多模态引导的可控极长视频生成框架" tabindex="-1"><a class="header-anchor" href="#longvie-多模态引导的可控极长视频生成框架"><span>LongVie：多模态引导的可控极长视频生成框架</span></a></h2><figure><img src="https://vchitect.github.io/LongVie-project/assets/images/teaser.jpg" alt="LongVie Teaser 图" tabindex="0" loading="lazy"><figcaption>LongVie Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>LongVie</strong> 是<strong>南京大学</strong>、<strong>南洋理工S-Lab</strong>、<strong>英伟达</strong>等团队联合提出的首个专注于<strong>可控、长时长（最长达1分钟）视频生成</strong>端到端自回归框架，。它通过统一的噪声初始化和全局控制信号归一化策略确保生成一致性，并结合<strong>密集（如深度图）与稀疏（如关键点）控制</strong>信号与退化感知训练机制，有效解决时间不连贯和质量退化问题。团队还发布了 <strong>LongVGenBench</strong> 基准，包含 100 条高质量的长视频，验证 LongVie 在可控性、一致性与视觉质量上实现领先表现。</p><p><strong>标签</strong>：#超长视频生成 #生成一致性 #多模态控制 #退化感知训练 #LongVGenBench</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://github.com/QwenLM/Qwen-Image" target="_blank" rel="noopener noreferrer">Qwen‑Image Github 仓库</a></li><li><a href="https://arxiv.org/abs/2508.02324" target="_blank" rel="noopener noreferrer">Qwen‑Image 论文</a></li><li><a href="https://github.com/SkyworkAI/UniPic" target="_blank" rel="noopener noreferrer">UniPic Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.03320" target="_blank" rel="noopener noreferrer">UniPic 论文</a></li><li><a href="https://huggingface.co/spaces/MCG-NJU/PixNerd" target="_blank" rel="noopener noreferrer">PixNerd Hugging Face Demo</a></li><li><a href="https://github.com/MCG-NJU/PixNerd" target="_blank" rel="noopener noreferrer">PixNerd Github 仓库</a></li><li><a href="https://arxiv.org/html/2507.23268" target="_blank" rel="noopener noreferrer">PixNerd 论文</a></li><li><a href="https://magenta.withgoogle.com/magenta-realtime" target="_blank" rel="noopener noreferrer">Magenta RealTime 项目主页</a></li><li><a href="https://github.com/magenta/magenta-realtime" target="_blank" rel="noopener noreferrer">Magenta RealTime Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.04651v1" target="_blank" rel="noopener noreferrer">Magenta RealTime 论文</a></li><li><a href="https://arxiv.org/html/2508.03142v1" target="_blank" rel="noopener noreferrer">UniEdit-I 论文</a></li><li><a href="https://vchitect.github.io/LongVie-project/" target="_blank" rel="noopener noreferrer">LongVie 项目主页</a></li><li><a href="https://github.com/Vchitect/LongVie" target="_blank" rel="noopener noreferrer">LongVie Github 仓库</a></li><li><a href="https://arxiv.org/html/2508.03694v1" target="_blank" rel="noopener noreferrer">LongVie 论文</a></li></ol>',40)]))}]]),g=JSON.parse('{"path":"/zh/posts/ai-weekly/050.html","title":"Qwen-Image顶级文本渲染与编辑|UniPic1.5B模型统一多模态生成|LongVie超长视频生成【AI周报】","lang":"zh-CN","frontmatter":{"description":"Qwen-Image顶级文本渲染与编辑|UniPic1.5B模型统一多模态生成|LongVie超长视频生成【AI周报】 封面图源自C站作者Koal2封面图源自C站作者Koal2 摘要 本周亮点：Qwen-Image聚焦复杂文本渲染与高一致性图像编辑，UniPic实现统一架构下的多模态理解、生成与编辑，LongVie突破极长视频的可控生成与一致性。更多精...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/050.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"Qwen-Image顶级文本渲染与编辑|UniPic1.5B模型统一多模态生成|LongVie超长视频生成【AI周报】"}],["meta",{"property":"og:description","content":"Qwen-Image顶级文本渲染与编辑|UniPic1.5B模型统一多模态生成|LongVie超长视频生成【AI周报】 封面图源自C站作者Koal2封面图源自C站作者Koal2 摘要 本周亮点：Qwen-Image聚焦复杂文本渲染与高一致性图像编辑，UniPic实现统一架构下的多模态理解、生成与编辑，LongVie突破极长视频的可控生成与一致性。更多精..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Qwen-Image顶级文本渲染与编辑|UniPic1.5B模型统一多模态生成|LongVie超长视频生成【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg\\",\\"https://camo.githubusercontent.com/842722e31238859e5605fb52bb0ae20b431f952346c2cc4f65b8245a601ba49f/68747470733a2f2f7169616e77656e2d7265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f5177656e2d496d6167652f6d65726765332e6a7067\\",\\"https://github.com/SkyworkAI/UniPic/raw/main/teaser.png\\",\\"https://github.com/MCG-NJU/PixNerd/raw/main/figs/arch.png\\",\\"https://arxiv.org/html/2508.04651v1/x1.png\\",\\"https://arxiv.org/html/2508.03142v1/x1.png\\",\\"https://vchitect.github.io/LongVie-project/assets/images/teaser.jpg\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"Qwen‑Image：面向复杂文本排版与精确图像编辑的 20B 图像基座模型","slug":"qwen‐image-面向复杂文本排版与精确图像编辑的-20b-图像基座模型","link":"#qwen‐image-面向复杂文本排版与精确图像编辑的-20b-图像基座模型","children":[]},{"level":2,"title":"UniPic：1.5B参数统一视觉理解、生成与编辑模型","slug":"unipic-1-5b参数统一视觉理解、生成与编辑模型","link":"#unipic-1-5b参数统一视觉理解、生成与编辑模型","children":[]},{"level":2,"title":"PixNerd：直接在像素空间高效生成高精度图像的新范式","slug":"pixnerd-直接在像素空间高效生成高精度图像的新范式","link":"#pixnerd-直接在像素空间高效生成高精度图像的新范式","children":[]},{"level":2,"title":"Magenta RealTime：Google 开源的实时交互音乐生成模型","slug":"magenta-realtime-google-开源的实时交互音乐生成模型","link":"#magenta-realtime-google-开源的实时交互音乐生成模型","children":[]},{"level":2,"title":"UniEdit-I：零训练统一视觉语言模型图像编辑框架","slug":"uniedit-i-零训练统一视觉语言模型图像编辑框架","link":"#uniedit-i-零训练统一视觉语言模型图像编辑框架","children":[]},{"level":2,"title":"LongVie：多模态引导的可控极长视频生成框架","slug":"longvie-多模态引导的可控极长视频生成框架","link":"#longvie-多模态引导的可控极长视频生成框架","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":6.11,"words":1833},"filePathRelative":"zh/posts/ai-weekly/050.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg\\" alt=\\"封面图源自C站作者Koal2\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面图源自C站作者Koal2</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：Qwen-Image聚焦复杂文本渲染与高一致性图像编辑，UniPic实现统一架构下的多模态理解、生成与编辑，LongVie突破极长视频的可控生成与一致性。更多精彩内容详见正文，相关参考链接请见文末。</p>","autoDesc":true}')}}]);