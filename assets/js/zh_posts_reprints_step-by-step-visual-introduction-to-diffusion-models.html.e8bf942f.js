"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[5113],{66262:(e,t)=>{t.A=(e,t)=>{const a=e.__vccOpts||e;for(const[e,i]of t)a[e]=i;return a}},41058:(e,t,a)=>{a.r(t),a.d(t,{comp:()=>n,data:()=>p});var i=a(20641);const s={},n=(0,a(66262).A)(s,[["render",function(e,t){return(0,i.uX)(),(0,i.CE)("div",null,t[0]||(t[0]=[(0,i.Fv)('<h1 id="diffusion-models-可视化逐步入门" tabindex="-1"><a class="header-anchor" href="#diffusion-models-可视化逐步入门"><span>Diffusion Models 可视化逐步入门</span></a></h1><blockquote><p>本文为原文《Step by Step visual introduction to Diffusion Models》的中文翻译，原作者 Kemal Erdem。</p></blockquote><h2 id="什么是扩散模型" tabindex="-1"><a class="header-anchor" href="#什么是扩散模型"><span>什么是扩散模型？</span></a></h2><p>扩散模型的概念并不久远。2015 年的论文《Deep Unsupervised Learning using Nonequilibrium Thermodynamics》中，作者这样描述：</p><blockquote><p>其核心思想，受非平衡统计物理学启发，是通过<strong>逐步的前向扩散过程</strong>系统性地、缓慢地破坏数据分布中的结构。随后我们学习一个<strong>反向扩散过程</strong>，恢复数据结构，从而获得对数据高度灵活且易于处理的生成模型。</p></blockquote><p>这里的<strong>扩散过程</strong>分为<strong>前向</strong>和<strong>反向</strong>两个阶段。前向扩散过程是将一张图片变成噪声，反向扩散过程则试图将噪声还原为图片。</p><h2 id="前向扩散过程" tabindex="-1"><a class="header-anchor" href="#前向扩散过程"><span>前向扩散过程</span></a></h2><p>如果上面的定义还不够清楚，别担心，我们可以解释其原理和实现方式。首先，你需要知道如何破坏数据分布中的结构。</p><div style="display:flex;gap:2%;align-items:flex-start;"><div style="flex:1;text-align:center;"><img src="https://erdem.pl/static/82f2bcfc15e077527beecb52281869e5/e4a55/noise0.jpg" alt="" style="max-width:100%;height:auto;"><div>(a) 原始图片</div></div><div style="flex:1;text-align:center;"><img src="https://erdem.pl/static/3c7977d706ed0a2c269e61c2e91af0ce/e4a55/noise10.jpg" alt="" style="max-width:100%;height:auto;"><div>(b) 纯噪声</div></div></div><p>图 1：前向扩散过程的输入与输出</p><p>如果我们取任意一张图片（如图 1a），它有某种非随机分布。我们虽然不知道具体分布，但目标是通过加噪声来破坏它。最终应得到类似纯噪声的结果。</p><p>图 2：仅用 10 步的前向扩散过程</p><p>每一步的前向扩散过程定义为：</p><p>$$ q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I) $$</p><p>其中 $q$ 是前向过程，$x_t$ 是第 $t$ 步的输出，$x_{t-1}$ 是输入，$\\mathcal{N}$ 表示正态分布，$\\sqrt{1-\\beta_t}x_{t-1}$ 是均值，$\\beta_t I$ 是方差。</p><h4 id="调度表-schedule" tabindex="-1"><a class="header-anchor" href="#调度表-schedule"><span>调度表（Schedule）</span></a></h4><p>$\\beta_t$ 称为“调度表”，取值范围通常在 $0$ 到 $1$ 之间。2020 年的论文[2]使用线性调度：</p><figure><img src="https://erdem.pl/static/c5e1011bc11d7f13fdf19295c0e94c3e/eea4a/linear_noise.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图 3：使用 1000 步线性调度的前向扩散过程</p><p>此时 $\\beta_t$ 从 <strong>0.0001</strong> 到 <strong>0.02</strong>，均值和方差变化如图 4 所示。</p><figure><img src="https://erdem.pl/static/6bf29ce5feb6b15753aff769147e7be8/21b4d/lin_schedule_beta.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图 4：不同步长下均值和方差的变化</p><p>OpenAI 2021 年的论文[3]认为线性调度效率不高。正如你所见，原图大部分信息在一半步数后就丢失了。他们设计了自己的“余弦调度”（cosine schedule，见图 5），使步数减少到 50。</p><figure><img src="https://erdem.pl/static/eb1c5091b8115b21499e408a8edf7b82/eea4a/cos_noise.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图 5：使用余弦调度的前向扩散过程</p><h4 id="实际加噪过程-只需关注最后一个公式" tabindex="-1"><a class="header-anchor" href="#实际加噪过程-只需关注最后一个公式"><span>实际加噪过程（只需关注最后一个公式）</span></a></h4><p>你可以想象，前向扩散过程加噪会很慢。训练时并不是严格按顺序采样，而是直接从任意 $t$ 采样。2020 年论文提出：</p><p>$$ q(x_{1:T}|x_0) := \\prod_{t=1}^{T} q(x_t|x_{t-1}) $$</p><p>这个过程可以看作函数复合：</p><p>$$ q_t(q_{t-1}(q_{t-2}(\\cdots q_1(x_0)))) $$</p><p>$t=1$ 时：</p><p>$$ q(x_1|x_0) = \\mathcal{N}(x_1; \\sqrt{1-\\beta_1}x_0, \\beta_1 I) $$</p><p>$t=2$ 时：</p><p>$$ q(x_2|x_1) = \\mathcal{N}(x_2; \\sqrt{1-\\beta_2}x_1, \\beta_2 I) $$</p><p>引入记号：</p><ul><li>$\\alpha_t = 1 - \\beta_t$</li><li>$\\bar\\alpha_t := \\prod_{s=1}^t \\alpha_s$</li></ul><p>则有：</p><p>$$ q(x_1|x_0) = \\mathcal{N}(x_1; \\sqrt{\\alpha_1}x_0, (1-\\alpha_1)I) $$</p><p>均值递推：</p><p>$$ \\mu_t = \\sqrt{\\bar\\alpha_t}x_0 $$</p><p>最终整体公式：</p><p>$$ q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar\\alpha_t}x_0, (1-\\bar\\alpha_t)I) $$</p><p>单步重参数化：</p><p>$$ q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I) = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t}\\epsilon $$</p><p>其中 $\\epsilon \\sim \\mathcal{N}(0, 1)$。</p><h2 id="反向扩散过程" tabindex="-1"><a class="header-anchor" href="#反向扩散过程"><span>反向扩散过程</span></a></h2><p>你可能已经猜到，反向扩散过程的目标是将纯噪声还原为图片。为此，我们会用到神经网络（暂不讨论架构，后文会详细介绍）。如果你熟悉 GAN（生成对抗网络，见图 6），我们要训练的类似于生成器网络。不同之处在于，我们的网络每次只需做一小步，任务更轻松。</p><figure><img src="https://erdem.pl/869cd128ff69bb1d304675166e8b7b34/gan_diagram_generator.svg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图 6：GAN 架构</p><p>那么为什么不用 GAN 呢？许多聪明人花了很长时间才让 GAN 取得不错的效果。要让网络直接把随机噪声变成有意义的图片，训练极其困难。2015 年的论文[1]作者发现，采用多步框架、每次只去除一部分噪声，更高效也更易训练。</p><blockquote><p>在这个框架下，学习的本质是估计扩散过程中的微小扰动。<strong>估计小扰动比用单一、不可解析归一化的势函数描述完整分布更易处理</strong>。此外，只要目标分布是光滑的，扩散过程总能拟合任意形式的数据分布。</p></blockquote><h3 id="关于反向扩散的误解" tabindex="-1"><a class="header-anchor" href="#关于反向扩散的误解"><span>关于反向扩散的误解</span></a></h3><p>你可能听说过“扩散概率模型是参数化马尔可夫链”。这没错，但很多人误解了扩散模型中的<strong>神经网络</strong>作用。2020 年论文[2]用下图描述了过程：</p><figure><img src="https://erdem.pl/static/897bdd95a9ca14cff82e0257de0dccc3/ee515/graphical_diffusion_model.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图 7：有向图模型</p><p>通常神经网络被这样可视化：</p><figure><img src="https://erdem.pl/static/512d5abbaa2b507956d1b737f85cee1b/34e70/one_diffusion_step.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图 8：反向扩散单步高层可视化</p><p>因此，很多人以为神经网络（也叫 diffusion model）是在去除输入图片的噪声，或预测要去除的噪声。其实都不对。<strong>扩散模型预测的是在给定步长下需要去除的全部噪声</strong>。比如步长 $t=600$，模型预测的是去除全部噪声后应还原到 $t=0$，而不是 $t=599$。后文会详细解释，先看反向扩散的逐步过程。</p><p>注意：我把步数从 1000 缩减到 10，是为了让人类更容易比较每一步的结果。</p><p>图 9：反向扩散过程</p><h3 id="一些数学推导-可跳过-但值得一读" tabindex="-1"><a class="header-anchor" href="#一些数学推导-可跳过-但值得一读"><span>一些数学推导（可跳过，但值得一读）</span></a></h3><p>过程看似简单，但你可能会问“这些输出公式从哪来的？”我们先引用 2020 年论文[2]的反向过程公式：</p><p>$$ p_\\theta(x_{0:T}) := p(x_T) \\prod_{t=1}^{T} p_\\theta(x_{t-1}|x_t) $$</p><p>其中：</p><p>$$ p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t,t), \\Sigma_\\theta(x_t,t)) $$</p><p>这其实就是：扩散过程 $p_\\theta(x_{0:T})$ 是一串高斯转移链，从 $p(x_T)$ 开始，迭代 $T$ 次，每次用 $p_\\theta(x_{t-1}|x_t)$。</p><p>单步公式有两部分：</p><ul><li>$\\mu_\\theta(x_t,t)$（均值）</li><li>$\\Sigma_\\theta(x_t,t)$，等于 $\\sigma_t^2 I$（方差）</li></ul><p>2020 年论文设定方差为<strong>随时间变化但不可训练</strong>，即 $\\beta_T I$，与前文调度表一致。只剩均值部分。更详细的数学推导可参考 Lilian Weng 的博客[6] 或论文附录。</p><p>我们只需知道：</p><p>$$ \\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar\\alpha_t}} \\epsilon_\\theta(x_t, t) \\right) $$</p><p>因此：</p><p>$$ x_{t-1} = \\mathcal{N}\\left(x_{t-1}, \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar\\alpha_t}} \\epsilon_\\theta(x_t, t)), \\sqrt{\\beta_t}\\epsilon\\right) $$</p><p>可用于计算任意步长 $t$ 的输出：</p><p>$$ x_{t-1} = \\frac{1}{\\sqrt{a_t}} \\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha}} \\epsilon_\\theta(x_t,t)\\right) + \\sqrt{\\beta_t}\\epsilon $$</p><p>其中 $\\epsilon_\\theta(x_t,t)$ 是<strong>模型输出</strong>（预测噪声）。</p><p>如图 9，最后一步不再加 $\\sqrt{\\beta_t}\\epsilon$，否则无法去除。</p><h3 id="反向扩散输出可视化" tabindex="-1"><a class="header-anchor" href="#反向扩散输出可视化"><span>反向扩散输出可视化</span></a></h3><p>在介绍架构前，先展示一个有趣的现象。每次神经网络预测噪声后，我们减去一部分并进入下一步，这就是扩散过程。但**如果每步都减去全部噪声会怎样？**我用线性调度（1~50 步）做了实验：</p><blockquote><p>注意！反向过程下，$t=1$ 时 $\\beta_t$ 实际为 $\\beta_{T-t+1}$，即 $t=1$ 用 $\\beta_{50}$，$t=2$ 用 $\\beta_{49}$，以此类推。</p></blockquote><figure><img src="https://erdem.pl/static/90207efb3cb80cde5f359b2205b0303a/0f840/sample_full_noise_removal.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图 10：全部噪声移除的结果</p><p>中间是第 $t$ 步输入，$t=0$ 时为纯噪声。右侧是神经网络预测的噪声，左侧是每步输入减去全部噪声的结果。只展示部分步长，完整过程见 <a href="https://drive.google.com/file/d/13UCkMZCs_AktbkEAAmJ_jeumuZlGFC0e/view?usp=sharing" target="_blank" rel="noopener noreferrer">gDrive</a>。</p><p>如文首所述，扩散模型本质上类似 GAN 的生成器，但单步去噪效果更差。你会发现，第一步移除全部噪声的结果与最终生成图像很接近。这是因为我们训练模型预测的是全部噪声，而不是差分。理论上完美模型应能预测出能还原正确图片的全部噪声，但这几乎不可能。</p><p>你可以得出两点：</p><ol><li>推理时可用<strong>更少步长</strong></li><li>推理时可用<strong>不同调度表</strong></li></ol><p>第一点很直观，模型预测的噪声已很接近目标时，可以“跳步”。第二点则是可以用不同斜率的调度表（如训练用线性，推理用余弦）。</p><h2 id="架构" tabindex="-1"><a class="header-anchor" href="#架构"><span>架构</span></a></h2><p>最后我们来讨论模型架构。</p><p>喜欢看 PyTorch 可视化的同学可参考 <a href="https://drive.google.com/file/d/1XVwlD8wuTazW2myf4sV-TZF_YkIfwL7z/view?usp=sharing" target="_blank" rel="noopener noreferrer">完整模型结构（gDrive）</a>。</p><p>图 11：Diffusion model 架构</p><p>模型采用改进版 U-Net[7]，基础结构简单，后续如 Stable Diffusion 等会加入更多特性（如潜空间编码）。本文只讲最基础版本，理解后可举一反三。</p><h4 id="embeddings" tabindex="-1"><a class="header-anchor" href="#embeddings"><span>Embeddings</span></a></h4><p>每一步都要输入步长和 prompt 信息。实际上，每步都要加上步长和 prompt 的 embedding（最早的 diffusion model 不支持 prompt）。步长用<strong>正弦位置编码</strong>，prompt 用<strong>嵌入器</strong>。相关原理可参考<a href="https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers#positional-encoding-visualization" target="_blank" rel="noopener noreferrer">transformer 位置编码</a>。</p><h5 id="嵌入器" tabindex="-1"><a class="header-anchor" href="#嵌入器"><span>嵌入器</span></a></h5><p>嵌入器可以是任意网络。最早的条件扩散模型（支持 prompt）用的很简单，比如本文实验用的 CIFAR-10 数据集只有 10 类，嵌入器只需编码类别。如果数据集更复杂，或无标注，可用如 CLIP 之类的嵌入器，训练时也要用同样的嵌入器。</p><p>位置编码和文本嵌入输出相加后，输入到下采样和上采样模块。</p><h5 id="resnet-block" tabindex="-1"><a class="header-anchor" href="#resnet-block"><span>ResNet Block</span></a></h5><figure><img src="https://erdem.pl/static/18fc014ed4e2d9fe4d1439b3431ccad9/d38a6/conv_block.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图 12：ResNet block</p><p>ResNet block 结构简单，后续会作为下采样和上采样模块的组成部分。</p><h5 id="downsample-block" tabindex="-1"><a class="header-anchor" href="#downsample-block"><span>Downsample Block</span></a></h5><figure><img src="https://erdem.pl/static/7767bbec58461aa09c8032a5dc9bf06c/409e6/downsample.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图 13：Downsample block</p><p>Downsample block 首先接收前一层输出和步长、prompt 信息。它用 MaxPool2d 层（核为 2）将输入尺寸减半（64x64 -&gt; 32x32），然后通过 2 个 ResNet block。</p><p>嵌入信息经过 SILU 和线性层处理后，与 ResNet block 输出相加，再送入下一个模块。</p><h5 id="self-attention-block" tabindex="-1"><a class="header-anchor" href="#self-attention-block"><span>Self-Attention Block</span></a></h5><figure><img src="https://erdem.pl/static/2f26fadad0f8290c51b1b8579c008aeb/41d3c/attention.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图 14：Self-Attention block</p><p>部分 ResNet block 被 Attention block 替换。Attention block 结构一致，这里以第一个为例。输入为 (128, 32, 32)，经过多头注意力（128 维，4 头），嵌入维度会变化，头数不变。</p><p>输入需 reshape，最后两维合并并转置，(128,32,32) -&gt; (128,1024) -&gt; (1024,128)。LayerNorm 后作为 Q、K、V。</p><p>内部有 2 个 skip connection，分别加到 attention 层输出和后续前馈层输出上。最后再 reshape 回原形状。</p><h5 id="upsample-block" tabindex="-1"><a class="header-anchor" href="#upsample-block"><span>Upsample block</span></a></h5><figure><img src="https://erdem.pl/static/e56f851d5e6c5e8347979cede14afc27/42a8d/upsample.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图 15：Upsample block</p><p>Upsample block 有 3 个输入。主输入经过上采样（scale=2），与残差连接拼接，形状统一后通过 2 个 ResNet block。第三个输入（步长和 prompt 嵌入）同样处理后加到第二个 ResNet block 输出。</p><p>最后用 1x1 卷积将输出通道数变回 (3,64,64)，即为预测噪声。</p><h2 id="训练" tabindex="-1"><a class="header-anchor" href="#训练"><span>训练</span></a></h2><p>训练过程非常简单，伪代码如下：</p><p>1: repeat<br> 2: $x_0 \\sim q(x_0)$<br> 3: $t \\sim \\mathrm{Uniform}({1,\\ldots,T})$<br> 4: $\\epsilon \\sim \\mathcal{N}(0,I)$<br> 5: 梯度下降 $\\nabla_\\theta | \\epsilon - \\epsilon_\\theta(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t} \\epsilon, t) |^2$<br> 6: 直到收敛</p><p>每次从数据集中采样图片（2），采样步长 $t$（3），采样噪声（4）。如前文所述，无需逐步加噪，直接用：</p><p>$$ q(x_t|x_0) = \\sqrt{\\bar\\alpha_t}x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon $$</p><p>优化目标即为（5），重复直到收敛。</p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>本文较长，但希望你能读懂。这里只介绍了早期扩散模型，未涉及 CFG、负向提示、LORA、ControlNet 等后续进展，后续会单独介绍。你需要记住：</p><ul><li>扩散过程包括<strong>前向扩散</strong>和<strong>反向扩散</strong></li><li>前向扩散通过<strong>调度表</strong>加噪</li><li>调度表有多种（如线性、余弦），决定每步加多少噪声</li><li>加噪可一步完成，无需迭代</li><li>反向扩散多步去噪，每步去除一小部分噪声</li><li><strong>扩散模型预测的是全部噪声</strong>，不是相邻两步的差分</li><li>推理时可用不同调度表和步数</li><li>模型结构为<strong>改进版 U-Net</strong></li><li>步长和 prompt 信息通过<strong>正弦编码</strong>和<strong>嵌入器</strong>输入</li><li>部分 ResNet block 被 Self-Attention block 替换</li></ul><h2 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span>参考文献</span></a></h2><ol><li><a href="https://arxiv.org/abs/1503.03585" target="_blank" rel="noopener noreferrer">Deep Unsupervised Learning using Nonequilibrium Thermodynamics（论文）</a></li><li><a href="https://arxiv.org/abs/2006.11239" target="_blank" rel="noopener noreferrer">Denoising Diffusion Probabilistic Models（论文）</a></li><li><a href="https://arxiv.org/abs/2102.09672" target="_blank" rel="noopener noreferrer">Improved Denoising Diffusion Probabilistic Models（论文）</a></li><li><a href="https://arxiv.org/abs/2105.05233" target="_blank" rel="noopener noreferrer">Diffusion Models Beat GANs on Image Synthesis（论文）</a></li><li><a href="https://arxiv.org/abs/2207.12598" target="_blank" rel="noopener noreferrer">Classifier-Free Diffusion Guidance（论文）</a></li><li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#reverse-diffusion-process" target="_blank" rel="noopener noreferrer">What are diffusion models?（博客）</a></li><li><a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener noreferrer">U-Net: Convolutional Networks for Biomedical Image Segmentation（论文）</a></li><li><a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener noreferrer">Learning Transferable Visual Models From Natural Language Supervision（论文）</a></li></ol><hr><h3 id="引用" tabindex="-1"><a class="header-anchor" href="#引用"><span>引用</span></a></h3><blockquote><p>Kemal Erdem, (Nov 2023). &quot;Step by Step visual introduction to Diffusion Models.&quot;. <a href="https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models" target="_blank" rel="noopener noreferrer">https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models</a></p></blockquote><p><strong>或</strong></p><div class="language-bibtex line-numbers-mode" data-highlighter="shiki" data-ext="bibtex" data-title="bibtex" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">@article</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">erdem2023stepByStepVisualIntroductionToDiffusionModels</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>\n<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">        title</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">   = </span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Step by Step visual introduction to Diffusion Models.</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>\n<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">        author</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  = </span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Kemal Erdem</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>\n<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">        journal</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> = </span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">https://erdem.pl</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>\n<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">        year</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    = </span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">2023</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>\n<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">        month</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">   = </span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Nov</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>\n<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">        url</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">     = </span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr><p>ML Developer, Software Architect, JS Engineer, Ultra-distance cyclist</p><p><strong>Kemal Erdem</strong> on Twitter: <a href="https://www.twitter.com/burnpiro" target="_blank" rel="noopener noreferrer">https://www.twitter.com/burnpiro</a></p>',137)]))}]]),p=JSON.parse('{"path":"/zh/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.html","title":"Diffusion Models 可视化逐步入门","lang":"zh-CN","frontmatter":{"title":"Diffusion Models 可视化逐步入门","author":"Kemal Erdem","translator":"你的名字","original_date":"2023-11-01T00:00:00.000Z","translated_date":"2025-06-28T00:00:00.000Z","tags":["机器学习","扩散模型"],"description":"Diffusion Models 可视化逐步入门 本文为原文《Step by Step visual introduction to Diffusion Models》的中文翻译，原作者 Kemal Erdem。 什么是扩散模型？ 扩散模型的概念并不久远。2015 年的论文《Deep Unsupervised Learning using Nonequ...","gitInclude":[],"head":[["link",{"rel":"alternate","hreflang":"en-us","href":"https://neverbiasu.github.io/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.html"}],["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"Diffusion Models 可视化逐步入门"}],["meta",{"property":"og:description","content":"Diffusion Models 可视化逐步入门 本文为原文《Step by Step visual introduction to Diffusion Models》的中文翻译，原作者 Kemal Erdem。 什么是扩散模型？ 扩散模型的概念并不久远。2015 年的论文《Deep Unsupervised Learning using Nonequ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://erdem.pl/static/c5e1011bc11d7f13fdf19295c0e94c3e/eea4a/linear_noise.jpg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:locale:alternate","content":"en-US"}],["meta",{"property":"article:author","content":"Kemal Erdem"}],["meta",{"property":"article:tag","content":"机器学习"}],["meta",{"property":"article:tag","content":"扩散模型"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Diffusion Models 可视化逐步入门\\",\\"image\\":[\\"https://erdem.pl/static/c5e1011bc11d7f13fdf19295c0e94c3e/eea4a/linear_noise.jpg\\",\\"https://erdem.pl/static/6bf29ce5feb6b15753aff769147e7be8/21b4d/lin_schedule_beta.png\\",\\"https://erdem.pl/static/eb1c5091b8115b21499e408a8edf7b82/eea4a/cos_noise.jpg\\",\\"https://erdem.pl/869cd128ff69bb1d304675166e8b7b34/gan_diagram_generator.svg\\",\\"https://erdem.pl/static/897bdd95a9ca14cff82e0257de0dccc3/ee515/graphical_diffusion_model.png\\",\\"https://erdem.pl/static/512d5abbaa2b507956d1b737f85cee1b/34e70/one_diffusion_step.png\\",\\"https://erdem.pl/static/90207efb3cb80cde5f359b2205b0303a/0f840/sample_full_noise_removal.jpg\\",\\"https://erdem.pl/static/18fc014ed4e2d9fe4d1439b3431ccad9/d38a6/conv_block.png\\",\\"https://erdem.pl/static/7767bbec58461aa09c8032a5dc9bf06c/409e6/downsample.png\\",\\"https://erdem.pl/static/2f26fadad0f8290c51b1b8579c008aeb/41d3c/attention.png\\",\\"https://erdem.pl/static/e56f851d5e6c5e8347979cede14afc27/42a8d/upsample.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Kemal Erdem\\"}]}"]]},"headers":[{"level":2,"title":"什么是扩散模型？","slug":"什么是扩散模型","link":"#什么是扩散模型","children":[]},{"level":2,"title":"前向扩散过程","slug":"前向扩散过程","link":"#前向扩散过程","children":[]},{"level":2,"title":"反向扩散过程","slug":"反向扩散过程","link":"#反向扩散过程","children":[{"level":3,"title":"关于反向扩散的误解","slug":"关于反向扩散的误解","link":"#关于反向扩散的误解","children":[]},{"level":3,"title":"一些数学推导（可跳过，但值得一读）","slug":"一些数学推导-可跳过-但值得一读","link":"#一些数学推导-可跳过-但值得一读","children":[]},{"level":3,"title":"反向扩散输出可视化","slug":"反向扩散输出可视化","link":"#反向扩散输出可视化","children":[]}]},{"level":2,"title":"架构","slug":"架构","link":"#架构","children":[]},{"level":2,"title":"训练","slug":"训练","link":"#训练","children":[]},{"level":2,"title":"总结","slug":"总结","link":"#总结","children":[]},{"level":2,"title":"参考文献","slug":"参考文献","link":"#参考文献","children":[{"level":3,"title":"引用","slug":"引用","link":"#引用","children":[]}]}],"readingTime":{"minutes":10.98,"words":3294},"filePathRelative":"zh/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.md","excerpt":"\\n<blockquote>\\n<p>本文为原文《Step by Step visual introduction to Diffusion Models》的中文翻译，原作者 Kemal Erdem。</p>\\n</blockquote>\\n<h2>什么是扩散模型？</h2>\\n<p>扩散模型的概念并不久远。2015 年的论文《Deep Unsupervised Learning using Nonequilibrium Thermodynamics》中，作者这样描述：</p>\\n<blockquote>\\n<p>其核心思想，受非平衡统计物理学启发，是通过<strong>逐步的前向扩散过程</strong>系统性地、缓慢地破坏数据分布中的结构。随后我们学习一个<strong>反向扩散过程</strong>，恢复数据结构，从而获得对数据高度灵活且易于处理的生成模型。</p>\\n</blockquote>","autoDesc":true}')}}]);