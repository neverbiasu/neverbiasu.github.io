"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[9098],{6262:(e,a)=>{a.A=(e,a)=>{const r=e.__vccOpts||e;for(const[e,n]of a)r[e]=n;return r}},2525:(e,a,r)=>{r.r(a),r.d(a,{comp:()=>i,data:()=>o});var n=r(641);const t={},i=(0,r(6262).A)(t,[["render",function(e,a){return(0,n.uX)(),(0,n.CE)("div",null,a[0]||(a[0]=[(0,n.Fv)('<h1 id="mils零训练多模态理解-layertracer优化svg生成-omnihuman高保真数字人【ai周报】" tabindex="-1"><a class="header-anchor" href="#mils零训练多模态理解-layertracer优化svg生成-omnihuman高保真数字人【ai周报】"><span>MILS零训练多模态理解|LayerTracer优化SVG生成|OmniHuman高保真数字人【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/e958ef00-1183-47ee-91f5-c1f1023f08a3/anim=false,width=450/55557070.jpeg" alt="封面源自C站作者Clear_Note" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Clear_Note</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：MILS展示了无需训练的多模态理解方法；LayerTracer通过DiT改进SVG生成；MakeAnything利用DiT优化程序化序列生成；OmniHuman生成高保真数字人视频；MatAnyone提供稳定的视频抠像方案；s1模型在推理任务中展现强劲实力。详情见正文。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#mils%E6%97%A0%E9%9C%80%E8%AE%AD%E7%BB%83%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E7%90%86%E8%A7%A3%E7%B3%BB%E7%BB%9F">MILS：无需训练的多模态理解系统</a></li><li><a href="#layertracer%E9%80%9A%E8%BF%87%E6%89%A9%E6%95%A3transformer%E5%AE%9E%E7%8E%B0%E8%AE%A4%E7%9F%A5%E5%AF%B9%E9%BD%90%E7%9A%84%E5%88%86%E5%B1%82svg%E5%90%88%E6%88%90">LayerTracer：通过扩散Transformer实现认知对齐的分层SVG合成</a></li><li><a href="#makeanything%E5%88%A9%E7%94%A8%E6%89%A9%E6%95%A3transformer%E8%BF%9B%E8%A1%8C%E5%A4%9A%E9%A2%86%E5%9F%9F%E7%A8%8B%E5%BA%8F%E5%8C%96%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90">MakeAnything：利用扩散Transformer进行多领域程序化序列生成</a></li><li><a href="#omnihuman-1%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E6%8E%A8%E5%87%BA%E7%9A%84ai%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0%E4%BB%8E%E5%8D%95%E5%BC%A0%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E9%AB%98%E5%BA%A6%E9%80%BC%E7%9C%9F%E7%9A%84%E4%BA%BA%E7%B1%BB%E8%A7%86%E9%A2%91">OmniHuman-1：字节跳动推出的AI模型，实现从单张图像生成高度逼真的人类视频</a></li><li><a href="#matanyone%E7%A8%B3%E5%AE%9A%E7%9A%84%E8%A7%86%E9%A2%91%E6%8A%A0%E5%83%8F%E6%A1%86%E6%9E%B6">MatAnyone：稳定的视频抠像框架</a></li><li><a href="#s1-%E6%A8%A1%E5%9E%8B%E6%9D%8E%E9%A3%9E%E9%A3%9E%E5%9B%A2%E9%98%9F%E6%8F%90%E5%87%BA%E7%9A%84%E9%AB%98%E6%95%88%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B">s1 模型：李飞飞团队提出的高效推理模型</a></li></ol><hr><h2 id="mils-无需训练的多模态理解系统" tabindex="-1"><a class="header-anchor" href="#mils-无需训练的多模态理解系统"><span>MILS：无需训练的多模态理解系统</span></a></h2><figure><img src="https://github.com/facebookresearch/MILS/raw/main/teaser.png" alt="MILS Teaser 图" tabindex="0" loading="lazy"><figcaption>MILS Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>MILS</strong>（Multimodal Iterative LLM Solver）是由<strong>Meta AI</strong>提出的一种新型多模态理解系统。与传统的多模态模型不同，MILS <strong>不需要额外训练</strong>，而是利用现有大型语言模型（LLMs）的推理能力，让其通过文本描述的方式来处理图像和音频数据。核心思想是<strong>将非文本模态转换为语言提示</strong>，让 LLMs 能够直接解析图像和音频内容，而无需调整模型参数。这种方法表明，LLMs 具备强大的多模态推理能力，即便未经过专门的视觉或音频训练，也能在多模态任务上表现出色。MILS 在多个多模态任务中取得了接近或超越现有专门训练模型的效果，表明这一方向的可行性。</p><p><strong>标签</strong>：#MILS #Meta #多模态理解 #零样本学习 #LLM</p><hr><h2 id="layertracer-通过扩散transformer实现认知对齐的分层svg合成" tabindex="-1"><a class="header-anchor" href="#layertracer-通过扩散transformer实现认知对齐的分层svg合成"><span>LayerTracer：通过扩散Transformer实现认知对齐的分层SVG合成</span></a></h2><p><img src="https://github.com/showlab/LayerTracer/raw/main/img/teaser.png" alt="LayerTracer Teaser 图" loading="lazy"><strong>概要</strong>：<strong>LayerTracer</strong> 是由新加坡国立大学的 <strong>Show Lab</strong> 提出的一个框架，旨在生成与人类认知相一致的分层SVG。该方法采用Diffusion Transformer架构，模拟设计师创建分层SVG的过程。具体而言，LayerTracer首先通过文本条件的DiT生成多阶段的栅格化蓝图，模拟人类设计工作流程。随后，通过逐层矢量化和路径去重，生成干净、可编辑的SVG。实验结果表明，LayerTracer在生成质量和可编辑性方面优于基于优化和神经网络的基线方法，有效地将AI生成的矢量与专业设计认知对齐。</p><p><strong>标签</strong>：#LayerTracer #SVG合成 #DiT #ShowLab</p><hr><h2 id="makeanything-利用扩散transformer进行多领域程序化序列生成" tabindex="-1"><a class="header-anchor" href="#makeanything-利用扩散transformer进行多领域程序化序列生成"><span>MakeAnything：利用扩散Transformer进行多领域程序化序列生成</span></a></h2><figure><img src="https://raw.githubusercontent.com/showlab/MakeAnything/main/images/teaser.png" alt="MakeAnything Teaser图" tabindex="0" loading="lazy"><figcaption>MakeAnything Teaser图</figcaption></figure><p><strong>概要</strong>：<strong>MakeAnything</strong> 是由 <strong>Show Lab</strong> 团队开发的一个框架，旨在生成多步骤的程序化教程，涵盖绘画、手工艺、烹饪等多种任务。该框架基于Diffusion Transformer（DiT），通过微调激活DiT的上下文能力，以生成逻辑连贯且视觉一致的步骤序列。此外，MakeAnything引入了用于图像生成的非对称低秩适应（LoRA）方法，在冻结编码器参数的同时，自适应地调整解码器层，以平衡泛化能力和任务特定性能。其ReCraft模型通过时空一致性约束，实现了从静态图像到过程的生成，使静态图像能够被分解为合理的创建序列。实验结果表明，MakeAnything在程序化生成任务上超越了现有方法，设定了新的性能基准。</p><p><strong>标签</strong>：#程序化生成 #DiffusionTransformer #多领域 #LoRA</p><hr><h2 id="omnihuman-1-字节跳动推出的ai模型-实现从单张图像生成高度逼真的人类视频" tabindex="-1"><a class="header-anchor" href="#omnihuman-1-字节跳动推出的ai模型-实现从单张图像生成高度逼真的人类视频"><span>OmniHuman-1：字节跳动推出的AI模型，实现从单张图像生成高度逼真的人类视频</span></a></h2><p><img src="https://arxiv.org/html/2502.01061v1/extracted/6173608/imgs/framework.jpg" alt="OmniHuman-1 Framework 图" loading="lazy"><strong>概要</strong>：<strong>字节跳动（ByteDance）</strong> 近期推出了一款名为 <strong>OmniHuman-1</strong> 的模型，该模型能够从单张图像和音频生成高度逼真的人类视频。OmniHuman-1采用了多模态运动条件混合训练策略，使其能够利用音频、视频或两者结合的运动信号，生成逼真的人类视频。 该模型支持任意纵横比的图像输入，包括肖像、半身和全身图像，生成的结果在动作、光照和纹理细节等方面都极为逼真。</p><p><strong>标签</strong>：#AI #OmniHuman #字节跳动 #深度伪造 #多模态训练</p><hr><h2 id="matanyone-稳定的视频抠像框架" tabindex="-1"><a class="header-anchor" href="#matanyone-稳定的视频抠像框架"><span>MatAnyone：稳定的视频抠像框架</span></a></h2><figure><img src="https://pq-yang.github.io/projects/MatAnyone/assets/images_mat/pipeline.png" alt="MatAnyone Pipeline 图" tabindex="0" loading="lazy"><figcaption>MatAnyone Pipeline 图</figcaption></figure><p><strong>概要</strong>：<strong>MatAnyone</strong> 是由 <strong>南洋理工大学 S-Lab</strong> 与 <strong>商汤科技新加坡研究团队</strong> 合作开发的一个实用性强的人物视频抠像框架。该框架支持目标指定，在核心区域的语义和精细边界细节方面表现稳定。通过引入一致的内存传播模块，MatAnyone能够自适应地融合前一帧的内存信息，确保核心区域的语义稳定性，同时保留对象边界的精细细节。此外，研究团队还提出了一种新的训练策略，有效利用大规模分割数据，提升抠像的稳定性。实验结果表明，MatAnyone在多种真实世界场景中实现了稳健且准确的视频抠像效果，优于现有方法。</p><p><strong>标签</strong>：#视频抠像 #内存传播 #目标指定 #S-Lab</p><hr><h2 id="s1-模型-李飞飞团队提出的高效推理模型" tabindex="-1"><a class="header-anchor" href="#s1-模型-李飞飞团队提出的高效推理模型"><span>s1 模型：李飞飞团队提出的高效推理模型</span></a></h2><figure><img src="https://github.com/simplescaling/s1/raw/main/visuals/scaling.png" alt="s1 Scaling 图" tabindex="0" loading="lazy"><figcaption>s1 Scaling 图</figcaption></figure><p><strong>概要</strong>：<strong>李飞飞团队</strong> 近期发布了名为 <strong>s1</strong> 的人工智能推理模型。该模型通过在阿里巴巴开源的 Qwen2.5-32B-Instruct 模型上进行监督微调，仅使用 1000 个精心设计的问题数据集，训练耗时约 26 分钟，成本不到 50 美元。实验结果显示，s1 在数学和编码能力测试中的表现与 OpenAI 的 o1 和 DeepSeek 的 R1 等尖端推理模型相当，甚至在竞赛数学问题上的表现比 o1-preview 高出 27%。</p><p><strong>标签</strong>：#推理模型 #李飞飞 #s1 #Qwen2.5-32B-Instruct</p><hr><h3 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span><strong>参考文献</strong></span></a></h3><ol><li><a href="https://github.com/facebookresearch/MILS" target="_blank" rel="noopener noreferrer">MILS GitHub</a></li><li><a href="https://arxiv.org/pdf/2501.18096" target="_blank" rel="noopener noreferrer">MILS 论文</a></li><li><a href="https://github.com/showlab/LayerTracer" target="_blank" rel="noopener noreferrer">LayerTracer GitHub</a></li><li><a href="https://arxiv.org/html/2502.01105" target="_blank" rel="noopener noreferrer">LayerTracer 论文</a></li><li><a href="https://github.com/showlab/MakeAnything" target="_blank" rel="noopener noreferrer">MakeAnything GitHub</a></li><li><a href="https://arxiv.org/html/2502.01572v2" target="_blank" rel="noopener noreferrer">MakeAnything 论文</a></li><li><a href="https://omnihuman-lab.github.io/" target="_blank" rel="noopener noreferrer">OmniHuman 官网</a></li><li><a href="https://arxiv.org/pdf/2502.01061" target="_blank" rel="noopener noreferrer">OmniHuman 论文</a></li><li><a href="https://pq-yang.github.io/projects/MatAnyone/" target="_blank" rel="noopener noreferrer">MatAnyone 官网</a></li><li><a href="https://arxiv.org/html/2501.14677v1" target="_blank" rel="noopener noreferrer">MatAnyone 论文</a></li><li><a href="https://github.com/simplescaling/s1" target="_blank" rel="noopener noreferrer">s1 GitHub</a></li><li><a href="https://arxiv.org/html/2501.19393v2" target="_blank" rel="noopener noreferrer">s1 论文</a></li></ol>',38)]))}]]),o=JSON.parse('{"path":"/zh/posts/ai-weekly/024.html","title":"MILS零训练多模态理解|LayerTracer优化SVG生成|OmniHuman高保真数字人【AI周报】","lang":"zh-CN","frontmatter":{"description":"MILS零训练多模态理解|LayerTracer优化SVG生成|OmniHuman高保真数字人【AI周报】 封面源自C站作者Clear_Note封面源自C站作者Clear_Note 摘要 本周亮点：MILS展示了无需训练的多模态理解方法；LayerTracer通过DiT改进SVG生成；MakeAnything利用DiT优化程序化序列生成；OmniHum...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/024.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"MILS零训练多模态理解|LayerTracer优化SVG生成|OmniHuman高保真数字人【AI周报】"}],["meta",{"property":"og:description","content":"MILS零训练多模态理解|LayerTracer优化SVG生成|OmniHuman高保真数字人【AI周报】 封面源自C站作者Clear_Note封面源自C站作者Clear_Note 摘要 本周亮点：MILS展示了无需训练的多模态理解方法；LayerTracer通过DiT改进SVG生成；MakeAnything利用DiT优化程序化序列生成；OmniHum..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/e958ef00-1183-47ee-91f5-c1f1023f08a3/anim=false,width=450/55557070.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"MILS零训练多模态理解|LayerTracer优化SVG生成|OmniHuman高保真数字人【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/e958ef00-1183-47ee-91f5-c1f1023f08a3/anim=false,width=450/55557070.jpeg\\",\\"https://github.com/facebookresearch/MILS/raw/main/teaser.png\\",\\"https://github.com/showlab/LayerTracer/raw/main/img/teaser.png\\",\\"https://raw.githubusercontent.com/showlab/MakeAnything/main/images/teaser.png\\",\\"https://arxiv.org/html/2502.01061v1/extracted/6173608/imgs/framework.jpg\\",\\"https://pq-yang.github.io/projects/MatAnyone/assets/images_mat/pipeline.png\\",\\"https://github.com/simplescaling/s1/raw/main/visuals/scaling.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"MILS：无需训练的多模态理解系统","slug":"mils-无需训练的多模态理解系统","link":"#mils-无需训练的多模态理解系统","children":[]},{"level":2,"title":"LayerTracer：通过扩散Transformer实现认知对齐的分层SVG合成","slug":"layertracer-通过扩散transformer实现认知对齐的分层svg合成","link":"#layertracer-通过扩散transformer实现认知对齐的分层svg合成","children":[]},{"level":2,"title":"MakeAnything：利用扩散Transformer进行多领域程序化序列生成","slug":"makeanything-利用扩散transformer进行多领域程序化序列生成","link":"#makeanything-利用扩散transformer进行多领域程序化序列生成","children":[]},{"level":2,"title":"OmniHuman-1：字节跳动推出的AI模型，实现从单张图像生成高度逼真的人类视频","slug":"omnihuman-1-字节跳动推出的ai模型-实现从单张图像生成高度逼真的人类视频","link":"#omnihuman-1-字节跳动推出的ai模型-实现从单张图像生成高度逼真的人类视频","children":[]},{"level":2,"title":"MatAnyone：稳定的视频抠像框架","slug":"matanyone-稳定的视频抠像框架","link":"#matanyone-稳定的视频抠像框架","children":[]},{"level":2,"title":"s1 模型：李飞飞团队提出的高效推理模型","slug":"s1-模型-李飞飞团队提出的高效推理模型","link":"#s1-模型-李飞飞团队提出的高效推理模型","children":[{"level":3,"title":"参考文献","slug":"参考文献","link":"#参考文献","children":[]}]}],"readingTime":{"minutes":5.75,"words":1725},"filePathRelative":"zh/posts/ai-weekly/024.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/e958ef00-1183-47ee-91f5-c1f1023f08a3/anim=false,width=450/55557070.jpeg\\" alt=\\"封面源自C站作者Clear_Note\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面源自C站作者Clear_Note</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：MILS展示了无需训练的多模态理解方法；LayerTracer通过DiT改进SVG生成；MakeAnything利用DiT优化程序化序列生成；OmniHuman生成高保真数字人视频；MatAnyone提供稳定的视频抠像方案；s1模型在推理任务中展现强劲实力。详情见正文。</p>","autoDesc":true}')}}]);