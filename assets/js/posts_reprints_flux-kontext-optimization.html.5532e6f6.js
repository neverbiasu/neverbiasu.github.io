"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[4179],{66262:(i,s)=>{s.A=(i,s)=>{const e=i.__vccOpts||i;for(const[i,t]of s)e[i]=t;return e}},96416:(i,s,e)=>{e.r(s),e.d(s,{comp:()=>n,data:()=>p});var t=e(20641);const a={},n=(0,e(66262).A)(a,[["render",function(i,s){return(0,t.uX)(),(0,t.CE)("div",null,s[0]||(s[0]=[(0,t.Fv)('<h1 id="how-we-optimized-flux-1-kontext-dev" tabindex="-1"><a class="header-anchor" href="#how-we-optimized-flux-1-kontext-dev"><span>How we optimized FLUX.1 Kontext [dev]</span></a></h1><figure><img src="https://replicate.com/_content/assets/top-graphic.CLh5lXp7_Z2h1V1F.webp" alt="FLUX.1 Kontext optimization graphic" tabindex="0" loading="lazy"><figcaption>FLUX.1 Kontext optimization graphic</figcaption></figure><p>In addition to making our FLUX.1 Kontext [dev] implementation open-source, we wanted to provide more guidance on how we chose to optimize it without compromising on quality.</p><p>In this post, you will mainly learn about TaylorSeer optimization, a method to approximate intermediate image predictions by using cached image changes (derivatives) and formulae derived from Taylor Series approximations.</p><p>Fellow optimization nerds, read on.</p><hr><p>(We pulled most of our implementation info from the following paper.)</p><p>If you head to the predict function in <code>predict.py</code> from our FLUX.1 Kontext [dev] repo, you will find the main logic. (Highly suggest working through the repo and using this post as a guide for understanding its structure.)</p><p>Let’s break it down.</p><h2 id="on-taylorseer" tabindex="-1"><a class="header-anchor" href="#on-taylorseer"><span>On TaylorSeer</span></a></h2><p>When generating a new image with FLUX.1 Kontext, you apply a diffusion transformation across multiple timesteps — around 30 steps in a row. At each step, a stack of transformer layers predicts an update to the image you are denoising. This process can take a while.</p><p>At any given timestep, the change predicted by the model has redundancies with the predictions at previous timesteps. We could take advantage of these redundancies by caching the model’s output at certain timesteps, and reusing cached outputs at future timesteps. This “naïve caching” — where you just reuse the last feature or latent value — sometimes works OK, but can lead to blurring, loss of detail, or sometimes total distortion of the image.</p><p>You could try something slightly smarter: a linear approximation. You can estimate your next step by looking at the difference between the last two steps (i.e. a first-order finite difference) and extending the line. It’s better, but still not great. It doesn’t capture curves, acceleration, or nonlinear changes — all of which are common in diffusion models.</p><p>TaylorSeer offers a solution for this. It uses <strong>Taylor series</strong> to approximate the model’s output at a timestep using <strong>a series of cached derivatives</strong>, capturing non-linear change.</p><p>Here’s the core idea in math terms. To predict the feature at a timestep <em>t+k</em> in a certain layer <em>l</em>, we use a truncated Taylor expansion:</p><figure><img src="https://replicate.com/_content/assets/equation_1.D448o7_6_2extT4.webp" alt="FLUX.1 Kontext optimization graphic" tabindex="0" loading="lazy"><figcaption>FLUX.1 Kontext optimization graphic</figcaption></figure><p>Notice the summation requires i-order derivatives of the feature function. Since we can’t compute the actual derivatives, we can use the finite difference between each i-1’th and i’th derivative. Check out the paper for the exact math, but when you perform that substitution and do a bit of simplifying, you get the following:</p><figure><img src="https://replicate.com/_content/assets/equation_2.Bd6ZJgzi_1c5lfT.webp" alt="FLUX.1 Kontext optimization equation 2" tabindex="0" loading="lazy"><figcaption>FLUX.1 Kontext optimization equation 2</figcaption></figure><p>This is our final approximation of the feature at time step <em>t+k</em>.</p><p>We now have a method to speed up our diffusion process by using the above estimation for the feature at particular time steps.</p><p>We set up a TaylorSeer cache to perform this approximation when comes time:</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">order </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> n_derivatives </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">+</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 1</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">taylor_seer_state </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> {</span></span>\n<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;dY_prev&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">*</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> order,</span></span>\n<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;dY_current&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">None</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">*</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> order,</span></span>\n<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;last_non_approximated_step&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>\n<span class="line"><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">        &quot;current_step&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">: </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">0</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Here, <code>order = n_derivatives + 1</code>. If <code>n_derivatives = 2</code>, for instance, then <code>order = 3</code>, and we cache:</p><ul><li><code>dY_current[0]</code>: The current feature</li><li><code>dY_current[1]</code>: The first-order derivative</li><li><code>dY_current[2]</code>: The second-order derivative</li></ul><p>The first few steps of <code>denoise()</code> always computes full predictions, which are used to initialize the finite differences. Later steps can be approximated.</p><hr><h2 id="step-by-step-how-taylorseer-works-in-flux-kontext" tabindex="-1"><a class="header-anchor" href="#step-by-step-how-taylorseer-works-in-flux-kontext"><span>Step-by-step: How TaylorSeer works in Flux Kontext</span></a></h2><p>Once we’ve prepared the inputs, we decide which steps to compute and which to approximate using <code>generate_compute_step_map()</code>:</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> generate_compute_step_map</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">acceleration_level</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> str</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> num_inference_steps</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> int</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        </span></span>\n<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">        if</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> acceleration_level </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">==</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;none&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>\n<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">                return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">*</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> num_inference_steps</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        </span></span>\n<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">        elif</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> acceleration_level </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">==</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;go fast&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>\n<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">                # compute first and last 4 steps and all steps in between alternating</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">                k </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">                compute_step_map </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [k[i </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">%</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 2</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> i </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> range</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(num_inference_steps)]</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">                compute_step_map[:</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">4</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">*</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 4</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">                compute_step_map[</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">4</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">*</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 4</span></span>\n<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">                return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> compute_step_map</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        </span></span>\n<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">        elif</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> acceleration_level </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">==</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> &quot;go really fast&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>\n<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">                # compute first + last 3 steps and all steps in between alternate between computing full once and approximating twice</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">                k </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">]</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">                compute_step_map </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [k[i </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">%</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 3</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> i </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> range</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(num_inference_steps)]</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">                compute_step_map[:</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">*</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 3</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">                compute_step_map[</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">-</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">3</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> [</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">*</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 3</span></span>\n<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">                return</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> compute_step_map</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">        ...</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>generate_compute_step_map()</code> follows a simple rule: always compute the first and last few steps, since that’s where the model makes the biggest changes. In the middle steps, we compute every other step for “go fast” mode, or every third step for “go really fast.” An adaptive approach like First Block Cache, which checks how much the first transformer block output changes, could be smart to ascertain which steps to skip, but this hard-coded strategy works well.</p><h2 id="two-paths-compute-or-approximate" tabindex="-1"><a class="header-anchor" href="#two-paths-compute-or-approximate"><span>Two paths: compute or approximate</span></a></h2><p>In the denoising loop:</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">if</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> compute_step_map[current_step]:</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        pred </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> model</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">...</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># full prediction</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        taylor_seer_state[</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&#39;dY_current&#39;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">] </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> approximate_derivative</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">...</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>\n<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">else</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        pred </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> approximate_value</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">...</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># predicted using TaylorSeer</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Let’s break down each path.</p><hr><h3 id="path-1-full-computation" tabindex="-1"><a class="header-anchor" href="#path-1-full-computation"><span>Path 1: Full computation</span></a></h3><p>We run the model normally and update our stored finite differences (derivatives):</p><p><code>dY_current[i+1] = (dY_current[i] - dY_prev[i]) / finite_difference_window</code></p><p>This is a recursive view of computing higher-order finite differences from the lower-order ones. The <em>m+1’th</em> order derivative comes from the difference of the <em>m’th</em> order derivative values over the time that passed.</p><p>These differences approximate how the feature values evolve over time.</p><hr><h3 id="path-2-approximate-using-taylor-series" tabindex="-1"><a class="header-anchor" href="#path-2-approximate-using-taylor-series"><span>Path 2: Approximate using Taylor series</span></a></h3><p>If we decide to skip a step (based on <code>compute_step_map</code>), we use the cached differences to estimate the next feature update (our approximation from equation 2):</p><p><code>output += (1 / math.factorial(i)) * dY_current[i] * (elapsed_steps ** i)</code></p><p>The time it takes to compute this approximation is nearly instantaneous compared to the time it takes to run the full model for a single denoising step.</p><hr><h3 id="update-the-latent" tabindex="-1"><a class="header-anchor" href="#update-the-latent"><span>Update the latent</span></a></h3><p>At every step, whether we computed or approximated, we apply the predicted delta to the image latent:</p><p><code>img = img + (t_prev - t_curr) * pred</code></p><p>This keeps the image transformation evolving across timesteps.</p><hr><p>After the denoise loop, we return our final image!</p><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary"><span>Summary</span></a></h2><p>Instead of evaluating the model at every single timestep, we:</p><ol><li><strong>Cache</strong> past predictions and their finite differences.</li><li><strong>Use Taylor series</strong> to approximate the model’s output at skipped steps.</li><li><strong>Reduce model calls</strong> from 30 to maybe 10–15, depending on speed settings.</li><li><strong>Maintain quality</strong>, especially at the start and end of generation, where accuracy matters most.</li></ol><p>TaylorSeer gives us a principled, flexible way to forecast intermediate steps in image generation using feature dynamics. It’s faster than running every step and smarter than linear extrapolation.</p><p>You can find this all in <code>denoise()</code> and <code>taylor_utils.py</code> in our FLUX.1 Kontext repo.</p><p>Hammer away at the repo, and let us know what you discover!</p><p>Give it a spin: Try FLUX.1 Kontext [dev]</p><h2 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h2><ul><li>[FLUX.1 Kontext [dev]]: https://replicate.com/black-forest-labs/flux-kontext-dev</li><li></li><li>[FLUX.1 Kontext [dev] repo]: https://github.com/replicate/cog-flux-kontext</li><li></li><li></li><li>[Try FLUX.1 Kontext [dev]]: https://replicate.com/black-forest-labs/flux-kontext-dev</li></ul>',61)]))}]]),p=JSON.parse('{"path":"/posts/reprints/flux-kontext-optimization.html","title":"How we optimized FLUX.1 Kontext [dev]","lang":"en-US","frontmatter":{"title":"How we optimized FLUX.1 Kontext [dev]","date":"2025-07-15T00:00:00.000Z","authors":[{"name":"shridharathi","link":"https://replicate.com/shridharathi"},{"name":"alexarmbr","link":"https://replicate.com/alexarmbr"}],"tags":["FLUX.1","Optimization","TaylorSeer","Replicate"],"description":"How we optimized FLUX.1 Kontext [dev] FLUX.1 Kontext optimization graphicFLUX.1 Kontext optimization graphic In addition to making our FLUX.1 Kontext [dev] implementation open-s...","gitInclude":[],"head":[["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://neverbiasu.github.io/zh/posts/reprints/flux-kontext-optimization.html"}],["meta",{"property":"og:url","content":"https://neverbiasu.github.io/posts/reprints/flux-kontext-optimization.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"How we optimized FLUX.1 Kontext [dev]"}],["meta",{"property":"og:description","content":"How we optimized FLUX.1 Kontext [dev] FLUX.1 Kontext optimization graphicFLUX.1 Kontext optimization graphic In addition to making our FLUX.1 Kontext [dev] implementation open-s..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://replicate.com/_content/assets/top-graphic.CLh5lXp7_Z2h1V1F.webp"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"property":"article:tag","content":"FLUX.1"}],["meta",{"property":"article:tag","content":"Optimization"}],["meta",{"property":"article:tag","content":"TaylorSeer"}],["meta",{"property":"article:tag","content":"Replicate"}],["meta",{"property":"article:published_time","content":"2025-07-15T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"How we optimized FLUX.1 Kontext [dev]\\",\\"image\\":[\\"https://replicate.com/_content/assets/top-graphic.CLh5lXp7_Z2h1V1F.webp\\",\\"https://replicate.com/_content/assets/equation_1.D448o7_6_2extT4.webp\\",\\"https://replicate.com/_content/assets/equation_2.Bd6ZJgzi_1c5lfT.webp\\"],\\"datePublished\\":\\"2025-07-15T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"On TaylorSeer","slug":"on-taylorseer","link":"#on-taylorseer","children":[]},{"level":2,"title":"Step-by-step: How TaylorSeer works in Flux Kontext","slug":"step-by-step-how-taylorseer-works-in-flux-kontext","link":"#step-by-step-how-taylorseer-works-in-flux-kontext","children":[]},{"level":2,"title":"Two paths: compute or approximate","slug":"two-paths-compute-or-approximate","link":"#two-paths-compute-or-approximate","children":[{"level":3,"title":"Path 1: Full computation","slug":"path-1-full-computation","link":"#path-1-full-computation","children":[]},{"level":3,"title":"Path 2: Approximate using Taylor series","slug":"path-2-approximate-using-taylor-series","link":"#path-2-approximate-using-taylor-series","children":[]},{"level":3,"title":"Update the latent","slug":"update-the-latent","link":"#update-the-latent","children":[]}]},{"level":2,"title":"Summary","slug":"summary","link":"#summary","children":[]},{"level":2,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}],"readingTime":{"minutes":3.9,"words":1171},"filePathRelative":"posts/reprints/flux-kontext-optimization.md","localizedDate":"July 15, 2025","excerpt":"\\n<figure><img src=\\"https://replicate.com/_content/assets/top-graphic.CLh5lXp7_Z2h1V1F.webp\\" alt=\\"FLUX.1 Kontext optimization graphic\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>FLUX.1 Kontext optimization graphic</figcaption></figure>\\n<p>In addition to making our FLUX.1 Kontext [dev] implementation open-source, we wanted to provide more guidance on how we chose to optimize it without compromising on quality.</p>","autoDesc":true}')}}]);