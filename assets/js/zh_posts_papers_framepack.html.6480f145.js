"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[6645],{6262:(e,a)=>{a.A=(e,a)=>{const r=e.__vccOpts||e;for(const[e,i]of a)r[e]=i;return r}},8510:(e,a,r)=>{r.r(a),r.d(a,{comp:()=>t,data:()=>o});var i=r(641);const n={},t=(0,r(6262).A)(n,[["render",function(e,a){return(0,i.uX)(),(0,i.CE)("div",null,a[0]||(a[0]=[(0,i.Fv)('<h1 id="【论文精读】framepack-packing-input-frame-context-in-next-frame-prediction-models-for-video-generation" tabindex="-1"><a class="header-anchor" href="#【论文精读】framepack-packing-input-frame-context-in-next-frame-prediction-models-for-video-generation"><span>【论文精读】FramePack：Packing Input Frame Context in Next-Frame Prediction Models for Video Generation</span></a></h1><figure><img src="https://paper-assets.alphaxiv.org/figures/2504.12626/x1.png" alt="FramePack Compression Approaches" tabindex="0" loading="lazy"><figcaption>FramePack Compression Approaches</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>FramePack 由斯坦福大学 Lvmin Zhang 和 Maneesh Agrawala 提出，创新性地解决了视频生成中“遗忘-漂移”两难问题。通过自适应帧压缩与反漂移采样，FramePack 兼顾长时序一致性与视觉质量，支持在有限算力下生成更长、更稳定的视频。该方法可无缝集成到现有主流视频扩散模型，极大提升了视频生成的实用性和可扩展性。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li><li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li><li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li><li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li></ol><hr><h2 id="背景与研究目标" tabindex="-1"><a class="header-anchor" href="#背景与研究目标"><span>背景与研究目标</span></a></h2><p>视频生成领域，基于“下一帧预测”的扩散模型因其逐帧生成能力受到关注。但此类模型普遍面临两大核心挑战：</p><ul><li><strong>遗忘（Forgetting）</strong>：模型难以长期保持时序一致性，导致角色、场景等随时间漂移。</li><li><strong>漂移（Drifting）</strong>：误差逐帧累积，后续帧画质下降，出现模糊、失真等问题。</li></ul><p>传统方法往往顾此失彼：增加历史帧输入可缓解遗忘，却加剧漂移；减少依赖则反之。加之 Transformer 架构的计算复杂度，直接扩展上下文帧数并不可行。</p><hr><h2 id="方法与创新点" tabindex="-1"><a class="header-anchor" href="#方法与创新点"><span>方法与创新点</span></a></h2><p>FramePack 提出两大核心创新：</p><h3 id="_1-自适应帧压缩-adaptive-frame-compression" tabindex="-1"><a class="header-anchor" href="#_1-自适应帧压缩-adaptive-frame-compression"><span>1. 自适应帧压缩（Adaptive Frame Compression）</span></a></h3><ul><li><strong>核心思想</strong>：利用视频帧间冗余，对不同时间距离的帧采用不同压缩率，远帧高压缩，近帧低压缩。</li><li><strong>实现方式</strong>：通过调整 Transformer 输入层 patchify kernel size，实现帧级别分辨率动态分配，保证总上下文长度受控。</li><li><strong>优势</strong>：无需修改主干架构，可直接兼容主流预训练模型，极大提升可用历史帧数。</li></ul><h3 id="_2-反漂移采样-anti-drifting-sampling" tabindex="-1"><a class="header-anchor" href="#_2-反漂移采样-anti-drifting-sampling"><span>2. 反漂移采样（Anti-Drifting Sampling）</span></a></h3><ul><li><strong>创新采样策略</strong>： <ul><li><strong>Vanilla</strong>：传统顺序采样，误差易累积。</li><li><strong>Anti-Drifting</strong>：先生成端点帧，再补中间帧，利用双向上下文提升一致性。</li><li><strong>Inverted Anti-Drifting</strong>：逆序采样，适合 image-to-video，能更好保持高质量起始帧特征。</li></ul></li><li><strong>实验发现</strong>：反漂移采样显著降低误差累积，提升长视频画质。</li></ul><hr><h2 id="实验与结果分析" tabindex="-1"><a class="header-anchor" href="#实验与结果分析"><span>实验与结果分析</span></a></h2><ul><li><strong>兼容性</strong>：FramePack 可直接微调 HunyuanVideo、Wan 等主流视频扩散模型。</li><li><strong>性能提升</strong>： <ul><li><strong>漂移抑制</strong>：Inverted Anti-Drifting 采样在多项指标和主观评价中表现最佳。</li><li><strong>画质提升</strong>：支持更大 batch size，训练更高效，生成视频更长且一致性更好。</li><li><strong>效率提升</strong>：6GB 显存即可驱动 13B 模型生成 1 分钟视频，极大降低硬件门槛。</li></ul></li><li><strong>消融实验</strong>：不同压缩策略和采样方法对性能影响显著，最佳配置为几何级压缩+反漂移采样。</li></ul><hr><h2 id="模型启发与方法延伸" tabindex="-1"><a class="header-anchor" href="#模型启发与方法延伸"><span>模型启发与方法延伸</span></a></h2><ul><li><strong>通用性</strong>：FramePack 作为输入预处理模块，可迁移至各类时序生成任务。</li><li><strong>与现有方法对比</strong>：相比单纯增加上下文或改进主干结构，FramePack以更低成本实现更优时序一致性。</li><li><strong>未来方向</strong>：可与更高效注意力机制、长时序建模方法结合，进一步提升超长视频生成能力。</li></ul><hr><h2 id="结论与未来展望" tabindex="-1"><a class="header-anchor" href="#结论与未来展望"><span>结论与未来展望</span></a></h2><p>FramePack 有效打破了“遗忘-漂移”二难困境，为视频生成模型带来更长时序、更高一致性和更低算力门槛。其自适应帧压缩与创新采样策略为后续视频生成研究提供了新范式。未来，FramePack 有望在内容创作、虚拟现实等领域发挥更大作用。</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h3><ol><li><a href="https://arxiv.org/abs/2504.12626" target="_blank" rel="noopener noreferrer">FramePack 论文</a></li><li><a href="https://lllyasviel.github.io/frame_pack_gitpage/" target="_blank" rel="noopener noreferrer">FramePack 项目主页</a></li><li><a href="https://github.com/lllyasviel/FramePack" target="_blank" rel="noopener noreferrer">FramePack GitHub</a></li><li><a href="https://www.alphaxiv.org/overview/2504.12626" target="_blank" rel="noopener noreferrer">alphaXiv 博客原文</a></li></ol>',31)]))}]]),o=JSON.parse('{"path":"/zh/posts/papers/framepack.html","title":"【论文精读】FramePack：Packing Input Frame Context in Next-Frame Prediction Models for Video Generation","lang":"zh-CN","frontmatter":{"description":"【论文精读】FramePack：Packing Input Frame Context in Next-Frame Prediction Models for Video Generation FramePack Compression ApproachesFramePack Compression Approaches 摘要 FramePack 由斯...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/papers/framepack.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"【论文精读】FramePack：Packing Input Frame Context in Next-Frame Prediction Models for Video Generation"}],["meta",{"property":"og:description","content":"【论文精读】FramePack：Packing Input Frame Context in Next-Frame Prediction Models for Video Generation FramePack Compression ApproachesFramePack Compression Approaches 摘要 FramePack 由斯..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://paper-assets.alphaxiv.org/figures/2504.12626/x1.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"【论文精读】FramePack：Packing Input Frame Context in Next-Frame Prediction Models for Video Generation\\",\\"image\\":[\\"https://paper-assets.alphaxiv.org/figures/2504.12626/x1.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"背景与研究目标","slug":"背景与研究目标","link":"#背景与研究目标","children":[]},{"level":2,"title":"方法与创新点","slug":"方法与创新点","link":"#方法与创新点","children":[{"level":3,"title":"1. 自适应帧压缩（Adaptive Frame Compression）","slug":"_1-自适应帧压缩-adaptive-frame-compression","link":"#_1-自适应帧压缩-adaptive-frame-compression","children":[]},{"level":3,"title":"2. 反漂移采样（Anti-Drifting Sampling）","slug":"_2-反漂移采样-anti-drifting-sampling","link":"#_2-反漂移采样-anti-drifting-sampling","children":[]}]},{"level":2,"title":"实验与结果分析","slug":"实验与结果分析","link":"#实验与结果分析","children":[]},{"level":2,"title":"模型启发与方法延伸","slug":"模型启发与方法延伸","link":"#模型启发与方法延伸","children":[]},{"level":2,"title":"结论与未来展望","slug":"结论与未来展望","link":"#结论与未来展望","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":3.34,"words":1002},"filePathRelative":"zh/posts/papers/framepack.md","excerpt":"\\n<figure><img src=\\"https://paper-assets.alphaxiv.org/figures/2504.12626/x1.png\\" alt=\\"FramePack Compression Approaches\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>FramePack Compression Approaches</figcaption></figure>\\n<h2>摘要</h2>\\n<p>FramePack 由斯坦福大学 Lvmin Zhang 和 Maneesh Agrawala 提出，创新性地解决了视频生成中“遗忘-漂移”两难问题。通过自适应帧压缩与反漂移采样，FramePack 兼顾长时序一致性与视觉质量，支持在有限算力下生成更长、更稳定的视频。该方法可无缝集成到现有主流视频扩散模型，极大提升了视频生成的实用性和可扩展性。</p>","autoDesc":true}')}}]);