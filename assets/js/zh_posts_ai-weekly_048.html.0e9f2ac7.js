"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[7044],{66262:(e,t)=>{t.A=(e,t)=>{const a=e.__vccOpts||e;for(const[e,r]of t)a[e]=r;return a}},16895:(e,t,a)=>{a.r(t),a.d(t,{comp:()=>n,data:()=>o});var r=a(20641);const i={},n=(0,a(66262).A)(i,[["render",function(e,t){return(0,r.uX)(),(0,r.CE)("div",null,t[0]||(t[0]=[(0,r.Fv)('<h1 id="designlab重构设计协作流程-step‐audio-2实现语音理解与对话统一-pusa-v1-0低成本生成高质视频【ai周报】" tabindex="-1"><a class="header-anchor" href="#designlab重构设计协作流程-step‐audio-2实现语音理解与对话统一-pusa-v1-0低成本生成高质视频【ai周报】"><span>DesignLab重构设计协作流程 | Step‑Audio 2实现语音理解与对话统一 | PUSA V1.0低成本生成高质视频【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg" alt="封面图源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面图源自C站作者Koal2</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：DesignLab 引入多角色迭代机制，革新幻灯片设计流程；Step‑Audio 2 集成语音识别与语义理解，构建端到端音频大语言模型；PUSA V1.0 以 $500 成本实现超高质图像/文本驱动视频生成，打破算力门槛。详见正文，相关参考链接请见文末。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#no-humans-required%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E9%AB%98%E8%B4%A8%E9%87%8F%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91%E4%B8%89%E5%85%83%E7%BB%84%E6%95%B0%E6%8D%AE%E9%9B%86">No Humans Required：自动生成高质量图像编辑三元组数据集</a></li><li><a href="#designlab%E8%BF%AD%E4%BB%A3%E5%BC%8F%E5%B9%BB%E7%81%AF%E7%89%87%E8%AE%BE%E8%AE%A1%E4%BC%98%E5%8C%96%E6%A1%86%E6%9E%B6">DesignLab：迭代式幻灯片设计优化框架</a></li><li><a href="#csd-var%E8%A7%86%E8%A7%89%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%86%85%E5%AE%B9%E4%B8%8E%E9%A3%8E%E6%A0%BC%E8%A7%A3%E8%80%A6%E6%A1%86%E6%9E%B6">CSD‑VAR：视觉自回归模型中的内容与风格解耦框架</a></li><li><a href="#yume%E4%BB%8E%E5%9B%BE%E5%83%8F%E6%9E%84%E5%BB%BA%E5%8F%AF%E6%8E%A2%E7%B4%A2%E5%8A%A8%E6%80%81%E4%B8%96%E7%95%8C">Yume：从图像构建可探索动态世界</a></li><li><a href="#step-audio2%E7%AB%AF%E5%88%B0%E7%AB%AF%E9%9F%B3%E9%A2%91%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0%E8%A1%8C%E4%B8%9A%E7%BA%A7%E7%90%86%E8%A7%A3%E4%B8%8E%E5%AF%B9%E8%AF%9D%E8%83%BD%E5%8A%9B">Step‑Audio 2：端到端音频大语言模型，实现行业级理解与对话能力</a></li><li><a href="#pusa-v10%E4%BD%8E%E6%88%90%E6%9C%AC%E9%AB%98%E6%80%A7%E8%83%BD%E5%9B%BE%E5%83%8F%E6%96%87%E5%AD%97%E9%A9%B1%E5%8A%A8%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">PUSA V1.0：低成本高性能图像／文字驱动视频生成模型</a></li></ol><hr><h2 id="no-humans-required-自动生成高质量图像编辑三元组数据集" tabindex="-1"><a class="header-anchor" href="#no-humans-required-自动生成高质量图像编辑三元组数据集"><span>No Humans Required：自动生成高质量图像编辑三元组数据集</span></a></h2><figure><img src="https://riko0.github.io/No-Humans-Required/images/pipeline.jpg" alt="No Humans Required Pipeline 图" tabindex="0" loading="lazy"><figcaption>No Humans Required Pipeline 图</figcaption></figure><p><strong>概要</strong>：<strong>SALUTEDEV</strong> 团队开发了一套无需人工参与的图像编辑数据构建流程<strong>No Humans Required</strong> 。通过自动采样自然图像、生成对应编辑指令与处理后结果，完成了极高保真度的 <strong>NHR‑Edit</strong> 数据集构建，含达 <strong>35.8 万组</strong>三元组信息。该数据广泛适用于训练指令驱动的图像编辑模型，生成效果在多个 benchmarks 上已接近或超过人工标注数据模型，推动图像编辑任务的数据自动化发展。</p><p><strong>标签</strong>：#图像编辑 #自动标签生成 #编辑三元组 #数据构建 #指令驱动</p><hr><h2 id="designlab-迭代式幻灯片设计优化框架" tabindex="-1"><a class="header-anchor" href="#designlab-迭代式幻灯片设计优化框架"><span>DesignLab：迭代式幻灯片设计优化框架</span></a></h2><figure><img src="https://yeolj00.github.io/personal-projects/designlab/assets/intro_teaser.png" alt="DesignLab Teaser 图" tabindex="0" loading="lazy"><figcaption>DesignLab Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>DesignLab</strong> 是由 <strong>索尼</strong> 与 <strong>KAIST</strong> 团队联合提出的全新设计协作系统，首次将幻灯片设计流程拆解为“Reviewer 检测→Contributor 修正→循环反馈”的迭代机制，以模拟人类设计师的真实工作流。系统能自动识别布局、配色、排版等问题，并引导修正优化，显著提升非设计专业用户输出幻灯片的视觉一致性与专业度 。</p><p><strong>标签</strong>：#幻灯片设计 #迭代优化 #人机协同 #视觉质量提升 #设计辅助</p><hr><h2 id="csd‐var-视觉自回归模型中的内容与风格解耦框架" tabindex="-1"><a class="header-anchor" href="#csd‐var-视觉自回归模型中的内容与风格解耦框架"><span>CSD‑VAR：视觉自回归模型中的内容与风格解耦框架</span></a></h2><figure><img src="https://arxiv.org/html/2507.13984v1/x1.png" alt="CSD‑VAR Teaser 图" tabindex="0" loading="lazy"><figcaption>CSD‑VAR Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>CSD‑VAR</strong> 由 <strong>Qualcomm AI Research</strong> 和 <strong>MovianAI</strong> 研究团队提出，是首个在视觉自回归（VAR）架构中实现图像内容与风格分离的方法。该方法采用<strong>尺度感知交替优化</strong>策略，将内容与风格分别绑定至不同尺度表示，引入 SVD 校正减少风格表征中的内容泄漏，并使用增强型键值记忆（Augmented K‑V Memory）机制强化内容一致性保存。研究还发布了针对该任务的 CSD‑100 数据集。实验结果表明，CSD‑VAR 在内容保留与风格迁移精度上均优于现有扩散基础方法，在艺术风格重构与内容迁移方面提供更高创作自由度。</p><p><strong>标签</strong>：#内容风格分离 #视觉自回归 #尺度优化 #记忆机制 #风格迁移控制</p><hr><h2 id="yume-从图像构建可探索动态世界" tabindex="-1"><a class="header-anchor" href="#yume-从图像构建可探索动态世界"><span>Yume：从图像构建可探索动态世界</span></a></h2><figure><img src="https://stdstu12.github.io/YUME-Project/static/images/yume1.png" alt="Yume Demo 图" tabindex="0" loading="lazy"><figcaption>Yume Demo 图</figcaption></figure><p><strong>概要</strong>：<strong>Yume</strong> 项目由<strong>中科大</strong>联手<strong>港科大</strong>等团队提出，旨在基于输入的图像、文本或视频，自动生成一个高度真实、动态交互的虚拟世界。用户可以通过键盘控制甚至神经信号进行探索，系统架构包括带记忆模块的 Masked Video Diffusion Transformer（MVDT）、抗伪影采样器（AAM）、时间旅行扩散采样（TTS‑SDE）和世界运动量化模块，训练使用 Sekai 世界探索数据集。当前该预览模型支持实时世界导航探索，展示了从静态视觉内容到连续视频环境的创新路径。项目预计持续迭代更新，已提供 GitHub 和项目官网内容。</p><p><strong>标签</strong>：#虚拟世界生成 #视频扩散 #记忆网络 #交互探索 #Sekai数据集</p><hr><h2 id="step‐audio-2-端到端音频大语言模型-实现行业级理解与对话能力" tabindex="-1"><a class="header-anchor" href="#step‐audio-2-端到端音频大语言模型-实现行业级理解与对话能力"><span>Step‑Audio 2：端到端音频大语言模型，实现行业级理解与对话能力</span></a></h2><figure><img src="https://github.com/stepfun-ai/Step-Audio2/raw/main/assets/usage.jpg" alt="Step‑Audio 2 Usage 图" tabindex="0" loading="lazy"><figcaption>Step‑Audio 2 Usage 图</figcaption></figure><p><strong>概要</strong>：<strong>Step‑Audio 2</strong> 是由 <strong>StepFun</strong> 团队开发的多模态大语言模型，可直接从原始音频输入理解语义与副语言信息，并生成连贯的文本与音频响应。模型通过隐向量音频编码器、强化学习（RL）与检索增强生成（RAG）机制，引入工具调用（如 Web 搜索、音频检索）实现对复杂任务的处理；系统支持语义理解、情感音调、方言风格等多维度控制，评测结果显示其在 ASR、多语种音频理解、工具调用以及语音对话任务上显著领先同类开源和商用模型。Step‑Audio 2 可用于真实语音代理、客服助手和智能对话系统。</p><p><strong>标签</strong>：#音频大语言模型 #端到端对话 #强化学习 #工具调用 #多语言理解</p><hr><h2 id="pusa-v1-0-低成本高性能图像-文字驱动视频生成模型" tabindex="-1"><a class="header-anchor" href="#pusa-v1-0-低成本高性能图像-文字驱动视频生成模型"><span>PUSA V1.0：低成本高性能图像／文字驱动视频生成模型</span></a></h2><figure><img src="https://yaofang-liu.github.io/Pusa_Web/Pusa_teaser1_01.png" alt="Pusa V1.0 Comparison 图" tabindex="0" loading="lazy"><figcaption>Pusa V1.0 Comparison 图</figcaption></figure><p><strong>概要</strong>：<strong>Pusa V1.0</strong> 是由 <strong>港城大</strong> 和 <strong>华为</strong> 等团队联合提出的创新视频生成模型。该模型引入 <strong>Vectorized Timestep Adaptation（VTA）</strong> 技术，通过将 Wan‑T2V‑14B 基础模型的时步变量从标量扩展为矢量，实现每帧噪声控制的非破坏性微调。此方法仅使用约 4000 个训练样本、$500 训练成本，即可在 VBench‑I2V 上达到 <strong>87.32%</strong> 的评分，略优于需 100 K 美元与千万级样本训练的 Wan‑I2V‑14B（86.86%）。Pusa 同时支持文本到视频、图像到视频、关键帧控制与视频延展等多项任务，无需额外训练即可零-shot 扩展功能。该项目完整发布训练与推理脚本、LoRA 模型权重及数据集，极大降低高质量视频生成的研发门槛，是视频扩散领域的一项里程碑进展。</p><p><strong>标签</strong>：#向量化时步控制 #视频扩散模型 #低资源训练 #多任务零 shot #高效生成</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://riko0.github.io/No-Humans-Required/" target="_blank" rel="noopener noreferrer">No Humans Required 项目页面</a></li><li><a href="https://huggingface.co/datasets/iitolstykh/NHR-Edit" target="_blank" rel="noopener noreferrer">NHR‑Edit 数据集（HuggingFace）</a></li><li><a href="https://arxiv.org/pdf/2507.14119" target="_blank" rel="noopener noreferrer">No Humans Required 论文</a></li><li><a href="https://yeolj00.github.io/personal-projects/designlab/" target="_blank" rel="noopener noreferrer">DesignLab 项目页面</a></li><li><a href="https://arxiv.org/html/2507.17202v1" target="_blank" rel="noopener noreferrer">DesignLab 论文</a></li><li><a href="https://arxiv.org/html/2507.13984v1" target="_blank" rel="noopener noreferrer">CSD‑VAR 论文</a></li><li><a href="https://stdstu12.github.io/YUME-Project/" target="_blank" rel="noopener noreferrer">Yume 项目页面</a></li><li><a href="https://github.com/stdstu12/YUME" target="_blank" rel="noopener noreferrer">Yume Github 仓库</a></li><li><a href="https://arxiv.org/html/2507.17744" target="_blank" rel="noopener noreferrer">Yume 论文</a></li><li><a href="https://github.com/stepfun-ai/Step-Audio2" target="_blank" rel="noopener noreferrer">Step‑Audio 2 Github 仓库</a></li><li><a href="https://arxiv.org/html/2507.16632" target="_blank" rel="noopener noreferrer">Step‑Audio 2 论文</a></li><li><a href="https://yaofang-liu.github.io/Pusa_Web/" target="_blank" rel="noopener noreferrer">Pusa V1.0 项目页面</a></li><li><a href="https://github.com/Yaofang-Liu/Pusa-VidGen" target="_blank" rel="noopener noreferrer">Pusa V1.0 Github 仓库</a></li><li><a href="https://arxiv.org/html/2507.16116v1" target="_blank" rel="noopener noreferrer">Pusa V1.0 论文</a></li></ol>',40)]))}]]),o=JSON.parse('{"path":"/zh/posts/ai-weekly/048.html","title":"DesignLab重构设计协作流程 | Step‑Audio 2实现语音理解与对话统一 | PUSA V1.0低成本生成高质视频【AI周报】","lang":"zh-CN","frontmatter":{"description":"DesignLab重构设计协作流程 | Step‑Audio 2实现语音理解与对话统一 | PUSA V1.0低成本生成高质视频【AI周报】 封面图源自C站作者Koal2封面图源自C站作者Koal2 摘要 本周亮点：DesignLab 引入多角色迭代机制，革新幻灯片设计流程；Step‑Audio 2 集成语音识别与语义理解，构建端到端音频大语言模型；P...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/048.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"DesignLab重构设计协作流程 | Step‑Audio 2实现语音理解与对话统一 | PUSA V1.0低成本生成高质视频【AI周报】"}],["meta",{"property":"og:description","content":"DesignLab重构设计协作流程 | Step‑Audio 2实现语音理解与对话统一 | PUSA V1.0低成本生成高质视频【AI周报】 封面图源自C站作者Koal2封面图源自C站作者Koal2 摘要 本周亮点：DesignLab 引入多角色迭代机制，革新幻灯片设计流程；Step‑Audio 2 集成语音识别与语义理解，构建端到端音频大语言模型；P..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"DesignLab重构设计协作流程 | Step‑Audio 2实现语音理解与对话统一 | PUSA V1.0低成本生成高质视频【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg\\",\\"https://riko0.github.io/No-Humans-Required/images/pipeline.jpg\\",\\"https://yeolj00.github.io/personal-projects/designlab/assets/intro_teaser.png\\",\\"https://arxiv.org/html/2507.13984v1/x1.png\\",\\"https://stdstu12.github.io/YUME-Project/static/images/yume1.png\\",\\"https://github.com/stepfun-ai/Step-Audio2/raw/main/assets/usage.jpg\\",\\"https://yaofang-liu.github.io/Pusa_Web/Pusa_teaser1_01.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"No Humans Required：自动生成高质量图像编辑三元组数据集","slug":"no-humans-required-自动生成高质量图像编辑三元组数据集","link":"#no-humans-required-自动生成高质量图像编辑三元组数据集","children":[]},{"level":2,"title":"DesignLab：迭代式幻灯片设计优化框架","slug":"designlab-迭代式幻灯片设计优化框架","link":"#designlab-迭代式幻灯片设计优化框架","children":[]},{"level":2,"title":"CSD‑VAR：视觉自回归模型中的内容与风格解耦框架","slug":"csd‐var-视觉自回归模型中的内容与风格解耦框架","link":"#csd‐var-视觉自回归模型中的内容与风格解耦框架","children":[]},{"level":2,"title":"Yume：从图像构建可探索动态世界","slug":"yume-从图像构建可探索动态世界","link":"#yume-从图像构建可探索动态世界","children":[]},{"level":2,"title":"Step‑Audio 2：端到端音频大语言模型，实现行业级理解与对话能力","slug":"step‐audio-2-端到端音频大语言模型-实现行业级理解与对话能力","link":"#step‐audio-2-端到端音频大语言模型-实现行业级理解与对话能力","children":[]},{"level":2,"title":"PUSA V1.0：低成本高性能图像／文字驱动视频生成模型","slug":"pusa-v1-0-低成本高性能图像-文字驱动视频生成模型","link":"#pusa-v1-0-低成本高性能图像-文字驱动视频生成模型","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":6.21,"words":1863},"filePathRelative":"zh/posts/ai-weekly/048.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg\\" alt=\\"封面图源自C站作者Koal2\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面图源自C站作者Koal2</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：DesignLab 引入多角色迭代机制，革新幻灯片设计流程；Step‑Audio 2 集成语音识别与语义理解，构建端到端音频大语言模型；PUSA V1.0 以 $500 成本实现超高质图像/文本驱动视频生成，打破算力门槛。详见正文，相关参考链接请见文末。</p>","autoDesc":true}')}}]);