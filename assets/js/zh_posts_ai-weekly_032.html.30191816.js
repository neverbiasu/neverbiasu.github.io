"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[3161],{66262:(e,r)=>{r.A=(e,r)=>{const a=e.__vccOpts||e;for(const[e,t]of r)a[e]=t;return a}},79307:(e,r,a)=>{a.r(r),a.d(r,{comp:()=>i,data:()=>o});var t=a(20641);const n={},i=(0,a(66262).A)(n,[["render",function(e,r){return(0,t.uX)(),(0,t.CE)("div",null,r[0]||(r[0]=[(0,t.Fv)('<h1 id="textcrafter精准文本渲染-mocha电影级角色合成-animegamer动漫生活模拟【ai周报】" tabindex="-1"><a class="header-anchor" href="#textcrafter精准文本渲染-mocha电影级角色合成-animegamer动漫生活模拟【ai周报】"><span>TextCrafter精准文本渲染|MoCha电影级角色合成|AnimeGamer动漫生活模拟【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/43d1dfeb-a0bc-4d38-a7d5-2694070fac43/original=true,quality=90/00687-2746924664-vaporwave" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：TextCrafter精准渲染文本；MoCha推出电影级角色合成；Any2Caption增强视频生成；AnimeGamer实现动漫生活模拟；ACTalker多模态音视频生成；OpenDeepSearch赋能搜索AI。其余详见正文。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#textcrafter%E5%A4%8D%E6%9D%82%E8%A7%86%E8%A7%89%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E5%A4%9A%E6%96%87%E6%9C%AC%E7%B2%BE%E7%A1%AE%E6%B8%B2%E6%9F%93">TextCrafter：复杂视觉场景下的多文本精确渲染</a></li><li><a href="#mocha%E8%BF%88%E5%90%91%E7%94%B5%E5%BD%B1%E7%BA%A7%E5%88%AB%E7%9A%84%E8%A7%92%E8%89%B2%E5%90%88%E6%88%90">MoCha：迈向电影级别的角色合成</a></li><li><a href="#any2caption%E5%AE%9E%E7%8E%B0%E5%8F%AF%E6%8E%A7%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E7%9A%84%E9%80%9A%E7%94%A8%E6%9D%A1%E4%BB%B6%E6%8F%8F%E8%BF%B0">Any2Caption：实现可控视频生成的通用条件描述</a></li><li><a href="#animegamer%E5%9F%BA%E4%BA%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%97%A0%E9%99%90%E5%8A%A8%E6%BC%AB%E7%94%9F%E6%B4%BB%E6%A8%A1%E6%8B%9F">AnimeGamer：基于多模态大语言模型的无限动漫生活模拟</a></li><li><a href="#actalker%E6%94%AF%E6%8C%81%E5%A4%9A%E6%A8%A1%E6%80%81%E6%8E%A7%E5%88%B6%E7%9A%84%E9%9F%B3%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E6%A1%86%E6%9E%B6">ACTalker：支持多模态控制的音视频生成框架</a></li><li><a href="#dreamactor-m1%E8%9E%8D%E5%90%88%E6%B7%B7%E5%90%88%E5%BC%95%E5%AF%BC%E7%9A%84%E4%BA%BA%E5%83%8F%E5%8A%A8%E7%94%BB%E7%94%9F%E6%88%90%E6%A1%86%E6%9E%B6">DreamActor-M1：融合混合引导的人像动画生成框架</a></li><li><a href="#opendeepsearch%E5%BC%80%E6%BA%90%E6%8E%A8%E7%90%86%E4%BB%A3%E7%90%86%E8%B5%8B%E8%83%BD%E6%90%9C%E7%B4%A2ai">OpenDeepSearch：开源推理代理赋能搜索AI</a></li></ol><hr><h2 id="textcrafter-复杂视觉场景下的多文本精确渲染" tabindex="-1"><a class="header-anchor" href="#textcrafter-复杂视觉场景下的多文本精确渲染"><span>TextCrafter：复杂视觉场景下的多文本精确渲染</span></a></h2><figure><img src="https://github.com/NJU-PCALab/TextCrafter/raw/main/assets/readme_images/teaser.png" alt="TextCrafter Teaser 图" tabindex="0" loading="lazy"><figcaption>TextCrafter Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>TextCrafter</strong> 是由 <strong>南京大学PCALab</strong> 提出的一种新颖方法，旨在解决复杂视觉文本生成（CVTG）任务中存在的文本混淆、遗漏和模糊等问题。该方法采用渐进式策略，将复杂的视觉文本分解为独立组件，确保文本内容与其视觉载体之间的强对齐性。此外，TextCrafter 引入了令牌聚焦增强机制（token focus enhancement mechanism），在生成过程中突出视觉文本的重要性。研究团队还构建了新的基准数据集 CVTG-2K，用于严格评估生成模型在 CVTG 任务中的性能。实验结果表明，TextCrafter 在复杂场景的文本生成准确性方面优于现有技术。</p><p><strong>标签</strong>：#复杂视觉文本生成 #CVTG #文本渲染 #DiT</p><hr><h2 id="mocha-迈向电影级别的角色合成" tabindex="-1"><a class="header-anchor" href="#mocha-迈向电影级别的角色合成"><span>MoCha：迈向电影级别的角色合成</span></a></h2><figure><img src="https://arxiv.org/html/2503.23307v1/x2.png" alt="MoCha Architecture 图" tabindex="0" loading="lazy"><figcaption>MoCha Architecture 图</figcaption></figure><p><strong>概要</strong>：<strong>MoCha</strong> 由 <strong>Meta GenAI</strong> 和 <strong>滑铁卢大学</strong> 联合推出，旨在直接从语音和文本生成全身说话角色动画。不同于传统的仅限于头部的生成方法，MoCha 能生成完整的人物形象。其核心创新包括引入语音-视频窗口注意力机制，确保语音与视频的精确同步；采用联合训练策略，利用语音和文本标注的视频数据，提升模型对多样化角色动作的泛化能力。此外，MoCha 设计了带有角色标签的结构化提示模板，实现多角色的轮流对话生成。实验结果显示，MoCha 在真实感、表现力、可控性和泛化性方面树立了新的标准。</p><p><strong>标签</strong>：#角色合成 #视频生成 #语音驱动 #多角色对话 #注意力机制</p><hr><h2 id="any2caption-实现可控视频生成的通用条件描述" tabindex="-1"><a class="header-anchor" href="#any2caption-实现可控视频生成的通用条件描述"><span>Any2Caption：实现可控视频生成的通用条件描述</span></a></h2><figure><img src="https://github.com/ChocoWu/Any2Caption/raw/main/assets/intro.png" alt="Any2Caption Intro 图" tabindex="0" loading="lazy"><figcaption>Any2Caption Intro 图</figcaption></figure><p><strong>概要</strong>：<strong>Any2Caption</strong> 是由 <strong>快手科技</strong> 和 <strong>新加坡国立大学</strong> 等机构联合提出的框架，旨在通过解耦条件解释与视频合成步骤，实现对视频生成的精确控制。该方法利用多模态大语言模型（MLLMs），将文本、图像、视频以及区域、运动、摄像机姿态等特定线索转化为密集且结构化的描述，为视频生成模型提供更精确的指导。此外，研究团队构建了包含337K实例和407K条件的大规模数据集 Any2CapIns，用于任意条件到描述的指令微调。综合评估表明，Any2Caption 在提升视频生成模型的可控性和生成质量方面具有显著优势。</p><p><strong>标签</strong>：#可控视频生成 #多模态大语言模型 #条件描述 #可控生成</p><hr><h2 id="animegamer-基于多模态大语言模型的无限动漫生活模拟" tabindex="-1"><a class="header-anchor" href="#animegamer-基于多模态大语言模型的无限动漫生活模拟"><span>AnimeGamer：基于多模态大语言模型的无限动漫生活模拟</span></a></h2><figure><img src="https://howe125.github.io/AnimeGamer.github.io/static/images/model-mllm.png" alt="AnimeGamer Overview 图" tabindex="0" loading="lazy"><figcaption>AnimeGamer Overview 图</figcaption></figure><p><strong>概要</strong>：<strong>AnimeGamer</strong> 由 <strong>腾讯ARC实验室</strong> 和 <strong>香港城市大学</strong> 联合推出，旨在通过多模态大语言模型（MLLMs）实现无限制的动漫生成。该方法通过引入动作感知的多模态表示，结合视频扩散模型，生成动态的动画片段，展现角色动作和状态更新。与传统方法相比，AnimeGamer 能够在游戏状态生成中保持上下文一致性和动态性，为玩家提供沉浸式的动漫角色体验。实验结果表明，AnimeGamer 在游戏体验的各个方面均优于现有方法。</p><p><strong>标签</strong>：#动漫生活模拟 #多模态大语言模型 #视频生成 #游戏状态预测 #视频扩散模型</p><hr><h2 id="actalker-支持多模态控制的音视频生成框架" tabindex="-1"><a class="header-anchor" href="#actalker-支持多模态控制的音视频生成框架"><span>ACTalker：支持多模态控制的音视频生成框架</span></a></h2><figure><img src="https://harlanhong.github.io/publications/actalker/assets/pics/framework.png" alt="ACTalker Framework 图" tabindex="0" loading="lazy"><figcaption>ACTalker Framework 图</figcaption></figure><p><strong>概要</strong>：<strong>ACTalker</strong> 是由 <strong>香港科技大学</strong> 和 <strong>腾讯</strong> 等机构联合提出的端到端视频扩散框架，旨在通过多信号控制生成自然的说话人头像视频。该框架设计了并行的 Mamba 结构，每个分支利用单独的驱动信号控制特定面部区域，并通过门控机制实现灵活的视频生成控制。为确保生成视频在时间和空间上的自然协调，ACTalker 引入了 Mask-SSM 策略，使每个驱动信号独立控制对应的面部区域，避免控制冲突。实验结果表明，该方法能够生成由多种信号驱动的自然面部视频，且 Mamba 层能够无缝整合多种驱动模态而无冲突。</p><p><strong>标签</strong>：#说话人头像生成 #多模态控制 #视频扩散模型 #面部动画</p><hr><h2 id="dreamactor-m1-融合混合引导的人像动画生成框架" tabindex="-1"><a class="header-anchor" href="#dreamactor-m1-融合混合引导的人像动画生成框架"><span>DreamActor-M1：融合混合引导的人像动画生成框架</span></a></h2><figure><img src="https://grisoon.github.io/DreamActor-M1/static/images/2-overview.png" alt="DreamActor-M1 Overview 图" tabindex="0" loading="lazy"><figcaption>DreamActor-M1 Overview 图</figcaption></figure><p><strong>概要</strong>：<strong>DreamActor-M1</strong> 由 <strong>字节跳动智能创作团队</strong> 提出，是一个基于扩散Transformer（DiT）的创新人像动画生成框架，旨在解决现有人像动画方法在精细控制、多尺度适应性和长期时间一致性方面的不足。该方法通过融合隐式面部表示、3D头部球体和3D身体骨架，实现对面部表情和身体动作的精确控制，生成具有表现力且能保持身份一致性的人像动画。此外，采用渐进式训练策略，处理从肖像到全身等不同尺度的人体姿态，并通过整合连续帧的运动模式与互补的视觉参考，确保在复杂动作中的长期时间一致性。实验结果表明，DreamActor-M1 在肖像、上半身和全身动画生成方面均优于现有技术，生成的视频在视觉质量、时间一致性和身份保持方面表现出色。</p><p><strong>标签</strong>：#人像动画 #扩散Transformer #混合引导 #时间一致性 #多尺度适应</p><hr><h2 id="opendeepsearch-开源推理代理赋能搜索ai" tabindex="-1"><a class="header-anchor" href="#opendeepsearch-开源推理代理赋能搜索ai"><span>OpenDeepSearch：开源推理代理赋能搜索AI</span></a></h2><figure><img src="https://arxiv.org/html/2503.20201v1/x1.png" alt="OpenDeepSearch Framework 图" tabindex="0" loading="lazy"><figcaption>OpenDeepSearch Framework 图</figcaption></figure><p><strong>概要</strong>：<strong>OpenDeepSearch (ODS)</strong> 由 <strong>Sentient Foundation</strong> 开发，旨在缩小专有搜索AI解决方案（如Perplexity和ChatGPT Search）与开源替代方案之间的差距。ODS通过引入推理代理，增强了开源大型语言模型（LLMs）的推理能力，使其能够明智地利用网络搜索工具回答查询。具体而言，ODS包含两个核心组件：Open Search Tool和Open Reasoning Agent。Open Search Tool高效地从网络检索信息，而Open Reasoning Agent利用LLMs生成深入的回答。评估结果显示，ODS在单跳查询（如SimpleQA）上表现与闭源替代方案相当，在多跳查询（如FRAMES基准）上表现更优，准确率提高了9.7%。该项目的开源特性为企业和开发者提供了定制AI搜索解决方案的灵活性，推动了搜索增强AI的民主化进程。</p><p><strong>标签</strong> #推理代理 #搜索AI #LLMs #Agent</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://dnknju.github.io/textcrafter-vue/" target="_blank" rel="noopener noreferrer">TextCrafter 项目主页</a></li><li><a href="https://github.com/NJU-PCALab/TextCrafter" target="_blank" rel="noopener noreferrer">TextCrafter GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2503.23461v2" target="_blank" rel="noopener noreferrer">TextCrafter 论文链接</a></li><li><a href="https://congwei1230.github.io/MoCha/" target="_blank" rel="noopener noreferrer">MoCha 项目主页</a></li><li><a href="https://arxiv.org/html/2503.23461v2" target="_blank" rel="noopener noreferrer">MoCha 论文链接</a></li><li><a href="https://sqwu.top/Any2Cap/" target="_blank" rel="noopener noreferrer">Any2Caption 项目主页</a></li><li><a href="https://github.com/ChocoWu/Any2Caption" target="_blank" rel="noopener noreferrer">Any2Caption GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2503.24379v1" target="_blank" rel="noopener noreferrer">Any2Caption 论文链接</a></li><li><a href="https://howe125.github.io/AnimeGamer.github.io/" target="_blank" rel="noopener noreferrer">AnimeGamer 项目主页</a></li><li><a href="https://github.com/TencentARC/AnimeGamer" target="_blank" rel="noopener noreferrer">AnimeGamer GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2504.01014v1" target="_blank" rel="noopener noreferrer">AnimeGamer 论文链接</a></li><li><a href="https://harlanhong.github.io/publications/actalker/index.html" target="_blank" rel="noopener noreferrer">ACTalker 项目主页</a></li><li><a href="https://github.com/harlanhong" target="_blank" rel="noopener noreferrer">ACTalker GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2504.02542v1" target="_blank" rel="noopener noreferrer">ACTalker 论文链接</a></li><li><a href="https://grisoon.github.io/DreamActor-M1/" target="_blank" rel="noopener noreferrer">DreamActor-M1 项目主页</a></li><li><a href="https://arxiv.org/html/2504.01724" target="_blank" rel="noopener noreferrer">DreamActor-M1 论文链接</a></li><li><a href="https://github.com/sentient-agi/opendeepsearch" target="_blank" rel="noopener noreferrer">OpenDeepSearch GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2503.20201v1" target="_blank" rel="noopener noreferrer">OpenDeepSearch 论文链接</a></li></ol>',45)]))}]]),o=JSON.parse('{"path":"/zh/posts/ai-weekly/032.html","title":"TextCrafter精准文本渲染|MoCha电影级角色合成|AnimeGamer动漫生活模拟【AI周报】","lang":"zh-CN","frontmatter":{"description":"TextCrafter精准文本渲染|MoCha电影级角色合成|AnimeGamer动漫生活模拟【AI周报】 封面源自C站作者Koal2封面源自C站作者Koal2 摘要 本周亮点：TextCrafter精准渲染文本；MoCha推出电影级角色合成；Any2Caption增强视频生成；AnimeGamer实现动漫生活模拟；ACTalker多模态音视频生成；O...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/032.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"TextCrafter精准文本渲染|MoCha电影级角色合成|AnimeGamer动漫生活模拟【AI周报】"}],["meta",{"property":"og:description","content":"TextCrafter精准文本渲染|MoCha电影级角色合成|AnimeGamer动漫生活模拟【AI周报】 封面源自C站作者Koal2封面源自C站作者Koal2 摘要 本周亮点：TextCrafter精准渲染文本；MoCha推出电影级角色合成；Any2Caption增强视频生成；AnimeGamer实现动漫生活模拟；ACTalker多模态音视频生成；O..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/43d1dfeb-a0bc-4d38-a7d5-2694070fac43/original=true,quality=90/00687-2746924664-vaporwave"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"TextCrafter精准文本渲染|MoCha电影级角色合成|AnimeGamer动漫生活模拟【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/43d1dfeb-a0bc-4d38-a7d5-2694070fac43/original=true,quality=90/00687-2746924664-vaporwave\\",\\"https://github.com/NJU-PCALab/TextCrafter/raw/main/assets/readme_images/teaser.png\\",\\"https://arxiv.org/html/2503.23307v1/x2.png\\",\\"https://github.com/ChocoWu/Any2Caption/raw/main/assets/intro.png\\",\\"https://howe125.github.io/AnimeGamer.github.io/static/images/model-mllm.png\\",\\"https://harlanhong.github.io/publications/actalker/assets/pics/framework.png\\",\\"https://grisoon.github.io/DreamActor-M1/static/images/2-overview.png\\",\\"https://arxiv.org/html/2503.20201v1/x1.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"TextCrafter：复杂视觉场景下的多文本精确渲染","slug":"textcrafter-复杂视觉场景下的多文本精确渲染","link":"#textcrafter-复杂视觉场景下的多文本精确渲染","children":[]},{"level":2,"title":"MoCha：迈向电影级别的角色合成","slug":"mocha-迈向电影级别的角色合成","link":"#mocha-迈向电影级别的角色合成","children":[]},{"level":2,"title":"Any2Caption：实现可控视频生成的通用条件描述","slug":"any2caption-实现可控视频生成的通用条件描述","link":"#any2caption-实现可控视频生成的通用条件描述","children":[]},{"level":2,"title":"AnimeGamer：基于多模态大语言模型的无限动漫生活模拟","slug":"animegamer-基于多模态大语言模型的无限动漫生活模拟","link":"#animegamer-基于多模态大语言模型的无限动漫生活模拟","children":[]},{"level":2,"title":"ACTalker：支持多模态控制的音视频生成框架","slug":"actalker-支持多模态控制的音视频生成框架","link":"#actalker-支持多模态控制的音视频生成框架","children":[]},{"level":2,"title":"DreamActor-M1：融合混合引导的人像动画生成框架","slug":"dreamactor-m1-融合混合引导的人像动画生成框架","link":"#dreamactor-m1-融合混合引导的人像动画生成框架","children":[]},{"level":2,"title":"OpenDeepSearch：开源推理代理赋能搜索AI","slug":"opendeepsearch-开源推理代理赋能搜索ai","link":"#opendeepsearch-开源推理代理赋能搜索ai","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":7.34,"words":2202},"filePathRelative":"zh/posts/ai-weekly/032.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/43d1dfeb-a0bc-4d38-a7d5-2694070fac43/original=true,quality=90/00687-2746924664-vaporwave\\" alt=\\"封面源自C站作者Koal2\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面源自C站作者Koal2</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：TextCrafter精准渲染文本；MoCha推出电影级角色合成；Any2Caption增强视频生成；AnimeGamer实现动漫生活模拟；ACTalker多模态音视频生成；OpenDeepSearch赋能搜索AI。其余详见正文。</p>","autoDesc":true}')}}]);