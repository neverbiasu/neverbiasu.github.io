"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[8617],{6262:(e,i)=>{i.A=(e,i)=>{const t=e.__vccOpts||e;for(const[e,r]of i)t[e]=r;return t}},7801:(e,i,t)=>{t.r(i),t.d(i,{comp:()=>a,data:()=>o});var r=t(641);const n={},a=(0,t(6262).A)(n,[["render",function(e,i){return(0,r.uX)(),(0,r.CE)("div",null,i[0]||(i[0]=[(0,r.Fv)('<h1 id="materialanything生成pbr材质-consisid优化id视频生成-flipsketch实现草图动画【ai周报】" tabindex="-1"><a class="header-anchor" href="#materialanything生成pbr材质-consisid优化id视频生成-flipsketch实现草图动画【ai周报】"><span>MaterialAnything生成PBR材质|ConsisID优化ID视频生成|FlipSketch实现草图动画【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/7ec38594-6a9f-469a-9515-cc2a8d4ae2f5/original=true,quality=90/42025535.jpeg" alt="封面源自C站作者Hannibal_Lecter" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Hannibal_Lecter</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周聚焦生成式AI创新：MaterialAnything自动生成PBR材质；OminiControl为Diffusion模型提供通用轻量控制框架；FlipSketch生成草图动画。这些进步展示了生成式AI的迭代效率之高，其余内容详见正文。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#showui">ShowUI: A Large Multimodal Dataset for AI-Powered UI Generation</a></li><li><a href="#roictrl">ROICtrl: Region-Based Image Editing Control with Diffusion Models</a></li><li><a href="#consisid">ConsisID: Consistency in Instance Discrimination</a></li><li><a href="#sketchagent">SketchAgent: Zero-Shot and Few-Shot Sketch Understanding</a></li><li><a href="#anchorcrafter">AnchorCrafter: Novel Methods for Generating Dense Image Anchors</a></li><li><a href="#texgen">TEXGen: Generative Textures for 3D Applications</a></li><li><a href="#materialanything">MaterialAnything: Material Diffusion Models for 3D Design</a></li><li><a href="#diptychprompting">DiptychPrompting: A Novel Prompting Paradigm for Image Editing</a></li><li><a href="#ominicontrol">OminiControl: Unified Framework for Interactive Control of AI Models</a></li><li><a href="#flipsketch">FlipSketch: AI-Powered Sketch Morphing and Editing</a></li></ol><hr><h2 id="showui-多模态用户界面生成数据集" tabindex="-1"><a class="header-anchor" href="#showui-多模态用户界面生成数据集"><span>ShowUI：多模态用户界面生成数据集</span></a></h2><figure><img src="https://arxiv.org/html/2411.17465v1/x4.png" alt="ShowUI Overview 图" tabindex="0" loading="lazy"><figcaption>ShowUI Overview 图</figcaption></figure><p><strong>概要</strong>：来自 <strong>新国立</strong> 的 <strong>ShowLab</strong> 介绍了一种名为 <strong>ShowUI</strong> 的视觉-语言-动作模型，旨在提高GUI代理在视觉感知和交互方面的性能。ShowUI通过UI引导的视觉标记选择、交错式视觉-语言-动作流以及数据管理策略，实现了高效且准确的GUI任务处理。具体来说，该模型通过将屏幕截图转换为UI连接图来降低计算成本，并自适应地识别冗余关系；同时，通过灵活的任务流管理和重采样策略，解决了数据不平衡问题。</p><p><strong>标签</strong>：#GUI代理 #视觉-语言-动作 #标记选择 #数据管理 #零样本学习</p><hr><h2 id="roictrl-多实例可控视觉生成框架" tabindex="-1"><a class="header-anchor" href="#roictrl-多实例可控视觉生成框架"><span>ROICtrl：多实例可控视觉生成框架</span></a></h2><figure><img src="https://roictrl.github.io/assets/method_overview.png" alt="ROICtrl  Overview 图" tabindex="0" loading="lazy"><figcaption>ROICtrl Overview 图</figcaption></figure><p><strong>概要</strong>：<strong>ROICtrl</strong> 是由新加坡国立大学 (NUS) 与 Meta 等联合开发的视觉生成工具，扩展了现有Diffusion模型（如 ControlNet、T2I-Adapter）的功能，支持对图像多个区域的精确控制。ROICtrl 利用区域对齐与反对齐 (ROI-Align 和 ROI-Unpool) 技术，实现高分辨率图像中每个实例的独立生成和操控，同时显著降低计算开销。</p><p><strong>标签</strong>：#视觉生成 #区域控制 #Diffusion模型 #多实例生成</p><hr><h2 id="consisid-一致性身份保留的文本到视频生成框架" tabindex="-1"><a class="header-anchor" href="#consisid-一致性身份保留的文本到视频生成框架"><span>ConsisID：一致性身份保留的文本到视频生成框架</span></a></h2><figure><img src="https://github.com/user-attachments/assets/c23b207f-96ff-4b94-9ea5-d385ed65c477" alt="ConsisID Overview 图" tabindex="0" loading="lazy"><figcaption>ConsisID Overview 图</figcaption></figure><p><strong>概要</strong>：<strong>ConsisID</strong> 是由北京大学 Yuan Group 开发的创新文本到视频生成框架，专注于在视频生成中保持人物身份一致性。该项目通过频率分解技术对 <strong>Diffusion Transformers (DiT)</strong> 模型进行了优化，能够在复杂场景下生成具有身份特征一致的多帧视频。这一框架无需额外的微调步骤，显著提升了生成质量和效率。</p><p><strong>标签</strong>：#文本到视频 #一致性生成 #北京大学 #Diffusion模型</p><hr><h2 id="sketchagent-语言驱动的交互式素描生成" tabindex="-1"><a class="header-anchor" href="#sketchagent-语言驱动的交互式素描生成"><span>SketchAgent：语言驱动的交互式素描生成</span></a></h2><figure><img src="https://github.com/yael-vinker/SketchAgent/blob/main/repo_images/teaser.jpg?raw=true" alt="SketchAgent Teaser 图" tabindex="0" loading="lazy"><figcaption>SketchAgent Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>SketchAgent</strong> 是由 <strong>MIT</strong>, <strong>Stanford</strong> 推出的一种新型素描生成智能体，结合多模态大语言模型 (LLM)，支持基于自然语言的连续素描生成和交互式绘画。用户可以通过文本描述生成复杂的素描，并使用交互界面进行协作式绘画或编辑。其设计支持多样化概念的生成，适合零样本或小样本的绘画任务。</p><p><strong>标签</strong>：#素描生成 #多模态AI #人机交互 #LLM</p><hr><h2 id="anchorcrafter-密集图像锚点生成的新方法" tabindex="-1"><a class="header-anchor" href="#anchorcrafter-密集图像锚点生成的新方法"><span>AnchorCrafter：密集图像锚点生成的新方法</span></a></h2><figure><img src="https://cangcz.github.io/Anchor-Crafter/static/images/method.png" alt="AnchorCrafter Pipeline 图" tabindex="0" loading="lazy"><figcaption>AnchorCrafter Pipeline 图</figcaption></figure><p><strong>概要</strong>：<strong>AnchorCrafter</strong> 是一种基于Diffusion的系统，用于自动生成具有高视觉保真度和可控互动的2D产品推广视频。该系统通过整合人-物交互（HOI）来解决姿态引导的人类视频生成中的核心问题。具体来说，AnchorCrafter提出了两个关键创新：HOI外观感知，增强从任意多视角识别物体外观并解耦物体和人类外观；HOI运动注入，通过克服物体轨迹条件和互遮挡管理的挑战，实现复杂的人-物互动。此外，还引入了HOI区域重加权损失，以增强物体细节的学习。</p><p><strong>标签</strong>：#Diffusion #图像表示 #HOI #数字人</p><hr><h2 id="texgen-基于diffusion模型的网格纹理生成" tabindex="-1"><a class="header-anchor" href="#texgen-基于diffusion模型的网格纹理生成"><span>TEXGen：基于Diffusion模型的网格纹理生成</span></a></h2><figure><img src="https://cvmi-lab.github.io/TEXGen/figs/teaser.jpg" alt="TEXGen Teaser 图" tabindex="0" loading="lazy"><figcaption>TEXGen Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>TEXGen</strong> 是一个生成式Diffusion模型，专注于通过Diffusion技术直接在UV域生成物体的纹理贴图，特别是Albedo纹理。该方法由 <strong>CVMI实验室</strong>（计算机视觉与机器智能实验室）开发，训练了一个大型的Diffusion模型，旨在提升纹理生成的效率与质量。TEXGen能够处理高分辨率和复杂纹理，同时支持不同类别的三维模型。项目的核心技术基于PyTorch Lightning构建，计划未来发布代码。</p><p><strong>标签</strong>：#纹理生成 #Diffusion模型 #3D建模 #PyTorch Lightning #CVMI实验室</p><hr><h2 id="materialanything-基于ai的自动pbr材质生成" tabindex="-1"><a class="header-anchor" href="#materialanything-基于ai的自动pbr材质生成"><span>MaterialAnything：基于AI的自动PBR材质生成</span></a></h2><figure><img src="https://xhuangcv.github.io/MaterialAnything/static/images/pipeline.jpg" alt="MaterialAnything Pipeline 图" tabindex="0" loading="lazy"><figcaption>MaterialAnything Pipeline 图</figcaption></figure><p><strong>概要</strong>：<strong>MaterialAnything</strong> 是由 <strong>3DTopia</strong> 团队开发的一个开源全自动、统一的扩散框架，用于生成3D对象的基于物理的材质。与依赖复杂流程或特定优化的现有方法不同，Material Anything提供了一种稳健的端到端解决方案，适用于各种光照条件下的对象。该方法利用预训练的图像扩散模型，并通过三头架构和渲染损失来提高稳定性和材质质量。此外，引入了置信度掩码作为扩散模型中的动态切换器，使其能够有效处理不同光照条件下的纹理和无纹理对象。通过使用由这些置信度掩码引导的渐进式材质生成策略以及UV空间材质细化器，确保了一致且UV就绪的材质输出。</p><p><strong>标签</strong>：#PBR材质 #3D建模 #3DMesh #开源</p><hr><h2 id="diptychprompting-零样本主题驱动的图像生成与编辑" tabindex="-1"><a class="header-anchor" href="#diptychprompting-零样本主题驱动的图像生成与编辑"><span>DiptychPrompting：零样本主题驱动的图像生成与编辑</span></a></h2><figure><img src="https://diptychprompting.github.io/DiptychPrompting_files/method.png" alt="DiptychPrompting Overview 图" tabindex="0" loading="lazy"><figcaption>DiptychPrompting Overview 图</figcaption></figure><p><strong>概要</strong>：<strong>DiptychPrompting</strong> 是 <strong>首尔国立大学</strong> 提出的一种创新的文本到图像生成方法，将主题驱动的生成任务重新定义为补全式绘画（inpainting）问题。该方法利用 Diptych（双联画）形式，将参考图像放在左侧，右侧为空白区域，通过文本条件生成新的图像内容。DiptychPrompting 强化了左侧图像与右侧生成的上下文对齐，能够实现高精度主题还原，同时支持风格化生成与主题编辑任务，在多样化场景下具备显著优势。</p><p><strong>标签</strong>：#图像生成 #零样本学习 #主题编辑 #补全式绘画 #风格化生成</p><hr><h2 id="ominicontrol-用于diffusion模型的通用控制框架" tabindex="-1"><a class="header-anchor" href="#ominicontrol-用于diffusion模型的通用控制框架"><span>OminiControl：用于Diffusion模型的通用控制框架</span></a></h2><figure><img src="https://arxiv.org/html/2411.15098v2/x1.png" alt="OminiControl Teaser 图" tabindex="0" loading="lazy"><figcaption>OminiControl Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>OminiControl</strong> 是一个由 <strong>新加坡国立大学</strong> 研发的轻量级通用控制框架，支持对Diffusion Transformer 模型（如 FLUX）的灵活控制。框架以最小化设计为特点，通过插入少量参数（仅 0.1% 额外参数）实现高效的主题驱动和空间控制能力。应用包括边缘引导生成、补全绘画、图像去模糊和颜色调整等任务，同时也支持主题驱动的生成场景。框架集成了易用的 Gradio 界面，适合快速实验和实际应用。</p><p><strong>标签</strong>：#Diffusion模型 #通用控制 #主题驱动生成 #边缘引导生成 #新加坡国立大学</p><hr><h2 id="flipsketch-将静态草图转换为文本引导的动画" tabindex="-1"><a class="header-anchor" href="#flipsketch-将静态草图转换为文本引导的动画"><span>FlipSketch：将静态草图转换为文本引导的动画</span></a></h2><figure><img src="https://file.notion.so/f/f/021ded55-a224-419c-939c-70c6888912f7/bc1ef986-ba63-4f6d-86d8-b03d881a57c8/flipsketch_web.gif?table=block&amp;id=14f5f3c4-a139-80cb-8359-d5f912a75a28&amp;spaceId=021ded55-a224-419c-939c-70c6888912f7&amp;expirationTimestamp=1733133600000&amp;signature=RM1JNYDflVP19hif_TiSNCv25kA5Krb_c2lSpbOyWaA" alt="FlipSketch Demo 图" tabindex="0" loading="lazy"><figcaption>FlipSketch Demo 图</figcaption></figure><p><strong>概要</strong>：FlipSketch 是一个 <strong>萨利大学</strong> 开发的创新生成系统，旨在将静态手绘草图转化为文本引导的动画片段。该工具基于改进的文本到视频生成模型（T2V），结合了草图的输入特性，通过关注点组成（attention composition）方法优化生成过程。在实际应用中，FlipSketch 支持用户使用草图作为输入，配合自然语言描述生成具有动态效果的动画，例如草图内容的移动或逐帧动画展示。其目标是为艺术家和设计者提供便捷的草图动画创作平台。</p><p><strong>标签</strong>：#草图动画 #文本到视频 #生成式AI #草图创作</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接：</span></a></h3><ol><li><a href="https://github.com/showlab/ShowUI" target="_blank" rel="noopener noreferrer">ShowUI</a></li><li><a href="https://arxiv.org/pdf/2411.17465" target="_blank" rel="noopener noreferrer">ShowUI 论文</a></li><li><a href="https://roictrl.github.io/" target="_blank" rel="noopener noreferrer">ROICtrl</a></li><li><a href="https://github.com/showlab/ROICtrl" target="_blank" rel="noopener noreferrer">ROICtrl GitHub</a></li><li><a href="https://arxiv.org/pdf/2411.17949" target="_blank" rel="noopener noreferrer">ROICtrl 论文</a></li><li><a href="https://pku-yuangroup.github.io/ConsisID/" target="_blank" rel="noopener noreferrer">ConsisID</a></li><li><a href="https://github.com/PKU-YuanGroup/ConsisID" target="_blank" rel="noopener noreferrer">ConsisID GitHub</a></li><li><a href="https://arxiv.org/pdf/2411.17440" target="_blank" rel="noopener noreferrer">ConsisID 论文</a></li><li><a href="https://yael-vinker.github.io/sketch-agent/" target="_blank" rel="noopener noreferrer">SketchAgent</a></li><li><a href="https://github.com/yael-vinker/SketchAgent" target="_blank" rel="noopener noreferrer">SketchAgent GitHub</a></li><li><a href="https://arxiv.org/pdf/2411.17673" target="_blank" rel="noopener noreferrer">SketchAgent 论文</a></li><li><a href="https://cangcz.github.io/Anchor-Crafter/" target="_blank" rel="noopener noreferrer">AnchorCrafter</a></li><li><a href="https://github.com/cangcz/AnchorCrafter" target="_blank" rel="noopener noreferrer">AnchorCrafter GitHub</a></li><li><a href="https://arxiv.org/pdf/2411.17383" target="_blank" rel="noopener noreferrer">AnchorCrafter 论文</a></li><li><a href="https://cvmi-lab.github.io/TEXGen/" target="_blank" rel="noopener noreferrer">TEXGen</a></li><li><a href="https://github.com/CVMI-Lab/TEXGen" target="_blank" rel="noopener noreferrer">TEXGen GitHub</a></li><li><a href="https://arxiv.org/pdf/2411.14740" target="_blank" rel="noopener noreferrer">TEXGen 论文</a></li><li><a href="https://github.com/3DTopia/MaterialAnything" target="_blank" rel="noopener noreferrer">MaterialAnything GitHub</a></li><li><a href="https://arxiv.org/pdf/2411.15138" target="_blank" rel="noopener noreferrer">MaterialAnything 论文</a></li><li><a href="https://diptychprompting.github.io/" target="_blank" rel="noopener noreferrer">DiptychPrompting</a></li><li><a href="https://arxiv.org/pdf/2411.15466" target="_blank" rel="noopener noreferrer">DiptychPrompting 论文</a></li><li><a href="https://github.com/Yuanshi9815/OminiControl" target="_blank" rel="noopener noreferrer">OminiControl GitHub</a></li><li><a href="https://arxiv.org/pdf/2411.15098" target="_blank" rel="noopener noreferrer">OminiControl 论文</a></li><li><a href="https://github.com/hmrishavbandy/FlipSketch" target="_blank" rel="noopener noreferrer">FlipSketch GitHub</a></li><li><a href="https://arxiv.org/pdf/2411.10818v1" target="_blank" rel="noopener noreferrer">FlipSketch 论文</a></li></ol>',60)]))}]]),o=JSON.parse('{"path":"/zh/posts/ai-weekly/014.html","title":"MaterialAnything生成PBR材质|ConsisID优化ID视频生成|FlipSketch实现草图动画【AI周报】","lang":"zh-CN","frontmatter":{"description":"MaterialAnything生成PBR材质|ConsisID优化ID视频生成|FlipSketch实现草图动画【AI周报】 封面源自C站作者Hannibal_Lecter封面源自C站作者Hannibal_Lecter 摘要 本周聚焦生成式AI创新：MaterialAnything自动生成PBR材质；OminiControl为Diffusion模型提...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/014.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"MaterialAnything生成PBR材质|ConsisID优化ID视频生成|FlipSketch实现草图动画【AI周报】"}],["meta",{"property":"og:description","content":"MaterialAnything生成PBR材质|ConsisID优化ID视频生成|FlipSketch实现草图动画【AI周报】 封面源自C站作者Hannibal_Lecter封面源自C站作者Hannibal_Lecter 摘要 本周聚焦生成式AI创新：MaterialAnything自动生成PBR材质；OminiControl为Diffusion模型提..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/7ec38594-6a9f-469a-9515-cc2a8d4ae2f5/original=true,quality=90/42025535.jpeg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"MaterialAnything生成PBR材质|ConsisID优化ID视频生成|FlipSketch实现草图动画【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/7ec38594-6a9f-469a-9515-cc2a8d4ae2f5/original=true,quality=90/42025535.jpeg\\",\\"https://arxiv.org/html/2411.17465v1/x4.png\\",\\"https://roictrl.github.io/assets/method_overview.png\\",\\"https://github.com/user-attachments/assets/c23b207f-96ff-4b94-9ea5-d385ed65c477\\",\\"https://github.com/yael-vinker/SketchAgent/blob/main/repo_images/teaser.jpg?raw=true\\",\\"https://cangcz.github.io/Anchor-Crafter/static/images/method.png\\",\\"https://cvmi-lab.github.io/TEXGen/figs/teaser.jpg\\",\\"https://xhuangcv.github.io/MaterialAnything/static/images/pipeline.jpg\\",\\"https://diptychprompting.github.io/DiptychPrompting_files/method.png\\",\\"https://arxiv.org/html/2411.15098v2/x1.png\\",\\"https://file.notion.so/f/f/021ded55-a224-419c-939c-70c6888912f7/bc1ef986-ba63-4f6d-86d8-b03d881a57c8/flipsketch_web.gif?table=block&id=14f5f3c4-a139-80cb-8359-d5f912a75a28&spaceId=021ded55-a224-419c-939c-70c6888912f7&expirationTimestamp=1733133600000&signature=RM1JNYDflVP19hif_TiSNCv25kA5Krb_c2lSpbOyWaA\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"ShowUI：多模态用户界面生成数据集","slug":"showui-多模态用户界面生成数据集","link":"#showui-多模态用户界面生成数据集","children":[]},{"level":2,"title":"ROICtrl：多实例可控视觉生成框架","slug":"roictrl-多实例可控视觉生成框架","link":"#roictrl-多实例可控视觉生成框架","children":[]},{"level":2,"title":"ConsisID：一致性身份保留的文本到视频生成框架","slug":"consisid-一致性身份保留的文本到视频生成框架","link":"#consisid-一致性身份保留的文本到视频生成框架","children":[]},{"level":2,"title":"SketchAgent：语言驱动的交互式素描生成","slug":"sketchagent-语言驱动的交互式素描生成","link":"#sketchagent-语言驱动的交互式素描生成","children":[]},{"level":2,"title":"AnchorCrafter：密集图像锚点生成的新方法","slug":"anchorcrafter-密集图像锚点生成的新方法","link":"#anchorcrafter-密集图像锚点生成的新方法","children":[]},{"level":2,"title":"TEXGen：基于Diffusion模型的网格纹理生成","slug":"texgen-基于diffusion模型的网格纹理生成","link":"#texgen-基于diffusion模型的网格纹理生成","children":[]},{"level":2,"title":"MaterialAnything：基于AI的自动PBR材质生成","slug":"materialanything-基于ai的自动pbr材质生成","link":"#materialanything-基于ai的自动pbr材质生成","children":[]},{"level":2,"title":"DiptychPrompting：零样本主题驱动的图像生成与编辑","slug":"diptychprompting-零样本主题驱动的图像生成与编辑","link":"#diptychprompting-零样本主题驱动的图像生成与编辑","children":[]},{"level":2,"title":"OminiControl：用于Diffusion模型的通用控制框架","slug":"ominicontrol-用于diffusion模型的通用控制框架","link":"#ominicontrol-用于diffusion模型的通用控制框架","children":[]},{"level":2,"title":"FlipSketch：将静态草图转换为文本引导的动画","slug":"flipsketch-将静态草图转换为文本引导的动画","link":"#flipsketch-将静态草图转换为文本引导的动画","children":[{"level":3,"title":"参考链接：","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":7.46,"words":2239},"filePathRelative":"zh/posts/ai-weekly/014.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/7ec38594-6a9f-469a-9515-cc2a8d4ae2f5/original=true,quality=90/42025535.jpeg\\" alt=\\"封面源自C站作者Hannibal_Lecter\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面源自C站作者Hannibal_Lecter</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周聚焦生成式AI创新：MaterialAnything自动生成PBR材质；OminiControl为Diffusion模型提供通用轻量控制框架；FlipSketch生成草图动画。这些进步展示了生成式AI的迭代效率之高，其余内容详见正文。</p>","autoDesc":true}')}}]);