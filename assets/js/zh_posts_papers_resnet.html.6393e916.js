"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[4960],{66262:(e,a)=>{a.A=(e,a)=>{const t=e.__vccOpts||e;for(const[e,n]of a)t[e]=n;return t}},53716:(e,a,t)=>{t.r(a),t.d(a,{comp:()=>s,data:()=>r});var n=t(20641);const i={},s=(0,t(66262).A)(i,[["render",function(e,a){return(0,n.uX)(),(0,n.CE)("div",null,a[0]||(a[0]=[(0,n.Fv)('<h1 id="【论文精读】resnet-deep-residual-learning-for-image-recognition" tabindex="-1"><a class="header-anchor" href="#【论文精读】resnet-deep-residual-learning-for-image-recognition"><span>【论文精读】ResNet：Deep Residual Learning for Image Recognition</span></a></h1><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>ResNet 通过残差学习成功解决超深网络的训练难题，克服梯度消失与退化问题。在 ImageNet 分类任务中以 3.57% Top-5 错误率刷新纪录，并在目标检测、分割等任务中展现卓越性能。本文解析其核心设计、实验验证及影响。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li><li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li><li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li><li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li></ol><hr><h2 id="背景与研究目标" tabindex="-1"><a class="header-anchor" href="#背景与研究目标"><span>背景与研究目标</span></a></h2><h3 id="数据集与任务背景" tabindex="-1"><a class="header-anchor" href="#数据集与任务背景"><span>数据集与任务背景</span></a></h3><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img1@main/2025/01/28/1738078788405-b5f5a142-9ca3-479d-8d34-b0b2f1f05937.png" alt="Motivation 图" tabindex="0" loading="lazy"><figcaption>Motivation 图</figcaption></figure><p>论文基于 ImageNet 数据集（1000 类，120 万训练图像）和 CIFAR-10（10 类，6 万图像）展开实验。随着 CNN 深度增加，模型在复杂任务中表现提升，但面临梯度消失和网络退化两大瓶颈。</p><h3 id="主要参考文献" tabindex="-1"><a class="header-anchor" href="#主要参考文献"><span>主要参考文献</span></a></h3><p>ResNet 的设计灵感来源于以下工作：</p><ul><li><strong>梯度消失问题</strong>：He 等人（2015）提出的权重初始化方法，以及 Ioffe 和 Szegedy（2015）的批归一化（Batch Normalization）技术，为训练深层网络奠定了基础。</li><li><strong>残差表示理论</strong>：Highway Networks 的跨层连接设计启发了残差块的构建。</li></ul><hr><h2 id="方法与创新点" tabindex="-1"><a class="header-anchor" href="#方法与创新点"><span>方法与创新点</span></a></h2><h3 id="残差学习的数学原理" tabindex="-1"><a class="header-anchor" href="#残差学习的数学原理"><span>残差学习的数学原理</span></a></h3><p>在传统神经网络中，目标是直接逼近一个映射 $H(x)$，但当网络变得很深时，梯度容易消失，导致优化困难。ResNet 通过引入<strong>残差学习</strong>，让网络学习一个残差函数 $F(x) = H(x) - x$，从而将最终映射表示为：</p><p>$$ H(x) = F(x) + x $$</p><p><strong>优化更容易的原因</strong>：</p><ol><li><strong>目标简化</strong>：如果 $H(x)$ 接近恒等映射，则 $F(x)$ 变成 0，使得优化更稳定。</li><li><strong>性能兜底</strong>：即使新增层未能学习到有效特征，网络仍可退化为浅层版本，避免性能损失。</li></ol><blockquote><p><strong>类比理解</strong>：普通深度网络像是在陡峭山坡上行走，容易摔倒（梯度消失）。ResNet 的残差连接相当于提供了“阶梯”，使得优化过程更加稳定。</p></blockquote><h3 id="网络架构设计" tabindex="-1"><a class="header-anchor" href="#网络架构设计"><span>网络架构设计</span></a></h3><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img10@main/2025/01/25/1737819865738-d6edb589-3ad4-4e73-ae2b-3dbbe280e1a7.png" alt="ResNet 不同版本架构对比" tabindex="0" loading="lazy"><figcaption>ResNet 不同版本架构对比</figcaption></figure><h4 id="基础残差块" tabindex="-1"><a class="header-anchor" href="#基础残差块"><span><strong>基础残差块</strong></span></a></h4><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img5@main/2025/01/28/1738078300833-f0c09d55-a7e2-4bed-a504-00010eef5482.png" alt="block" tabindex="0" loading="lazy"><figcaption>block</figcaption></figure><ul><li><strong>残差连接</strong>：通过恒等映射实现 $H(x) = F(x) + x$，确保深层网络至少不差于浅层网络。</li><li>适用于较浅的网络，如 ResNet-18 和 ResNet-34。</li></ul><h4 id="瓶颈残差块" tabindex="-1"><a class="header-anchor" href="#瓶颈残差块"><span><strong>瓶颈残差块</strong></span></a></h4><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img16@main/2025/01/29/1738157450347-bd138c52-c528-43b7-8030-a86ad619d57c.png" alt="瓶颈结构" tabindex="0" loading="lazy"><figcaption>瓶颈结构</figcaption></figure><ul><li>ResNet-50 及以上深层网络采用 <strong>$1 \\times 1$ → $3 \\times 3$ → $1 \\times 1$</strong> 结构： <ul><li><strong>$1 \\times 1$ 卷积</strong>：降维，减少计算量。</li><li><strong>$3 \\times 3$ 卷积</strong>：执行特征提取。</li><li><strong>$1 \\times 1$ 卷积</strong>：升维，还原通道数。</li></ul></li></ul><h3 id="残差学习与梯度传播" tabindex="-1"><a class="header-anchor" href="#残差学习与梯度传播"><span>残差学习与梯度传播</span></a></h3><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img19@main/2025/01/28/1738078459607-ced7b642-5f3f-44a6-998b-52e99af7c168.png" alt="18层和34层的plain与ResNet网络的top-1-error对比" tabindex="0" loading="lazy"><figcaption>18层和34层的plain与ResNet网络的top-1-error对比</figcaption></figure><ul><li><strong>退化问题解决</strong>：传统深层网络随着深度增加出现性能下降，而残差连接允许梯度直接回传至浅层，缓解梯度消失。</li><li><strong>恒等映射优势</strong>：即使新增层未能学习有效特征，网络仍可退化为浅层版本，避免性能损失。</li></ul><h3 id="批归一化与初始化策略" tabindex="-1"><a class="header-anchor" href="#批归一化与初始化策略"><span>批归一化与初始化策略</span></a></h3><ul><li><strong>批归一化（BN）</strong>：每个卷积层后加入 BN 层，加速训练并提升稳定性。</li><li><strong>He 初始化</strong>：配合 ReLU 激活函数，使用 He 初始化方法避免梯度爆炸。</li></ul><hr><h2 id="实验与结果分析" tabindex="-1"><a class="header-anchor" href="#实验与结果分析"><span>实验与结果分析</span></a></h2><h3 id="实验设置与数据处理" tabindex="-1"><a class="header-anchor" href="#实验设置与数据处理"><span>实验设置与数据处理</span></a></h3><ul><li><strong>硬件环境</strong>：8 GPU 并行训练，采用同步 SGD 优化器。</li><li><strong>数据增强</strong>：随机裁剪、水平翻转和 PCA 颜色扰动，提升模型鲁棒性。</li></ul><h3 id="imagenet-分类实验" tabindex="-1"><a class="header-anchor" href="#imagenet-分类实验"><span>ImageNet 分类实验</span></a></h3><h4 id="resnet-vs-plain-network" tabindex="-1"><a class="header-anchor" href="#resnet-vs-plain-network"><span><strong>ResNet vs Plain Network</strong></span></a></h4><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img10@main/2025/01/31/1738306472113-02f679ac-4478-4045-ada1-cd7042113871.png" alt="ImageNet 上的训练损失图" tabindex="0" loading="lazy"><figcaption>ImageNet 上的训练损失图</figcaption></figure><ul><li><strong>普通 CNN</strong>：34 层 plain 网络的训练误差高于 18 层，深度增加导致优化困难。</li><li><strong>ResNet</strong>：通过残差学习，34 层 ResNet 的性能优于 18 层 ResNet。</li></ul><h4 id="resnet-在-imagenet-上的训练和验证误差" tabindex="-1"><a class="header-anchor" href="#resnet-在-imagenet-上的训练和验证误差"><span><strong>ResNet 在 ImageNet 上的训练和验证误差</strong></span></a></h4><ul><li>ResNet 训练误差下降更快，最终验证误差更低。</li></ul><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img7@main/2025/01/29/1738154550256-a94ed28f-ab4b-45e9-9dcf-485fa0c54d3a.png" alt="ImageNet 测试集上排名前5的错误率" tabindex="0" loading="lazy"><figcaption>ImageNet 测试集上排名前5的错误率</figcaption></figure><ul><li>152 层 ResNet 的单模型 Top-5 验证错误率仅为 3.57%，优于 GoogLeNet <strong>近一倍</strong>。</li></ul><h3 id="cifar-10-超深网络实验" tabindex="-1"><a class="header-anchor" href="#cifar-10-超深网络实验"><span><strong>CIFAR-10 超深网络实验</strong></span></a></h3><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img5@main/2025/01/29/1738152326567-46a9467a-f0af-41d0-a1d1-e74ff9cff6fe.png" alt="CIFAR-10 上的训练损失图" tabindex="0" loading="lazy"><figcaption>CIFAR-10 上的训练损失图</figcaption></figure><h5 id="plain-网络-vs-resnet" tabindex="-1"><a class="header-anchor" href="#plain-网络-vs-resnet"><span><strong>Plain 网络 vs ResNet</strong></span></a></h5><ul><li>普通 CNN：深度增加导致训练误差和测试误差上升，表现不稳定。</li><li>ResNet：ResNet-110 训练误差低，测试误差较低，深度对其影响较小。</li></ul><h5 id="cifar-10-上超深-resnet-110-层-vs-1202-层" tabindex="-1"><a class="header-anchor" href="#cifar-10-上超深-resnet-110-层-vs-1202-层"><span><strong>CIFAR-10 上超深 ResNet（110 层 vs 1202 层）</strong></span></a></h5><ul><li>1202 层 ResNet 训练误差几乎为 0，但测试误差略高，表明可能<strong>过拟合</strong>。</li></ul><hr><h3 id="shortcut-选项实验" tabindex="-1"><a class="header-anchor" href="#shortcut-选项实验"><span><strong>Shortcut 选项实验</strong></span></a></h3><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img14@main/2025/01/29/1738153206929-42966673-df57-4ab5-8e59-c85ddde50cbe.png" alt="不同 Shortcut 选项在 ImageNet 上的错误率对比" tabindex="0" loading="lazy"><figcaption>不同 Shortcut 选项在 ImageNet 上的错误率对比</figcaption></figure><ul><li><strong>Option B（投影连接）</strong> 在计算量可接受的情况下，性能最佳。</li><li><strong>Option C（全投影连接）</strong> 效果稍好，但计算量过大。</li></ul><hr><h3 id="目标检测实验-coco-pascal-voc" tabindex="-1"><a class="header-anchor" href="#目标检测实验-coco-pascal-voc"><span><strong>目标检测实验（COCO &amp; Pascal VOC）</strong></span></a></h3><h4 id="coco-数据集上-faster-r-cnn-目标检测-map" tabindex="-1"><a class="header-anchor" href="#coco-数据集上-faster-r-cnn-目标检测-map"><span><strong>COCO 数据集上 Faster R-CNN 目标检测 mAP</strong></span></a></h4><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img7@main/2025/01/29/1738152528891-b5a21d34-fb2d-480f-af26-afa16730f7ff.png" alt="COCO 数据集上的 mAP 比较" tabindex="0" loading="lazy"><figcaption>COCO 数据集上的 mAP 比较</figcaption></figure><p>-*ResNet-101 替换 VGG-16 后，mAP 提升 5.8%。</p><h4 id="pascal-voc-数据集上-faster-r-cnn-目标检测-map" tabindex="-1"><a class="header-anchor" href="#pascal-voc-数据集上-faster-r-cnn-目标检测-map"><span><strong>PASCAL VOC 数据集上 Faster R-CNN 目标检测 mAP</strong></span></a></h4><figure><img src="https://fastly.jsdelivr.net/gh/bucketio/img5@main/2025/01/29/1738152542273-e9273207-ec14-4f19-902b-a5bfd7e40ba1.png" alt="PASCAL VOC 数据集上的 mAP 比较" tabindex="0" loading="lazy"><figcaption>PASCAL VOC 数据集上的 mAP 比较</figcaption></figure><ul><li>在 PASCAL VOC 数据集上，mAP 提升 3.5%。</li></ul><hr><h2 id="模型启发与方法延伸" tabindex="-1"><a class="header-anchor" href="#模型启发与方法延伸"><span>模型启发与方法延伸</span></a></h2><p>ResNet 的残差学习框架影响深远，催生了多个优化模型，并在不同领域得到广泛应用。</p><h3 id="深度网络的优化" tabindex="-1"><a class="header-anchor" href="#深度网络的优化"><span><strong>深度网络的优化</strong></span></a></h3><ul><li><strong>ResNeXt</strong>：通过 <strong>分组卷积</strong> 降低计算量，提高计算效率。</li><li><strong>DenseNet</strong>：采用 <strong>密集连接</strong>，增强特征复用，减少参数冗余。</li><li><strong>EfficientNet</strong>：结合 NAS（神经架构搜索）优化深度、宽度、分辨率的比例，提高计算效率。</li></ul><h3 id="残差思想的跨领域应用" tabindex="-1"><a class="header-anchor" href="#残差思想的跨领域应用"><span><strong>残差思想的跨领域应用</strong></span></a></h3><ul><li><strong>NLP</strong>：Transformer 采用 <strong>残差连接</strong> 改善梯度流动，提升训练稳定性。</li><li><strong>蛋白质结构预测</strong>：AlphaFold 2 利用 ResNet 结构建模远程依赖，提高预测精度。</li><li><strong>GAN</strong>：BigGAN 通过残差连接提升生成器的稳定性，缓解模式崩溃问题。</li></ul><h3 id="轻量化与高效网络" tabindex="-1"><a class="header-anchor" href="#轻量化与高效网络"><span><strong>轻量化与高效网络</strong></span></a></h3><ul><li><strong>MobileNetV2</strong>：使用 <strong>倒残差（Inverted Residuals）</strong> 结构，提高移动端计算效率。</li><li><strong>ShuffleNet</strong>：结合 ResNet 和分组卷积，优化计算资源分配。</li><li><strong>GhostNet</strong>：降低冗余计算，提高模型推理速度。</li></ul><hr><h2 id="结论与未来展望" tabindex="-1"><a class="header-anchor" href="#结论与未来展望"><span>结论与未来展望</span></a></h2><p>ResNet 通过残差学习成功解决了深度网络的训练瓶颈，但仍有优化空间：</p><h3 id="计算效率优化" tabindex="-1"><a class="header-anchor" href="#计算效率优化"><span><strong>计算效率优化</strong></span></a></h3><ul><li><strong>问题</strong>：深层 ResNet（如 ResNet-152）计算成本高，参数冗余。</li><li><strong>改进</strong>：ResNeXt 采用 <strong>分组卷积</strong> 降低计算量，EfficientNet 通过 NAS 进行结构优化。</li></ul><h3 id="特征复用与梯度流动" tabindex="-1"><a class="header-anchor" href="#特征复用与梯度流动"><span><strong>特征复用与梯度流动</strong></span></a></h3><ul><li><strong>问题</strong>：部分残差块学习冗余特征，网络利用率不足。</li><li><strong>改进</strong>：DenseNet 通过 <strong>跨层连接</strong> 增强特征复用，Stochastic Depth 随机丢弃残差块提升泛化能力。</li></ul><h3 id="多任务学习与模型适应性" tabindex="-1"><a class="header-anchor" href="#多任务学习与模型适应性"><span><strong>多任务学习与模型适应性</strong></span></a></h3><ul><li><strong>问题</strong>：ResNet 主要针对单任务优化，难以适应多模态任务。</li><li><strong>改进</strong>：结合 <strong>Transformer 结构</strong> 提高跨任务泛化能力，探索 <strong>动态深度网络</strong> 以提高计算效率。</li></ul><h3 id="理论研究与优化" tabindex="-1"><a class="header-anchor" href="#理论研究与优化"><span><strong>理论研究与优化</strong></span></a></h3><ul><li><strong>问题</strong>：残差学习为何有效？最优深度如何确定？退化问题的数学原理仍不明确。</li><li><strong>改进</strong>：进一步研究 <strong>梯度行为、最优深度估计</strong> 及更稳健的优化方法。</li></ul><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h3><ol><li><a href="https://arxiv.org/pdf/1512.03385" target="_blank" rel="noopener noreferrer">ResNet 论文</a></li><li><a href="https://github.com/KaimingHe/deep-residual-networks" target="_blank" rel="noopener noreferrer">ResNet 代码</a></li><li><a href="https://www.bilibili.com/video/BV1T3411u7Qt" target="_blank" rel="noopener noreferrer">ResNet 论文逐段精读</a></li><li><a href="https://zhuanlan.zhihu.com/p/496445232" target="_blank" rel="noopener noreferrer">知乎：ResNet 为何如此重要？</a></li><li><a href="https://zhouyifan.net/2022/08/09/20220807-ResNet/" target="_blank" rel="noopener noreferrer">Yifan Zhou 博客</a></li><li><a href="https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8" target="_blank" rel="noopener noreferrer">Towards Data Science 博客</a></li></ol>',88)]))}]]),r=JSON.parse('{"path":"/zh/posts/papers/resnet.html","title":"【论文精读】ResNet：Deep Residual Learning for Image Recognition","lang":"zh-CN","frontmatter":{"description":"【论文精读】ResNet：Deep Residual Learning for Image Recognition 摘要 ResNet 通过残差学习成功解决超深网络的训练难题，克服梯度消失与退化问题。在 ImageNet 分类任务中以 3.57% Top-5 错误率刷新纪录，并在目标检测、分割等任务中展现卓越性能。本文解析其核心设计、实验验证及影响。 ...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/papers/resnet.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"【论文精读】ResNet：Deep Residual Learning for Image Recognition"}],["meta",{"property":"og:description","content":"【论文精读】ResNet：Deep Residual Learning for Image Recognition 摘要 ResNet 通过残差学习成功解决超深网络的训练难题，克服梯度消失与退化问题。在 ImageNet 分类任务中以 3.57% Top-5 错误率刷新纪录，并在目标检测、分割等任务中展现卓越性能。本文解析其核心设计、实验验证及影响。 ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://fastly.jsdelivr.net/gh/bucketio/img1@main/2025/01/28/1738078788405-b5f5a142-9ca3-479d-8d34-b0b2f1f05937.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"【论文精读】ResNet：Deep Residual Learning for Image Recognition\\",\\"image\\":[\\"https://fastly.jsdelivr.net/gh/bucketio/img1@main/2025/01/28/1738078788405-b5f5a142-9ca3-479d-8d34-b0b2f1f05937.png\\",\\"https://fastly.jsdelivr.net/gh/bucketio/img10@main/2025/01/25/1737819865738-d6edb589-3ad4-4e73-ae2b-3dbbe280e1a7.png\\",\\"https://fastly.jsdelivr.net/gh/bucketio/img5@main/2025/01/28/1738078300833-f0c09d55-a7e2-4bed-a504-00010eef5482.png\\",\\"https://fastly.jsdelivr.net/gh/bucketio/img16@main/2025/01/29/1738157450347-bd138c52-c528-43b7-8030-a86ad619d57c.png\\",\\"https://fastly.jsdelivr.net/gh/bucketio/img19@main/2025/01/28/1738078459607-ced7b642-5f3f-44a6-998b-52e99af7c168.png\\",\\"https://fastly.jsdelivr.net/gh/bucketio/img10@main/2025/01/31/1738306472113-02f679ac-4478-4045-ada1-cd7042113871.png\\",\\"https://fastly.jsdelivr.net/gh/bucketio/img7@main/2025/01/29/1738154550256-a94ed28f-ab4b-45e9-9dcf-485fa0c54d3a.png\\",\\"https://fastly.jsdelivr.net/gh/bucketio/img5@main/2025/01/29/1738152326567-46a9467a-f0af-41d0-a1d1-e74ff9cff6fe.png\\",\\"https://fastly.jsdelivr.net/gh/bucketio/img14@main/2025/01/29/1738153206929-42966673-df57-4ab5-8e59-c85ddde50cbe.png\\",\\"https://fastly.jsdelivr.net/gh/bucketio/img7@main/2025/01/29/1738152528891-b5a21d34-fb2d-480f-af26-afa16730f7ff.png\\",\\"https://fastly.jsdelivr.net/gh/bucketio/img5@main/2025/01/29/1738152542273-e9273207-ec14-4f19-902b-a5bfd7e40ba1.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"背景与研究目标","slug":"背景与研究目标","link":"#背景与研究目标","children":[{"level":3,"title":"数据集与任务背景","slug":"数据集与任务背景","link":"#数据集与任务背景","children":[]},{"level":3,"title":"主要参考文献","slug":"主要参考文献","link":"#主要参考文献","children":[]}]},{"level":2,"title":"方法与创新点","slug":"方法与创新点","link":"#方法与创新点","children":[{"level":3,"title":"残差学习的数学原理","slug":"残差学习的数学原理","link":"#残差学习的数学原理","children":[]},{"level":3,"title":"网络架构设计","slug":"网络架构设计","link":"#网络架构设计","children":[]},{"level":3,"title":"残差学习与梯度传播","slug":"残差学习与梯度传播","link":"#残差学习与梯度传播","children":[]},{"level":3,"title":"批归一化与初始化策略","slug":"批归一化与初始化策略","link":"#批归一化与初始化策略","children":[]}]},{"level":2,"title":"实验与结果分析","slug":"实验与结果分析","link":"#实验与结果分析","children":[{"level":3,"title":"实验设置与数据处理","slug":"实验设置与数据处理","link":"#实验设置与数据处理","children":[]},{"level":3,"title":"ImageNet 分类实验","slug":"imagenet-分类实验","link":"#imagenet-分类实验","children":[]},{"level":3,"title":"CIFAR-10 超深网络实验","slug":"cifar-10-超深网络实验","link":"#cifar-10-超深网络实验","children":[]},{"level":3,"title":"Shortcut 选项实验","slug":"shortcut-选项实验","link":"#shortcut-选项实验","children":[]},{"level":3,"title":"目标检测实验（COCO & Pascal VOC）","slug":"目标检测实验-coco-pascal-voc","link":"#目标检测实验-coco-pascal-voc","children":[]}]},{"level":2,"title":"模型启发与方法延伸","slug":"模型启发与方法延伸","link":"#模型启发与方法延伸","children":[{"level":3,"title":"深度网络的优化","slug":"深度网络的优化","link":"#深度网络的优化","children":[]},{"level":3,"title":"残差思想的跨领域应用","slug":"残差思想的跨领域应用","link":"#残差思想的跨领域应用","children":[]},{"level":3,"title":"轻量化与高效网络","slug":"轻量化与高效网络","link":"#轻量化与高效网络","children":[]}]},{"level":2,"title":"结论与未来展望","slug":"结论与未来展望","link":"#结论与未来展望","children":[{"level":3,"title":"计算效率优化","slug":"计算效率优化","link":"#计算效率优化","children":[]},{"level":3,"title":"特征复用与梯度流动","slug":"特征复用与梯度流动","link":"#特征复用与梯度流动","children":[]},{"level":3,"title":"多任务学习与模型适应性","slug":"多任务学习与模型适应性","link":"#多任务学习与模型适应性","children":[]},{"level":3,"title":"理论研究与优化","slug":"理论研究与优化","link":"#理论研究与优化","children":[]},{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":6.51,"words":1953},"filePathRelative":"zh/posts/papers/resnet.md","excerpt":"\\n<h2>摘要</h2>\\n<p>ResNet 通过残差学习成功解决超深网络的训练难题，克服梯度消失与退化问题。在 ImageNet 分类任务中以 3.57% Top-5 错误率刷新纪录，并在目标检测、分割等任务中展现卓越性能。本文解析其核心设计、实验验证及影响。</p>\\n<hr>\\n<h2>目录</h2>\\n<ol>\\n<li><a href=\\"#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87\\">背景与研究目标</a></li>\\n<li><a href=\\"#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9\\">方法与创新点</a></li>\\n<li><a href=\\"#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90\\">实验与结果分析</a></li>\\n<li><a href=\\"#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8\\">模型启发与方法延伸</a></li>\\n<li><a href=\\"#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B\\">结论与未来展望</a></li>\\n</ol>","autoDesc":true}')}}]);