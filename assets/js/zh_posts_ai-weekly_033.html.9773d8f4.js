"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[3842],{6262:(t,a)=>{a.A=(t,a)=>{const r=t.__vccOpts||t;for(const[t,e]of a)r[t]=e;return r}},9444:(t,a,r)=>{r.r(a),r.d(a,{comp:()=>n,data:()=>o});var e=r(641);const i={},n=(0,r(6262).A)(i,[["render",function(t,a){return(0,e.uX)(),(0,e.CE)("div",null,a[0]||(a[0]=[(0,e.Fv)('<h1 id="omnicaptioner统一视觉描述-uno多主体定制-spf-portrait消除污染提升肖像定制【ai周报】" tabindex="-1"><a class="header-anchor" href="#omnicaptioner统一视觉描述-uno多主体定制-spf-portrait消除污染提升肖像定制【ai周报】"><span>OmniCaptioner统一视觉描述 | UNO多主体定制 | SPF-Portrait消除污染提升肖像定制【AI周报】</span></a></h1><figure><img src="https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/7ca09232-41e5-47f6-a2dc-39eb85b35853/original=true,quality=90/00883-3137192565-masterpiece" alt="封面源自C站作者Koal2" tabindex="0" loading="lazy"><figcaption>封面源自C站作者Koal2</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：OmniCaptioner实现跨视觉域语言描述；UNO统一个性化定制多主体生成；SPF-Portrait解决语义污染；FantasyTalking生成音频驱动数字人；SmolVLM2发布轻量级视频理解模型。其余详见正文。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#omnicaptioner%E7%BB%9F%E4%B8%80%E5%A4%9A%E8%A7%86%E8%A7%89%E5%9F%9F%E6%8F%8F%E8%BF%B0%E7%94%9F%E6%88%90%E7%9A%84%E5%88%9B%E6%96%B0%E6%A1%86%E6%9E%B6">OmniCaptioner：统一多视觉域描述生成的创新框架</a></li><li><a href="#uno%E7%BB%9F%E4%B8%80%E5%A4%9A%E4%B8%BB%E4%BD%93%E4%B8%AA%E6%80%A7%E5%8C%96%E5%AE%9A%E5%88%B6%E6%96%B9%E6%B3%95">UNO：统一多主体个性化定制方法</a></li><li><a href="#omnisvg%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%A4%9A%E6%A8%A1%E6%80%81%E7%9F%A2%E9%87%8F%E5%9B%BE%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">OmniSVG：端到端多模态矢量图生成模型</a></li><li><a href="#visualcloze%E7%BB%9F%E4%B8%80%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%A7%86%E8%A7%89%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6">VisualCloze：统一图像生成任务的视觉上下文学习框架</a></li><li><a href="#spf-portrait%E6%B6%88%E9%99%A4%E8%AF%AD%E4%B9%89%E6%B1%A1%E6%9F%93%E7%9A%84%E8%82%96%E5%83%8F%E5%AE%9A%E5%88%B6%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95">SPF-Portrait：消除语义污染的肖像定制微调方法</a></li><li><a href="#fantasytalking%E9%9F%B3%E9%A2%91%E9%A9%B1%E5%8A%A8%E7%9A%84%E9%AB%98%E4%BF%9D%E7%9C%9F%E6%95%B0%E5%AD%97%E4%BA%BA%E7%94%9F%E6%88%90%E6%A1%86%E6%9E%B6">FantasyTalking：音频驱动的高保真数字人生成框架</a></li><li><a href="#smolvlm2%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E6%A8%A1%E5%9E%8B%E9%80%82%E9%85%8D%E5%A4%9A%E8%AE%BE%E5%A4%87%E9%83%A8%E7%BD%B2">SmolVLM2：轻量级视频理解模型，适配多设备部署</a></li></ol><hr><h2 id="omnicaptioner-统一多视觉域描述生成的创新框架" tabindex="-1"><a class="header-anchor" href="#omnicaptioner-统一多视觉域描述生成的创新框架"><span>OmniCaptioner：统一多视觉域描述生成的创新框架</span></a></h2><figure><img src="https://alpha-innovator.github.io/OmniCaptioner-project-page/static/images/Figure1.png" alt="OmniCaptioner Teaser 图" tabindex="0" loading="lazy"><figcaption>OmniCaptioner Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>OmniCaptioner</strong> 是由 <strong>上海人工智能实验室、复旦大学、中国科技大学</strong> 等机构提出的创新视觉描述生成框架，旨在统一处理自然图像、结构化图像（如表格、图表）和视觉文本图像（如UI、海报）等多种视觉域。其训练流程包括构建多样化的描述数据集，采用两阶段的描述生成流程（初始描述生成与扩展），并通过统一的预训练机制提升跨领域的视觉理解能力。在推理阶段，OmniCaptioner 首先将图像转化为语义结构化的语言描述，然后利用大语言模型（如 DeepSeek-R1、Qwen2.5）进行任务无关的语言推理，实现感知与推理的解耦，增强了模型的泛化性和适用性。</p><p><strong>标签</strong>：#视觉描述生成 #多视觉域 #统一预训练 #语言推理 #多模态理解</p><hr><h2 id="uno-统一多主体个性化定制方法" tabindex="-1"><a class="header-anchor" href="#uno-统一多主体个性化定制方法"><span>UNO：统一多主体个性化定制方法</span></a></h2><figure><img src="https://arxiv.org/html/2504.02160v1/x1.png" alt="UNO Teaser 图" tabindex="0" loading="lazy"><figcaption>UNO Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>UNO</strong> 是由 <strong>字节跳动智能创作团队</strong> 提出的一种统一个性化定制方法，旨在解决现有图像生成中单一主体定制的局限性。UNO 引入了渐进式跨模态对齐（Progressive Cross-Modal Alignment）和通用旋转位置编码（Universal Rotary Position Embedding, UnoPE），通过两阶段训练流程，从单主体到多主体逐步扩展，提升了模型在多主体生成任务中的一致性和可控性。实验结果表明，UNO 在保持高一致性的同时，实现了从单主体到多主体的统一定制能力。</p><p><strong>标签</strong>：#图像生成 #个性化定制 #多主体生成 #跨模态对齐 #位置编码</p><hr><h2 id="omnisvg-端到端多模态矢量图生成模型" tabindex="-1"><a class="header-anchor" href="#omnisvg-端到端多模态矢量图生成模型"><span>OmniSVG：端到端多模态矢量图生成模型</span></a></h2><figure><img src="https://omnisvg.github.io/assets/omnisvg-pipeline.jpg" alt="OmniSVG Pipeline 图" tabindex="0" loading="lazy"><figcaption>OmniSVG Pipeline 图</figcaption></figure><p><strong>概要</strong>：<strong>OmniSVG</strong> 是由 <strong>复旦大学</strong> 与 <strong>阶跃星辰（StepFun）</strong> 联合提出的端到端多模态 SVG 生成模型，支持从简单图标到复杂动漫角色的高质量矢量图生成。该模型基于预训练视觉语言模型 Qwen2.5-VL，并集成了 SVG 标记化器，将文本和图像输入标记为前缀标记，同时将 SVG 命令和坐标参数化为离散标记，从而解耦结构逻辑与底层几何，实现高效训练和生成。OmniSVG 支持三种生成模式：文本生成 SVG、图像转 SVG、角色参考 SVG 生成。为支持训练，研究团队构建了包含两百万个带注释 SVG 的多模态数据集 MMSVG-2M，并制定了标准化评估协议。实验结果表明，OmniSVG 在生成质量和多样性方面优于现有方法，展示了其在专业 SVG 设计工作流程中的集成潜力。</p><p><strong>标签</strong>：#SVG生成 #多模态学习 #视觉语言模型 #矢量图形 #图像生成</p><hr><h2 id="visualcloze-统一图像生成任务的视觉上下文学习框架" tabindex="-1"><a class="header-anchor" href="#visualcloze-统一图像生成任务的视觉上下文学习框架"><span>VisualCloze：统一图像生成任务的视觉上下文学习框架</span></a></h2><figure><img src="https://arxiv.org/html/2504.07960v1/x1.png" alt="VisualCloze Teaser 图" tabindex="0" loading="lazy"><figcaption>VisualCloze Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>VisualCloze</strong> 是由 <strong>南开大学、上海人工智能实验室、北京邮电大学</strong> 等机构提出的一种通用图像生成框架，旨在通过视觉上下文学习（Visual In-Context Learning）实现多任务统一与泛化。该方法通过“视觉示例”而非语言指令引导模型理解任务，避免了语言歧义问题，并提升泛化能力。研究团队构建了图结构数据集 Graph200K，涵盖多种任务形式，增强任务密度与可迁移性。VisualCloze 基于图像填充模型 FLUX.1，无需修改架构即可支持统一建模与反向生成，展示了在未见任务上的强泛化能力。</p><p><strong>标签</strong>：#图像生成 #视觉上下文学习 #多任务泛化 #图结构数据 #视觉指令学习</p><hr><h2 id="spf-portrait-消除语义污染的肖像定制微调方法" tabindex="-1"><a class="header-anchor" href="#spf-portrait-消除语义污染的肖像定制微调方法"><span>SPF-Portrait：消除语义污染的肖像定制微调方法</span></a></h2><figure><img src="https://spf-portrait.github.io/SPF-Portrait/static/github_image/teaser.jpg" alt="SPF-Portrait Teaser 图" tabindex="0" loading="lazy"><figcaption>SPF-Portrait Teaser 图</figcaption></figure><p><strong>概要</strong>：<strong>SPF-Portrait</strong> 是由 <strong>深圳大学、清华大学与快手 KwaiVGI</strong> 联合提出的一种创新方法，旨在解决文本驱动肖像定制中常见的语义污染问题。该方法引入了双路径对比学习框架，将原始模型作为参考路径，通过对比学习确保目标属性的适应性，同时保持其他无关属性与原始肖像的一致性。此外，SPF-Portrait 设计了语义感知精细控制图，以空间方式指导对比路径之间的对齐过程，有效保留原模型性能，避免过度对齐。实验结果表明，SPF-Portrait 在实现目标属性定制的同时，显著减少了语义污染，提升了模型的泛化能力和稳定性。</p><p><strong>标签</strong>：#肖像定制 #语义污染 #对比学习 #图像生成 #模型微调</p><hr><h2 id="fantasytalking-音频驱动的高保真数字人生成框架" tabindex="-1"><a class="header-anchor" href="#fantasytalking-音频驱动的高保真数字人生成框架"><span>FantasyTalking：音频驱动的高保真数字人生成框架</span></a></h2><figure><img src="https://fantasy-amap.github.io/fantasy-talking/assets/幻灯片1.png" alt="FantasyTalking Overview 图" tabindex="0" loading="lazy"><figcaption>FantasyTalking Overview 图</figcaption></figure><p><strong>概要</strong>：<strong>FantasyTalking</strong> 是由 <strong>阿里巴巴 AMAP 团队</strong> 与 <strong>北京邮电大学</strong> 联合提出的数字人生成框架，基于预训练的视频扩散变换器模型。该方法采用双阶段音频-视觉对齐策略：第一阶段在剪辑级别对齐音频驱动的全局动态，包括参考肖像、上下文对象和背景；第二阶段使用唇部跟踪掩码在帧级别精细化唇部运动，确保与音频信号的精确同步。为保持身份一致性，框架引入面部专注的交叉注意力模块，有效维护视频中的面部特征一致性。此外，集成的运动强度调制模块允许对表情和身体动作强度进行显式控制，实现超越唇部运动的可控肖像动作生成。实验结果表明，FantasyTalking 在真实感、一致性、动作强度和身份保持方面表现优异。</p><p><strong>标签</strong>：#数字人 #视频生成 #音频驱动 #身份保持 #动作控制</p><hr><h2 id="smolvlm2-轻量级视频理解模型-适配多设备部署" tabindex="-1"><a class="header-anchor" href="#smolvlm2-轻量级视频理解模型-适配多设备部署"><span>SmolVLM2：轻量级视频理解模型，适配多设备部署</span></a></h2><figure><img src="https://arxiv.org/html/2504.05299v1/x20.png" alt="SmolVLM2 Architecture 图" tabindex="0" loading="lazy"><figcaption>SmolVLM2 Architecture 图</figcaption></figure><p><strong>概要</strong>：<strong>SmolVLM2</strong> 是由 <strong>Hugging Face</strong> 推出的轻量级多模态模型，旨在实现高效的视频理解能力。该模型提供三种参数规模（2.2B、500M、256M），支持图像、视频和文本输入，输出文本结果。SmolVLM2 采用优化的视觉编码器和语言模型架构，能够在低资源设备上运行，适用于从手机到服务器的多种部署场景。实验结果显示，SmolVLM2 在保持较小模型规模的同时，具备强大的视觉理解能力，适合广泛的多模态应用。</p><p><strong>标签</strong>：#视频理解 #多模态模型 #轻量级模型 #HuggingFace #SmolVLM2</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://alpha-innovator.github.io/OmniCaptioner-project-page/" target="_blank" rel="noopener noreferrer">OmniCaptioner 项目主页</a></li><li><a href="https://github.com/Alpha-Innovator/OmniCaptioner" target="_blank" rel="noopener noreferrer">OmniCaptioner GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2504.07089" target="_blank" rel="noopener noreferrer">OmniCaptioner 论文链接</a></li><li><a href="https://bytedance.github.io/UNO/" target="_blank" rel="noopener noreferrer">UNO 项目主页</a></li><li><a href="https://github.com/bytedance/UNO" target="_blank" rel="noopener noreferrer">UNO GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2504.02160v1" target="_blank" rel="noopener noreferrer">UNO 论文链接</a></li><li><a href="https://omnisvg.github.io/" target="_blank" rel="noopener noreferrer">OmniSVG 项目主页</a></li><li><a href="https://github.com/OmniSVG/OmniSVG" target="_blank" rel="noopener noreferrer">OmniSVG GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2504.06263v1" target="_blank" rel="noopener noreferrer">OmniSVG 论文链接</a></li><li><a href="https://visualcloze.github.io/" target="_blank" rel="noopener noreferrer">VisualCloze 项目主页</a></li><li><a href="https://github.com/lzyhha/VisualCloze" target="_blank" rel="noopener noreferrer">VisualCloze GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2504.07960v1" target="_blank" rel="noopener noreferrer">VisualCloze 论文链接</a></li><li><a href="https://spf-portrait.github.io/SPF-Portrait/" target="_blank" rel="noopener noreferrer">SPF-Portrait 项目主页</a></li><li><a href="https://github.com/KwaiVGI/SPF-Portrait" target="_blank" rel="noopener noreferrer">SPF-Portrait GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2504.00396v2" target="_blank" rel="noopener noreferrer">SPF-Portrait 论文链接</a></li><li><a href="https://fantasy-amap.github.io/fantasy-talking/" target="_blank" rel="noopener noreferrer">FantasyTalking 项目主页</a></li><li><a href="https://github.com/Fantasy-AMAP/fantasy-talking" target="_blank" rel="noopener noreferrer">FantasyTalking GitHub 仓库</a></li><li><a href="https://huggingface.co/collections/HuggingFaceTB/smolvlm2-smallest-video-lm-ever-67ab6b5e84bf8aaa60cb17c7" target="_blank" rel="noopener noreferrer">SmolVLM2 模型集合</a></li><li><a href="https://github.com/huggingface/smollm" target="_blank" rel="noopener noreferrer">SmolVLM2 GitHub 仓库</a></li><li><a href="https://arxiv.org/html/2504.05299" target="_blank" rel="noopener noreferrer">SmolVLM2 论文链接</a></li></ol>',45)]))}]]),o=JSON.parse('{"path":"/zh/posts/ai-weekly/033.html","title":"OmniCaptioner统一视觉描述 | UNO多主体定制 | SPF-Portrait消除污染提升肖像定制【AI周报】","lang":"zh-CN","frontmatter":{"description":"OmniCaptioner统一视觉描述 | UNO多主体定制 | SPF-Portrait消除污染提升肖像定制【AI周报】 封面源自C站作者Koal2封面源自C站作者Koal2 摘要 本周亮点：OmniCaptioner实现跨视觉域语言描述；UNO统一个性化定制多主体生成；SPF-Portrait解决语义污染；FantasyTalking生成音频驱动数...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/ai-weekly/033.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"OmniCaptioner统一视觉描述 | UNO多主体定制 | SPF-Portrait消除污染提升肖像定制【AI周报】"}],["meta",{"property":"og:description","content":"OmniCaptioner统一视觉描述 | UNO多主体定制 | SPF-Portrait消除污染提升肖像定制【AI周报】 封面源自C站作者Koal2封面源自C站作者Koal2 摘要 本周亮点：OmniCaptioner实现跨视觉域语言描述；UNO统一个性化定制多主体生成；SPF-Portrait解决语义污染；FantasyTalking生成音频驱动数..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/7ca09232-41e5-47f6-a2dc-39eb85b35853/original=true,quality=90/00883-3137192565-masterpiece"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"OmniCaptioner统一视觉描述 | UNO多主体定制 | SPF-Portrait消除污染提升肖像定制【AI周报】\\",\\"image\\":[\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/7ca09232-41e5-47f6-a2dc-39eb85b35853/original=true,quality=90/00883-3137192565-masterpiece\\",\\"https://alpha-innovator.github.io/OmniCaptioner-project-page/static/images/Figure1.png\\",\\"https://arxiv.org/html/2504.02160v1/x1.png\\",\\"https://omnisvg.github.io/assets/omnisvg-pipeline.jpg\\",\\"https://arxiv.org/html/2504.07960v1/x1.png\\",\\"https://spf-portrait.github.io/SPF-Portrait/static/github_image/teaser.jpg\\",\\"https://fantasy-amap.github.io/fantasy-talking/assets/%E5%B9%BB%E7%81%AF%E7%89%871.png\\",\\"https://arxiv.org/html/2504.05299v1/x20.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"OmniCaptioner：统一多视觉域描述生成的创新框架","slug":"omnicaptioner-统一多视觉域描述生成的创新框架","link":"#omnicaptioner-统一多视觉域描述生成的创新框架","children":[]},{"level":2,"title":"UNO：统一多主体个性化定制方法","slug":"uno-统一多主体个性化定制方法","link":"#uno-统一多主体个性化定制方法","children":[]},{"level":2,"title":"OmniSVG：端到端多模态矢量图生成模型","slug":"omnisvg-端到端多模态矢量图生成模型","link":"#omnisvg-端到端多模态矢量图生成模型","children":[]},{"level":2,"title":"VisualCloze：统一图像生成任务的视觉上下文学习框架","slug":"visualcloze-统一图像生成任务的视觉上下文学习框架","link":"#visualcloze-统一图像生成任务的视觉上下文学习框架","children":[]},{"level":2,"title":"SPF-Portrait：消除语义污染的肖像定制微调方法","slug":"spf-portrait-消除语义污染的肖像定制微调方法","link":"#spf-portrait-消除语义污染的肖像定制微调方法","children":[]},{"level":2,"title":"FantasyTalking：音频驱动的高保真数字人生成框架","slug":"fantasytalking-音频驱动的高保真数字人生成框架","link":"#fantasytalking-音频驱动的高保真数字人生成框架","children":[]},{"level":2,"title":"SmolVLM2：轻量级视频理解模型，适配多设备部署","slug":"smolvlm2-轻量级视频理解模型-适配多设备部署","link":"#smolvlm2-轻量级视频理解模型-适配多设备部署","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":7.48,"words":2245},"filePathRelative":"zh/posts/ai-weekly/033.md","excerpt":"\\n<figure><img src=\\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/7ca09232-41e5-47f6-a2dc-39eb85b35853/original=true,quality=90/00883-3137192565-masterpiece\\" alt=\\"封面源自C站作者Koal2\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面源自C站作者Koal2</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：OmniCaptioner实现跨视觉域语言描述；UNO统一个性化定制多主体生成；SPF-Portrait解决语义污染；FantasyTalking生成音频驱动数字人；SmolVLM2发布轻量级视频理解模型。其余详见正文。</p>","autoDesc":true}')}}]);