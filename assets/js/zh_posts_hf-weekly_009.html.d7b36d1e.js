"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[6291],{66262:(e,a)=>{a.A=(e,a)=>{const n=e.__vccOpts||e;for(const[e,t]of a)n[e]=t;return n}},99459:(e,a,n)=>{n.r(a),n.d(a,{comp:()=>i,data:()=>s});var t=n(20641);const r={},i=(0,n(66262).A)(r,[["render",function(e,a){return(0,t.uX)(),(0,t.CE)("div",null,a[0]||(a[0]=[(0,t.Fv)('<h1 id="qwen图像编辑模型-deepseek-v3-1混合思维-字节豆包seed-oss开源【hf周报】" tabindex="-1"><a class="header-anchor" href="#qwen图像编辑模型-deepseek-v3-1混合思维-字节豆包seed-oss开源【hf周报】"><span>Qwen图像编辑模型|DeepSeek-V3.1混合思维|字节豆包Seed-OSS开源【HF周报】</span></a></h1><figure><img src="/assets/images/placeholder.png" alt="封面图" tabindex="0" loading="lazy"><figcaption>封面图</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本周亮点：Qwen推出专业图像编辑模型，支持语义与外观双重编辑；DeepSeek升级V3.1版本，首次实现思维与非思维模式混合；字节跳动开源Seed-OSS-36B，具备灵活思维预算控制。详细内容见下文，相关参考链接请见文末。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#qwen-image-edit%E4%B8%93%E4%B8%9A%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">Qwen-Image-Edit：专业图像编辑生成模型</a></li><li><a href="#deepseek-v31%E6%B7%B7%E5%90%88%E6%80%9D%E7%BB%B4%E6%A8%A1%E5%BC%8F%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B">DeepSeek-V3.1：混合思维模式语言模型</a></li><li><a href="#seed-oss-36b-instruct%E7%81%B5%E6%B4%BB%E6%80%9D%E7%BB%B4%E9%A2%84%E7%AE%97%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%9E%8B">Seed-OSS-36B-Instruct：灵活思维预算控制模型</a></li><li><a href="#omnineural-4b%E9%A6%96%E6%AC%BEnpu%E6%84%9F%E7%9F%A5%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B">OmniNeural-4B：首款NPU感知多模态模型</a></li><li><a href="#matrix-game-20%E5%AE%9E%E6%97%B6%E4%BA%A4%E4%BA%92%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B">Matrix-Game-2.0：实时交互世界模型</a></li><li><a href="#hdm-xut-340m-anime%E8%BD%BB%E9%87%8F%E7%BA%A7%E5%8A%A8%E6%BC%AB%E9%A3%8E%E6%A0%BC%E7%94%9F%E6%88%90">HDM-xut-340M-anime：轻量级动漫风格生成</a></li><li><a href="#dinov3-vit-7b%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B">DINOv3-ViT-7B：大规模视觉基础模型</a></li><li><a href="#nextstep-1-large%E8%BF%9E%E7%BB%AD%E6%A0%87%E8%AE%B0%E8%87%AA%E5%9B%9E%E5%BD%92%E7%94%9F%E6%88%90">NextStep-1-Large：连续标记自回归生成</a></li></ol><hr><h2 id="qwen-image-edit-专业图像编辑生成模型" tabindex="-1"><a class="header-anchor" href="#qwen-image-edit-专业图像编辑生成模型"><span>Qwen-Image-Edit：专业图像编辑生成模型</span></a></h2><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_homepage.jpg" alt="Qwen-Image-Edit Demo 图" tabindex="0" loading="lazy"><figcaption>Qwen-Image-Edit Demo 图</figcaption></figure><p><strong>概要</strong>：阿里云Qwen团队发布Qwen-Image-Edit图像编辑模型，基于20B参数的Qwen-Image模型构建，成功将其独特的文本渲染能力扩展到图像编辑任务。该模型同时支持语义编辑（如IP创作、对象旋转、风格转换）和外观编辑（如添加、删除或修改特定元素），并具备精确的中英文双语文本编辑能力，能够直接对图像中的文字进行添加、删除和修改，同时保持原有字体、大小和样式。</p><p><strong>标签</strong>：#Qwen #图像编辑 #文本渲染 #多语言支持 #语义编辑</p><hr><h2 id="deepseek-v3-1-混合思维模式语言模型" tabindex="-1"><a class="header-anchor" href="#deepseek-v3-1-混合思维模式语言模型"><span>DeepSeek-V3.1：混合思维模式语言模型</span></a></h2><figure><img src="https://cdn.deepseek.com/api-docs/v3.1_benchmark_1.webp" alt="DeepSeek‑V3.1 Benchmark 图" tabindex="0" loading="lazy"><figcaption>DeepSeek‑V3.1 Benchmark 图</figcaption></figure><p><strong>概要</strong>：DeepSeek发布V3.1版本，这是一个支持思维和非思维两种模式的混合模型，具备671B参数和128K上下文长度。该版本基于V3.1-Base构建，通过两阶段长上下文扩展方法，32K扩展阶段增加10倍至630B标记，128K扩展阶段扩展3.3倍至209B标记。与前版本相比带来多项改进：通过改变聊天模板实现一个模型同时支持两种模式，通过后训练优化显著提升工具调用和智能体任务性能，并在保持与DeepSeek-R1-0528相当答案质量的同时实现更快的思维效率。模型采用UE8M0 FP8规模数据格式训练，确保与微缩放数据格式的兼容性。</p><p><strong>标签</strong>：#DeepSeek #混合思维模式 #工具调用优化 #671B参数 #长上下文扩展</p><hr><h2 id="seed-oss-36b-instruct-灵活思维预算控制模型" tabindex="-1"><a class="header-anchor" href="#seed-oss-36b-instruct-灵活思维预算控制模型"><span>Seed-OSS-36B-Instruct：灵活思维预算控制模型</span></a></h2><p><strong>概要</strong>：字节跳动Seed团队发布Seed-OSS开源模型系列，具备36B参数和512K原生长上下文支持。该模型的突出特性是灵活的思维预算控制，允许用户根据需要动态调整推理长度，从而在实际应用场景中提高推理效率。模型在推理、智能体和通用能力方面表现优异，支持工具使用和问题解决等智能体任务，同时提供对研究友好的多样化版本选择。</p><p><strong>标签</strong>：#字节跳动 #Seed-OSS #思维预算控制 #36B参数 #智能体任务</p><hr><h2 id="omnineural-4b-首款npu感知多模态模型" tabindex="-1"><a class="header-anchor" href="#omnineural-4b-首款npu感知多模态模型"><span>OmniNeural-4B：首款NPU感知多模态模型</span></a></h2><figure><img src="https://cdn-uploads.huggingface.co/production/uploads/6618e0424dbef6bd3c72f89a/oINYbgXILJgTuKxKc1aO_.png" alt="OmniNeural Architecture 图" tabindex="0" loading="lazy"><figcaption>OmniNeural Architecture 图</figcaption></figure><p><strong>概要</strong>：Nexa AI发布全球首款专为神经处理单元（NPU）设计的多模态模型OmniNeural，原生理解文本、图像和音频，可在PC、移动设备、汽车、物联网和机器人等平台运行。该模型采用NPU友好的架构设计，使用ReLU操作、稀疏张量、卷积层和静态图执行，相比非NPU感知模型提速20%，音频处理速度提升9倍，图像处理速度提升3.5倍。</p><p><strong>标签</strong>：#Nexa AI #NPU优化 #多模态智能 #硬件感知架构 #边缘计算</p><hr><h2 id="matrix-game-2-0-实时交互世界模型" tabindex="-1"><a class="header-anchor" href="#matrix-game-2-0-实时交互世界模型"><span>Matrix-Game-2.0：实时交互世界模型</span></a></h2><figure><img src="https://huggingface.co/Skywork/Matrix-Game-2.0/resolve/main/architecture.png" alt="Matrix-Game Architecture 图" tabindex="0" loading="lazy"><figcaption>Matrix-Game Architecture 图</figcaption></figure><p><strong>概要</strong>：Skywork发布Matrix-Game-2.0交互式世界模型，具备1.8B参数，通过少步自回归扩散技术实现实时长视频生成。该模型支持25 FPS流式视频合成，可生成分钟级高保真视频，并具备精确的动作注入功能，通过鼠标/键盘到帧的模块实现帧级控制和动态响应。系统还包含针对虚幻引擎和GTA5的大规模交互数据管道，生成约1200小时的高质量交互视频数据。</p><p><strong>标签</strong>：#Skywork #交互世界模型 #实时视频生成 #动作控制 #游戏AI</p><hr><h2 id="hdm-xut-340m-anime-轻量级动漫风格生成" tabindex="-1"><a class="header-anchor" href="#hdm-xut-340m-anime-轻量级动漫风格生成"><span>HDM-xut-340M-anime：轻量级动漫风格生成</span></a></h2><figure><img src="https://huggingface.co/KBlueLeaf/HDM-xut-340M-anime/resolve/main/images/thumbnail.webp" alt="HDM Teaser 图" tabindex="0" loading="lazy"><figcaption>HDM Teaser 图</figcaption></figure><p><strong>概要</strong>：琥珀青叶（KBlueLeaf）发布HDM-xut-340M-anime，这是世界上最小、最便宜的动漫风格文本到图像基础模型。该项目探索了&quot;在家预训练T2I模型&quot;的专门训练配方，引入了为多模态生成模型设计的新型变换器骨干&quot;XUT&quot;（Cross-U-Transformer）。通过最小化架构设计和TREAD技术，该模型在最多花费650美元的计算资源下实现可用性能，支持1024x1024或更高分辨率生成。</p><p><strong>标签</strong>：#KBlueLeaf #HDM #XUT架构 #动漫生成 #低成本训练</p><hr><h2 id="dinov3-vit-7b-大规模视觉基础模型" tabindex="-1"><a class="header-anchor" href="#dinov3-vit-7b-大规模视觉基础模型"><span>DINOv3-ViT-7B：大规模视觉基础模型</span></a></h2><figure><img src="https://scontent.fyyc6-1.fna.fbcdn.net/v/t39.2365-6/533218684_738909568994563_5324898431385079354_n.png?_nc_cat=103&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=BId3_-q_AW4Q7kNvwGNcw1a&amp;_nc_oc=AdlWr0DN3ri06Diz0d2DqsXuZ93A5ewj_FoQ6rqpsDQf5oF7qBIMHayVq8A_8ChFmcg&amp;_nc_zt=14&amp;_nc_ht=scontent.fyyc6-1.fna&amp;_nc_gid=QA94fzBTpvuEYJWsKiLvpQ&amp;oh=00_AfWAvd1ONy4dnleyvfclhEOqxzV1wEetOmdWbx60j-k1Bw&amp;oe=68C56861" alt="DINOv3 Benchmark 图" tabindex="0" loading="lazy"><figcaption>DINOv3 Benchmark 图</figcaption></figure><p><strong>概要</strong>：Meta AI发布DINOv3视觉基础模型家族，其中ViT-7B模型具备67亿参数，在广泛的视觉任务中无需微调即可超越专门的最先进技术。该模型在LVD-1689M数据集（16.89亿图像）上训练，采用DINO自蒸馏损失、iBOT掩码图像建模损失、KoLeo正则化等先进训练技术，能够产生高质量的密集特征，在各种视觉任务上实现优异性能。</p><p><strong>标签</strong>：#Meta AI #DINOv3 #视觉基础模型 #自监督学习 #67亿参数</p><hr><h2 id="nextstep-1-large-连续标记自回归生成" tabindex="-1"><a class="header-anchor" href="#nextstep-1-large-连续标记自回归生成"><span>NextStep-1-Large：连续标记自回归生成</span></a></h2><figure><img src="https://huggingface.co/stepfun-ai/NextStep-1-Large/resolve/main/assets/teaser.jpg" alt="NextStep-1 Teaser 图" tabindex="0" loading="lazy"><figcaption>NextStep-1 Teaser 图</figcaption></figure><p><strong>概要</strong>：阶跃星辰（StepFun）发布NextStep-1-Large，这是一个14B参数的自回归模型，配备157M流匹配头，在离散文本标记和连续图像标记上进行下一个标记预测目标训练。该模型在文本到图像生成任务中为自回归模型实现了最先进的性能，在高保真图像合成方面展现出强大能力，代表了大规模连续标记自回归图像生成的重要进展。</p><p><strong>标签</strong>：#StepFun #NextStep #自回归生成 #连续标记 #流匹配技术</p><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://huggingface.co/Qwen/Qwen-Image-Edit" target="_blank" rel="noopener noreferrer">Qwen-Image-Edit</a></li><li><a href="https://arxiv.org/abs/2508.02324" target="_blank" rel="noopener noreferrer">Qwen-Image-Edit 论文</a></li><li><a href="https://qwenlm.github.io/blog/qwen-image-edit/" target="_blank" rel="noopener noreferrer">Qwen-Image-Edit 博客</a></li><li><a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1" target="_blank" rel="noopener noreferrer">DeepSeek-V3.1</a></li><li><a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base" target="_blank" rel="noopener noreferrer">DeepSeek-V3.1-Base</a></li><li><a href="https://arxiv.org/abs/2412.19437" target="_blank" rel="noopener noreferrer">DeepSeek-V3 技术报告</a></li><li><a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct" target="_blank" rel="noopener noreferrer">Seed-OSS-36B-Instruct</a></li><li><a href="https://huggingface.co/NexaAI/OmniNeural-4B" target="_blank" rel="noopener noreferrer">OmniNeural-4B</a></li><li><a href="https://huggingface.co/Skywork/Matrix-Game-2.0" target="_blank" rel="noopener noreferrer">Matrix-Game-2.0</a></li><li><a href="https://arxiv.org/abs/2508.13009" target="_blank" rel="noopener noreferrer">Matrix-Game-2.0 论文</a></li><li><a href="https://huggingface.co/KBlueLeaf/HDM-xut-340M-anime" target="_blank" rel="noopener noreferrer">HDM-xut-340M-anime</a></li><li><a href="https://arxiv.org/abs/2501.04765" target="_blank" rel="noopener noreferrer">HDM-xut-340M-anime 技术报告</a></li><li><a href="https://huggingface.co/facebook/dinov3-vit7b16-pretrain-lvd1689m" target="_blank" rel="noopener noreferrer">DINOv3-ViT-7B</a></li><li><a href="https://arxiv.org/abs/2508.10104" target="_blank" rel="noopener noreferrer">DINOv3 论文</a></li><li><a href="https://huggingface.co/stepfun-ai/NextStep-1-Large" target="_blank" rel="noopener noreferrer">NextStep-1-Large</a></li><li><a href="https://arxiv.org/abs/2508.10711" target="_blank" rel="noopener noreferrer">NextStep-1 论文</a></li></ol>',49)]))}]]),s=JSON.parse('{"path":"/zh/posts/hf-weekly/009.html","title":"Qwen图像编辑模型|DeepSeek-V3.1混合思维|字节豆包Seed-OSS开源【HF周报】","lang":"zh-CN","frontmatter":{"description":"Qwen图像编辑模型|DeepSeek-V3.1混合思维|字节豆包Seed-OSS开源【HF周报】 封面图封面图 摘要 本周亮点：Qwen推出专业图像编辑模型，支持语义与外观双重编辑；DeepSeek升级V3.1版本，首次实现思维与非思维模式混合；字节跳动开源Seed-OSS-36B，具备灵活思维预算控制。详细内容见下文，相关参考链接请见文末。 目录 ...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/hf-weekly/009.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"Qwen图像编辑模型|DeepSeek-V3.1混合思维|字节豆包Seed-OSS开源【HF周报】"}],["meta",{"property":"og:description","content":"Qwen图像编辑模型|DeepSeek-V3.1混合思维|字节豆包Seed-OSS开源【HF周报】 封面图封面图 摘要 本周亮点：Qwen推出专业图像编辑模型，支持语义与外观双重编辑；DeepSeek升级V3.1版本，首次实现思维与非思维模式混合；字节跳动开源Seed-OSS-36B，具备灵活思维预算控制。详细内容见下文，相关参考链接请见文末。 目录 ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://neverbiasu.github.io/assets/images/placeholder.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Qwen图像编辑模型|DeepSeek-V3.1混合思维|字节豆包Seed-OSS开源【HF周报】\\",\\"image\\":[\\"https://neverbiasu.github.io/assets/images/placeholder.png\\",\\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit_homepage.jpg\\",\\"https://cdn.deepseek.com/api-docs/v3.1_benchmark_1.webp\\",\\"https://cdn-uploads.huggingface.co/production/uploads/6618e0424dbef6bd3c72f89a/oINYbgXILJgTuKxKc1aO_.png\\",\\"https://huggingface.co/Skywork/Matrix-Game-2.0/resolve/main/architecture.png\\",\\"https://huggingface.co/KBlueLeaf/HDM-xut-340M-anime/resolve/main/images/thumbnail.webp \\",\\"https://scontent.fyyc6-1.fna.fbcdn.net/v/t39.2365-6/533218684_738909568994563_5324898431385079354_n.png?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=BId3_-q_AW4Q7kNvwGNcw1a&_nc_oc=AdlWr0DN3ri06Diz0d2DqsXuZ93A5ewj_FoQ6rqpsDQf5oF7qBIMHayVq8A_8ChFmcg&_nc_zt=14&_nc_ht=scontent.fyyc6-1.fna&_nc_gid=QA94fzBTpvuEYJWsKiLvpQ&oh=00_AfWAvd1ONy4dnleyvfclhEOqxzV1wEetOmdWbx60j-k1Bw&oe=68C56861\\",\\"https://huggingface.co/stepfun-ai/NextStep-1-Large/resolve/main/assets/teaser.jpg\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"Qwen-Image-Edit：专业图像编辑生成模型","slug":"qwen-image-edit-专业图像编辑生成模型","link":"#qwen-image-edit-专业图像编辑生成模型","children":[]},{"level":2,"title":"DeepSeek-V3.1：混合思维模式语言模型","slug":"deepseek-v3-1-混合思维模式语言模型","link":"#deepseek-v3-1-混合思维模式语言模型","children":[]},{"level":2,"title":"Seed-OSS-36B-Instruct：灵活思维预算控制模型","slug":"seed-oss-36b-instruct-灵活思维预算控制模型","link":"#seed-oss-36b-instruct-灵活思维预算控制模型","children":[]},{"level":2,"title":"OmniNeural-4B：首款NPU感知多模态模型","slug":"omnineural-4b-首款npu感知多模态模型","link":"#omnineural-4b-首款npu感知多模态模型","children":[]},{"level":2,"title":"Matrix-Game-2.0：实时交互世界模型","slug":"matrix-game-2-0-实时交互世界模型","link":"#matrix-game-2-0-实时交互世界模型","children":[]},{"level":2,"title":"HDM-xut-340M-anime：轻量级动漫风格生成","slug":"hdm-xut-340m-anime-轻量级动漫风格生成","link":"#hdm-xut-340m-anime-轻量级动漫风格生成","children":[]},{"level":2,"title":"DINOv3-ViT-7B：大规模视觉基础模型","slug":"dinov3-vit-7b-大规模视觉基础模型","link":"#dinov3-vit-7b-大规模视觉基础模型","children":[]},{"level":2,"title":"NextStep-1-Large：连续标记自回归生成","slug":"nextstep-1-large-连续标记自回归生成","link":"#nextstep-1-large-连续标记自回归生成","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":6.41,"words":1923},"filePathRelative":"zh/posts/hf-weekly/009.md","excerpt":"\\n<figure><img src=\\"/assets/images/placeholder.png\\" alt=\\"封面图\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>封面图</figcaption></figure>\\n<h2>摘要</h2>\\n<p>本周亮点：Qwen推出专业图像编辑模型，支持语义与外观双重编辑；DeepSeek升级V3.1版本，首次实现思维与非思维模式混合；字节跳动开源Seed-OSS-36B，具备灵活思维预算控制。详细内容见下文，相关参考链接请见文末。</p>\\n<hr>\\n<h2>目录</h2>\\n<ol>\\n<li><a href=\\"#qwen-image-edit%E4%B8%93%E4%B8%9A%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B\\">Qwen-Image-Edit：专业图像编辑生成模型</a></li>\\n<li><a href=\\"#deepseek-v31%E6%B7%B7%E5%90%88%E6%80%9D%E7%BB%B4%E6%A8%A1%E5%BC%8F%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B\\">DeepSeek-V3.1：混合思维模式语言模型</a></li>\\n<li><a href=\\"#seed-oss-36b-instruct%E7%81%B5%E6%B4%BB%E6%80%9D%E7%BB%B4%E9%A2%84%E7%AE%97%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%9E%8B\\">Seed-OSS-36B-Instruct：灵活思维预算控制模型</a></li>\\n<li><a href=\\"#omnineural-4b%E9%A6%96%E6%AC%BEnpu%E6%84%9F%E7%9F%A5%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B\\">OmniNeural-4B：首款NPU感知多模态模型</a></li>\\n<li><a href=\\"#matrix-game-20%E5%AE%9E%E6%97%B6%E4%BA%A4%E4%BA%92%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B\\">Matrix-Game-2.0：实时交互世界模型</a></li>\\n<li><a href=\\"#hdm-xut-340m-anime%E8%BD%BB%E9%87%8F%E7%BA%A7%E5%8A%A8%E6%BC%AB%E9%A3%8E%E6%A0%BC%E7%94%9F%E6%88%90\\">HDM-xut-340M-anime：轻量级动漫风格生成</a></li>\\n<li><a href=\\"#dinov3-vit-7b%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B\\">DINOv3-ViT-7B：大规模视觉基础模型</a></li>\\n<li><a href=\\"#nextstep-1-large%E8%BF%9E%E7%BB%AD%E6%A0%87%E8%AE%B0%E8%87%AA%E5%9B%9E%E5%BD%92%E7%94%9F%E6%88%90\\">NextStep-1-Large：连续标记自回归生成</a></li>\\n</ol>","autoDesc":true}')}}]);