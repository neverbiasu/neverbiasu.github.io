"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[1455],{66262:(e,i)=>{i.A=(e,i)=>{const a=e.__vccOpts||e;for(const[e,l]of i)a[e]=l;return a}},79102:(e,i,a)=>{a.r(i),a.d(i,{comp:()=>n,data:()=>t});var l=a(20641);const r={},n=(0,a(66262).A)(r,[["render",function(e,i){return(0,l.uX)(),(0,l.CE)("div",null,i[0]||(i[0]=[(0,l.Fv)('<h1 id="【论文精读】pixnerd-像素神经场扩散" tabindex="-1"><a class="header-anchor" href="#【论文精读】pixnerd-像素神经场扩散"><span>【论文精读】PixNerd：像素神经场扩散</span></a></h1><figure><img src="https://arxiv.org/html/2507.23268v2/x3.png" alt="示例样本（256×256 与 512×512），由 PixNerd-XL/16 生成；CFG=3.5，展示整体生成质量与风格多样性" tabindex="0" loading="lazy"><figcaption>示例样本（256×256 与 512×512），由 PixNerd-XL/16 生成；CFG=3.5，展示整体生成质量与风格多样性</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>PixNerd 将神经场融入扩散 Transformer，于像素空间按块解码像素速度，无需 VAE。ImageNet 256×256 FID 2.16、512×512 FID 2.84；GenEval 0.73、DPG 80.9，训练推理与潜在扩散同量级。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li><li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li><li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li><li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li></ol><hr><h2 id="背景与研究目标" tabindex="-1"><a class="header-anchor" href="#背景与研究目标"><span>背景与研究目标</span></a></h2><ol><li>背景与问题：主流扩散模型多在 VAE 压缩的潜在空间中训练（如 DiT、SiT），计算高效但存在两阶段范式带来的累积误差与解码伪影。直接像素空间扩散虽可避免 VAE 伪影，但因高维度与大 patch 学习困难，常需复杂级联流程，训练/采样负担重。</li><li>相关工作与数据集：对比 LDM/DiT/SiT 等潜在扩散，以及 ADM、PixelFlow、RDM 等像素扩散；主要实验在 ImageNet 256×256 与 512×512，文本到图像在 GenEval 与 DPG 基准上评测。</li><li>研究目标：提出单尺度、单阶段、端到端的像素空间扩散框架，在不依赖 VAE 的前提下兼顾高保真与高效率，并在大 patch 设置下学习细粒度细节。</li></ol><hr><h2 id="方法与创新点、" tabindex="-1"><a class="header-anchor" href="#方法与创新点、"><span>方法与创新点、</span></a></h2><figure><img src="https://arxiv.org/html/2507.23268v2/x2.png" alt="左：与其他扩散范式对比；右：PixNerd 架构，遵循 DiT 设计并以神经场替代最终线性投影以建模大块内细节" tabindex="0" loading="lazy"><figcaption>左：与其他扩散范式对比；右：PixNerd 架构，遵循 DiT 设计并以神经场替代最终线性投影以建模大块内细节</figcaption></figure><h3 id="_1-整体流程" tabindex="-1"><a class="header-anchor" href="#_1-整体流程"><span>1. 整体流程：</span></a></h3><p>在扩散 Transformer（Rectified Flow 速度参数化）中，最后的“逐块线性投影”被“逐块神经场解码”替代。具体做法是用 Transformer 的隐藏状态为每个图像块预测一组小型 MLP 的权重，然后将块内像素坐标的编码与噪声像素拼接输入该 MLP，输出像素级速度，完成去噪。</p><ol><li>按块权重预测：由隐藏状态 Xⁿ 经 SiLU+Linear 预测 {W¹ⁿ, W²ⁿ}，构成该块的 MLP；对权重与输出特征施以行/特征归一化以稳定训练与收敛。</li><li>坐标编码：采用 DCT-Basis 相比传统正弦/余弦编码收敛更快、细节更优；将局部像素坐标 (i,j) 编码后与噪声像素值拼接供块内 MLP 解码像素速度。</li><li>单尺度、单阶段：保持与潜在扩散近似的 token 数，不使用级联金字塔与 VAE，简化训练与推理。</li></ol><h3 id="_2-关键增强" tabindex="-1"><a class="header-anchor" href="#_2-关键增强"><span>2. 关键增强：</span></a></h3><ol><li>架构与训练：SwiGLU、RMSNorm、RoPE、lognorm sampling 调度；引入与 DINOv2-Base 的表示对齐损失（权重 0.5），提升稳定性与感知质量。</li><li>采样器：实践表明 Adams 二阶求解器在有限步数下优于欧拉；间隔式 CFG（区间约 [0.1,1]，默认 3.5）带来更好 FID。</li></ol><h3 id="_3-与现有方法的不同" tabindex="-1"><a class="header-anchor" href="#_3-与现有方法的不同"><span>3. 与现有方法的不同：</span></a></h3><p>不依赖 VAE，无两阶段误差与解码伪影；相较像素级级联方法（如 PixelFlow/RDM），以单阶段端到端替代复杂管线；相较“线性投影解码”，按块神经场能在大 patch 下保留像素级细节。</p><hr><h2 id="实验与结果分析" tabindex="-1"><a class="header-anchor" href="#实验与结果分析"><span>实验与结果分析</span></a></h2><figure><img src="https://arxiv.org/html/2507.23268v2/x4.png" alt="文本到图像 512×512 可视化示例，展示不同长度与风格的文本描述。针对不同长度和风格的文本输入，PixNerd 能以大 patch（16）生成高质量样本。采样采用 Adams 二阶求解器（25 步），CFG 设为 4.0。" tabindex="0" loading="lazy"><figcaption>文本到图像 512×512 可视化示例，展示不同长度与风格的文本描述。针对不同长度和风格的文本输入，PixNerd 能以大 patch（16）生成高质量样本。采样采用 Adams 二阶求解器（25 步），CFG 设为 4.0。</figcaption></figure><h3 id="_1-计算效率" tabindex="-1"><a class="header-anchor" href="#_1-计算效率"><span>1. 计算效率：</span></a></h3><ol><li>资源对比显示，PixNerd 训练/推理显存与延迟与潜在扩散相当；相较 ADM-G、PixelFlow 等像素扩散，延迟近 8× 更优，使直接像素合成具备实用性。</li></ol><h3 id="_2-类别到图像-imagenet" tabindex="-1"><a class="header-anchor" href="#_2-类别到图像-imagenet"><span>2. 类别到图像（ImageNet）：</span></a></h3><ol><li>256×256：PixNerd-XL/16 在 50 步 Adams-2 下达 2.16 FID（Euler-100 为 2.15），与 DiT/SiT 等潜在扩散相当，显著优于 JetFormer/FractalMAR/ADM 等像素基线；</li><li>512×512：微调达 2.84 FID，继续保持竞争力。</li></ol><h3 id="_3-文本到图像" tabindex="-1"><a class="header-anchor" href="#_3-文本到图像"><span>3. 文本到图像：</span></a></h3><ol><li>使用 ~45M 开源数据训练，PixNerd-XXL/16 在 GenEval 总分 0.73、DPG 平均 80.9，优于 PixelFlow，并与部分主流潜在扩散模型竞争。</li></ol><h3 id="_4-任意分辨率-免训练" tabindex="-1"><a class="header-anchor" href="#_4-任意分辨率-免训练"><span>4. 任意分辨率（免训练）：</span></a></h3><figure><img src="https://arxiv.org/html/2507.23268v2/x5.png" alt="免训练任意分辨率生成；保持预训练分辨率的 token 数，仅对神经场坐标做插值以适配目标分辨率" tabindex="0" loading="lazy"><figcaption>免训练任意分辨率生成；保持预训练分辨率的 token 数，仅对神经场坐标做插值以适配目标分辨率</figcaption></figure><ol><li>通过在 Transformer 中保持 token 数不变，仅插值神经场坐标来匹配目标分辨率，实现无需重新训练的任意分辨率/纵横比生成。</li></ol><h3 id="消融与关键结论" tabindex="-1"><a class="header-anchor" href="#消融与关键结论"><span>消融与关键结论</span></a></h3><ol><li>坐标编码：DCT-Basis 显著优于 sin/cos 编码；</li><li>神经场归一化：对 FC1/FC2 权重与输出特征同时归一化最佳；</li><li>MLP 结构：2 层、64 通道在性能/效率间最优折中；</li><li>推理：Adams-2 在有限步数下优于欧拉，CFG 约 3.4–3.6 最优。</li></ol><hr><h2 id="模型启发与方法延伸" tabindex="-1"><a class="header-anchor" href="#模型启发与方法延伸"><span>模型启发与方法延伸</span></a></h2><ol><li>神经场可作为“可微像素解码器”无缝嵌入生成模型，在保持可控 token 规模下，增强大 patch 的细粒度表达能力；</li><li>与语义表征对齐（如 DINOv2、CLIP）结合，有望进一步提升构图与结构一致性；</li><li>训练与部署简化（无 VAE、无级联）对工业落地友好；任意分辨率机制为长宽比自适应与分辨率放大提供工程思路；</li><li>后续可探索原生长宽比训练、像素空间后训练与奖励建模、与视频/3D 神经场的跨域迁移等。</li></ol><hr><h2 id="结论与未来展望" tabindex="-1"><a class="header-anchor" href="#结论与未来展望"><span>结论与未来展望</span></a></h2><ol><li>贡献总结：提出单尺度、单阶段、端到端的像素扩散框架 PixNerd，以按块神经场解码替代线性投影，在 ImageNet 与 T2I 基准上达到与潜在扩散相当的质量与效率，并显著优于既有像素扩散方法；</li><li>优势与不足：优点在于无 VAE 伪影、管线简洁、任意分辨率灵活与高效采样；不足包括个别场景细节仍不清晰，与部分最强潜在模型仍存在指标差距；</li><li>展望：结合更强文本编码与多模态对齐、原生尺寸训练与像素空间 SFT/RL，将进一步缩小差距；神经场解码范式有望拓展到视频、3D 与跨模态生成。</li></ol><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span><strong>参考链接</strong></span></a></h3><ol><li><a href="https://huggingface.co/spaces/MCG-NJU/PixNerd" target="_blank" rel="noopener noreferrer">项目主页</a></li><li><a href="https://github.com/MCG-NJU/PixNerd" target="_blank" rel="noopener noreferrer">代码仓库</a></li><li><a href="https://huggingface.co/spaces/MCG-NJU/PixNerd" target="_blank" rel="noopener noreferrer">模型仓库</a></li><li><a href="https://arxiv.org/abs/2507.23268" target="_blank" rel="noopener noreferrer">论文原文</a></li></ol>',43)]))}]]),t=JSON.parse('{"path":"/zh/posts/papers/pixnerd.html","title":"【论文精读】PixNerd：像素神经场扩散","lang":"zh-CN","frontmatter":{"description":"【论文精读】PixNerd：像素神经场扩散 示例样本（256×256 与 512×512），由 PixNerd-XL/16 生成；CFG=3.5，展示整体生成质量与风格多样性示例样本（256×256 与 512×512），由 PixNerd-XL/16 生成；CFG=3.5，展示整体生成质量与风格多样性 摘要 PixNerd 将神经场融入扩散 Tran...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/papers/pixnerd.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"【论文精读】PixNerd：像素神经场扩散"}],["meta",{"property":"og:description","content":"【论文精读】PixNerd：像素神经场扩散 示例样本（256×256 与 512×512），由 PixNerd-XL/16 生成；CFG=3.5，展示整体生成质量与风格多样性示例样本（256×256 与 512×512），由 PixNerd-XL/16 生成；CFG=3.5，展示整体生成质量与风格多样性 摘要 PixNerd 将神经场融入扩散 Tran..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://arxiv.org/html/2507.23268v2/x3.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"【论文精读】PixNerd：像素神经场扩散\\",\\"image\\":[\\"https://arxiv.org/html/2507.23268v2/x3.png\\",\\"https://arxiv.org/html/2507.23268v2/x2.png\\",\\"https://arxiv.org/html/2507.23268v2/x4.png\\",\\"https://arxiv.org/html/2507.23268v2/x5.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"背景与研究目标","slug":"背景与研究目标","link":"#背景与研究目标","children":[]},{"level":2,"title":"方法与创新点、","slug":"方法与创新点、","link":"#方法与创新点、","children":[{"level":3,"title":"1. 整体流程：","slug":"_1-整体流程","link":"#_1-整体流程","children":[]},{"level":3,"title":"2. 关键增强：","slug":"_2-关键增强","link":"#_2-关键增强","children":[]},{"level":3,"title":"3. 与现有方法的不同：","slug":"_3-与现有方法的不同","link":"#_3-与现有方法的不同","children":[]}]},{"level":2,"title":"实验与结果分析","slug":"实验与结果分析","link":"#实验与结果分析","children":[{"level":3,"title":"1. 计算效率：","slug":"_1-计算效率","link":"#_1-计算效率","children":[]},{"level":3,"title":"2. 类别到图像（ImageNet）：","slug":"_2-类别到图像-imagenet","link":"#_2-类别到图像-imagenet","children":[]},{"level":3,"title":"3. 文本到图像：","slug":"_3-文本到图像","link":"#_3-文本到图像","children":[]},{"level":3,"title":"4. 任意分辨率（免训练）：","slug":"_4-任意分辨率-免训练","link":"#_4-任意分辨率-免训练","children":[]},{"level":3,"title":"消融与关键结论","slug":"消融与关键结论","link":"#消融与关键结论","children":[]}]},{"level":2,"title":"模型启发与方法延伸","slug":"模型启发与方法延伸","link":"#模型启发与方法延伸","children":[]},{"level":2,"title":"结论与未来展望","slug":"结论与未来展望","link":"#结论与未来展望","children":[{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":5.48,"words":1645},"filePathRelative":"zh/posts/papers/pixnerd.md","excerpt":"\\n<figure><img src=\\"https://arxiv.org/html/2507.23268v2/x3.png\\" alt=\\"示例样本（256×256 与 512×512），由 PixNerd-XL/16 生成；CFG=3.5，展示整体生成质量与风格多样性\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>示例样本（256×256 与 512×512），由 PixNerd-XL/16 生成；CFG=3.5，展示整体生成质量与风格多样性</figcaption></figure>\\n<h2>摘要</h2>\\n<p>PixNerd 将神经场融入扩散 Transformer，于像素空间按块解码像素速度，无需 VAE。ImageNet 256×256 FID 2.16、512×512 FID 2.84；GenEval 0.73、DPG 80.9，训练推理与潜在扩散同量级。</p>","autoDesc":true}')}}]);