"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[2175],{6262:(i,t)=>{t.A=(i,t)=>{const e=i.__vccOpts||i;for(const[i,l]of t)e[i]=l;return e}},5681:(i,t,e)=>{e.r(t),e.d(t,{comp:()=>a,data:()=>r});var l=e(641);const n={},a=(0,e(6262).A)(n,[["render",function(i,t){return(0,l.uX)(),(0,l.CE)("div",null,t[0]||(t[0]=[(0,l.Fv)('<h1 id="【论文精读】colorizediffusion-基于参考图像和文本的可调整草图上色方法" tabindex="-1"><a class="header-anchor" href="#【论文精读】colorizediffusion-基于参考图像和文本的可调整草图上色方法"><span>【论文精读】ColorizeDiffusion：基于参考图像和文本的可调整草图上色方法</span></a></h1><figure><img src="https://arxiv.org/html/2401.01456v3/x1.png" alt="ColorizeDiffusion Teaser" tabindex="0" loading="lazy"><figcaption>ColorizeDiffusion Teaser</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>ColorizeDiffusion由东京工业大学研究团队提出，旨在解决草图上色中的&quot;分布问题&quot;——参考图像与草图结构的平衡困境。基于扩散模型，该方法通过三种创新训练策略和零样本文本调控，实现精确可控的上色效果，支持动漫/漫画风格创作。</p><hr><h2 id="目录" tabindex="-1"><a class="header-anchor" href="#目录"><span>目录</span></a></h2><ol><li><a href="#%E8%83%8C%E6%99%AF%E4%B8%8E%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87">背景与研究目标</a></li><li><a href="#%E6%96%B9%E6%B3%95%E4%B8%8E%E5%88%9B%E6%96%B0%E7%82%B9">方法与创新点</a></li><li><a href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">实验与结果分析</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%90%AF%E5%8F%91%E4%B8%8E%E6%96%B9%E6%B3%95%E5%BB%B6%E4%BC%B8">模型启发与方法延伸</a></li><li><a href="#%E7%BB%93%E8%AE%BA%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B">结论与未来展望</a></li></ol><hr><h2 id="背景与研究目标" tabindex="-1"><a class="header-anchor" href="#背景与研究目标"><span>背景与研究目标</span></a></h2><h3 id="问题-参考图像上色的-分布问题" tabindex="-1"><a class="header-anchor" href="#问题-参考图像上色的-分布问题"><span>问题：参考图像上色的&quot;分布问题&quot;</span></a></h3><p>现有参考图像上色方法面临一个根本性矛盾。即使是先进的T2I（Text-to-Image）模型，在结合如ControlNet等工具进行草图引导上色时，也可能出现&quot;分布问题&quot;。如下图所示，当文本提示与草图结构存在潜在冲突时（例如，提示中描述的服装覆盖了草图中的手臂区域），模型可能优先遵循文本提示，导致与草图不一致的分割和颜色分配，尤其是在预期为肤色的区域。具体而言，下图展示了网络在手臂区域优先处理提示条件而非草图，导致非预期的上色差异，尤其是在预期为肤色的区域，造成视觉上不协调的分割。此结果由ControlNet_lineart_anime + Anything v3框架生成。</p><figure><img src="https://arxiv.org/html/2401.01456v3/x2.png" alt="T2I上色中的分布问题示例" tabindex="0" loading="lazy"><figcaption>T2I上色中的分布问题示例</figcaption></figure><ul><li><strong>核心挑战</strong>：参考图像提供了丰富的颜色信息，但往往导致结果优先参考图像信息而忽略草图结构。</li><li><strong>本质分析</strong>：在分布层面，上色结果分布 $$p(z|s,r)$$ 偏离单纯基于草图的分布 $$p(z|s)$$，导致结构失真。下图直观地展示了这一问题：训练后，优化后的条件生成分布 $$p_{\\theta}(z|s,r)$$ 的大部分区域会偏离仅基于草图的理想分布 $$p(z|s)$$。</li></ul><figure><img src="https://arxiv.org/html/2401.01456v3/x4.png" alt="条件生成分布与理想草图分布的偏离" tabindex="0" loading="lazy"><figcaption>条件生成分布与理想草图分布的偏离</figcaption></figure><h3 id="图像特征表示与cls标记" tabindex="-1"><a class="header-anchor" href="#图像特征表示与cls标记"><span>图像特征表示与CLS标记</span></a></h3><p>在理解ColorizeDiffusion的创新之前，需要了解视觉特征表示的基本概念：</p><ul><li><p><strong>CLS标记（Class Token）</strong>：源自视觉Transformer（ViT）和CLIP等模型，是一种特殊标记，用于捕获整个图像的全局特征。它通常位于序列开始位置，经过自注意力机制后包含图像整体语义信息。</p></li><li><p><strong>本地标记（Local Tokens）</strong>：代表图像不同区域的局部特征，保留位置和细节信息，有助于精确控制特定区域。</p></li></ul><p>ColorizeDiffusion基于这两种特征表示方式，设计了全局CLS模型和局部注意力模型两种变体，分别适用于不同精度和效率需求的上色场景。</p><h3 id="三大创新点" tabindex="-1"><a class="header-anchor" href="#三大创新点"><span>三大创新点</span></a></h3><ol><li><p><strong>分布问题解决策略</strong>：提出丢弃训练、噪声训练和双条件训练三种策略，系统解决参考图像与草图结构的平衡问题</p></li><li><p><strong>双重模型架构</strong>：设计CLS模型（全局特征）和注意力模型（局部特征）两种架构，适应不同应用场景和用户需求</p></li><li><p><strong>零样本文本调控</strong>：无需重训练即可通过文本提示精确调整上色结果，支持全局和局部两种调控机制</p></li></ol><hr><h2 id="方法与创新点" tabindex="-1"><a class="header-anchor" href="#方法与创新点"><span>方法与创新点</span></a></h2><h3 id="架构概览" tabindex="-1"><a class="header-anchor" href="#架构概览"><span>架构概览</span></a></h3><p>ColorizeDiffusion基于Stable Diffusion微调，包含两种主要模型变体：</p><table><thead><tr><th>模型类型</th><th>特征获取</th><th>优势</th><th>应用场景</th></tr></thead><tbody><tr><td><strong>CLS模型</strong></td><td>仅使用CLS标记（全局特征）</td><td>参数少、计算高效、操作简单</td><td>全局颜色调整、快速原型</td></tr><tr><td><strong>注意力模型</strong></td><td>使用所有本地标记（局部特征）</td><td>质量高、保留结构好、局部控制精确</td><td>精细上色、细节调整</td></tr></tbody></table><p>下图展示了CLS模型的训练流程示意图。 <img src="https://arxiv.org/html/2401.01456v3/x6.png" alt="CLS模型的训练流程" loading="lazy"></p><h3 id="三种解决-分布问题-的创新策略" tabindex="-1"><a class="header-anchor" href="#三种解决-分布问题-的创新策略"><span>三种解决&quot;分布问题&quot;的创新策略</span></a></h3><ol><li><p><strong>丢弃训练</strong>：</p><ul><li>训练中高概率（0.75-0.8）随机丢弃参考图像</li><li>减缓交叉注意力模块优化，保持模型对草图结构敏感</li></ul></li><li><p><strong>噪声训练</strong> （最有效）：</p><ul><li>向参考图像特征添加动态噪声：$$\\tau_{\\phi,t}(r)=\\alpha_t \\cdot \\tau_{\\phi}(r) + \\beta_t \\cdot \\epsilon_r$$</li><li>降低参考图像语义权重，同时避免模型退化为编码器解码器</li></ul></li><li><p><strong>双条件训练</strong>：</p><ul><li>添加辅助损失项：$$\\lambda \\cdot |\\epsilon^{\\prime} - \\epsilon<sup>{\\prime}_{\\theta}(z</sup>{\\prime}_t,t,s)|^2_2$$</li><li>直接惩罚仅基于草图结果与真实图像差异</li></ul></li></ol><h3 id="文本调控技术" tabindex="-1"><a class="header-anchor" href="#文本调控技术"><span>文本调控技术</span></a></h3><p>无需重训练即可通过文本提示精确调整上色结果。推理流程如下图所示，图像标记（image tokens）在输入到去噪U-Net之前被编辑。图中具体展示了注意力模型使用局部操控（local manipulation）生成的结果。</p><figure><img src="https://arxiv.org/html/2401.01456v3/x7.png" alt="ColorizeDiffusion推理流程" tabindex="0" loading="lazy"><figcaption>ColorizeDiffusion推理流程</figcaption></figure><p><strong>1. 全局文本调控</strong> (CLS模型)</p><ul><li>通过调整全局CLS标记实现整体颜色变化</li><li>支持增强/替换两种操作模式和强度控制</li></ul><p><strong>2. 局部文本调控</strong> (注意力模型)</p><ul><li>使用位置权重矩阵(PWM)定位并修改特定区域</li><li>基于目标文本(如&quot;紫色头发&quot;)和锚文本(如&quot;棕色头发&quot;)计算相关性</li></ul><hr><h2 id="实验与结果分析" tabindex="-1"><a class="header-anchor" href="#实验与结果分析"><span>实验与结果分析</span></a></h2><p>本章节介绍ColorizeDiffusion模型的关键实验结果与评估。</p><h3 id="训练策略比较与消融实验" tabindex="-1"><a class="header-anchor" href="#训练策略比较与消融实验"><span>训练策略比较与消融实验</span></a></h3><p>论文比较了三种解决&quot;分布问题&quot;的训练策略效果。噪声训练策略在保持草图结构和色彩丰富度方面表现最佳，能有效减轻分布问题。消融实验表明，合适的噪声级别和丢弃率对生成质量有显著影响。</p><figure><img src="https://arxiv.org/html/2401.01456v3/x3.png" alt="不同训练策略上色效果对比" tabindex="0" loading="lazy"><figcaption>不同训练策略上色效果对比</figcaption></figure><p>对比图显示噪声训练的ColorizeDiffusion(d行)比其他方法更好地平衡了草图结构与参考颜色。特别是Shuffle-noisy模型即使在长时间训练后，仍能保持对草图输入的语义保真度。</p><figure><img src="https://arxiv.org/html/2401.01456v3/x11.png" alt="消融研究中的模型上色结果" tabindex="0" loading="lazy"><figcaption>消融研究中的模型上色结果</figcaption></figure><h3 id="模型架构与调控技术评估" tabindex="-1"><a class="header-anchor" href="#模型架构与调控技术评估"><span>模型架构与调控技术评估</span></a></h3><h4 id="cls与attention模型对比" tabindex="-1"><a class="header-anchor" href="#cls与attention模型对比"><span>CLS与Attention模型对比</span></a></h4><p>论文提出的两种架构各有优势：Attention模型在细节保留和颜色迁移精确性上表现更优；CLS模型结构更简单、计算效率更高，适用于对细节要求不高的快速上色场景。</p><figure><img src="https://arxiv.org/html/2401.01456v3/x8.png" alt="CLS与Attention模型效果对比" tabindex="0" loading="lazy"><figcaption>CLS与Attention模型效果对比</figcaption></figure><h4 id="文本调控能力评估" tabindex="-1"><a class="header-anchor" href="#文本调控能力评估"><span>文本调控能力评估</span></a></h4><p>在局部文本调控中，位置权重矩阵(PWM)是关键组件，通过调整阈值可以精确控制编辑区域范围。实验表明，局部调控能更精确地改变目标区域（如头发颜色），同时保持其他区域不变。</p><p>局部文本操控不仅能修改特定对象的颜色或纹理，还能保持整体图像风格的一致性，使编辑更加自然。</p><h3 id="与现有方法对比" tabindex="-1"><a class="header-anchor" href="#与现有方法对比"><span>与现有方法对比</span></a></h3><p>ColorizeDiffusion与现有方法在颜色准确性、结构保持、细节丰富度等方面进行了对比。结果表明，该方法在保持草图结构的同时，能更好地转移参考图像的颜色信息。</p><figure><img src="https://arxiv.org/html/2401.01456v3/x15.png" alt="与现有方法效果对比" tabindex="0" loading="lazy"><figcaption>与现有方法效果对比</figcaption></figure><p>用户研究也验证了模型的实用性和在真实场景中的接受度，参与者普遍认为ColorizeDiffusion生成的结果在视觉质量和一致性上更为出色。</p><h3 id="综合优势" tabindex="-1"><a class="header-anchor" href="#综合优势"><span>综合优势</span></a></h3><p>实验结果突显了ColorizeDiffusion的四大优势：</p><p><strong>结构保真</strong>：有效解决&quot;分布问题&quot;，保持草图线条完整性<br><strong>颜色转移</strong>：准确从参考图像提取颜色信息<br><strong>灵活控制</strong>：支持全局/局部颜色调整，无需重训练<br><strong>多样适配</strong>：两种模型架构适应不同场景需求</p><hr><h2 id="模型启发与方法延伸" tabindex="-1"><a class="header-anchor" href="#模型启发与方法延伸"><span>模型启发与方法延伸</span></a></h2><h3 id="技术启示" tabindex="-1"><a class="header-anchor" href="#技术启示"><span>技术启示</span></a></h3><ol><li><strong>分布问题的普适性</strong>：可推广至其他多条件生成任务</li><li><strong>模块化架构设计</strong>：功能分离为独立模块，用户可按需选择</li><li><strong>交互式生成范式</strong>：初始生成+交互式调整，无需重训练实现精细控制</li></ol><h3 id="应用场景拓展" tabindex="-1"><a class="header-anchor" href="#应用场景拓展"><span>应用场景拓展</span></a></h3><p><strong>动漫/漫画制作</strong>：减轻专业艺术家上色负担，提供快速原型工具<br><strong>艺术教育</strong>：帮助学习者理解颜色应用原理，提供即时反馈<br><strong>游戏/媒体</strong>：支持内容创作中的个性化颜色方案设计<br><strong>跨领域应用</strong>：可拓展到建筑设计、时装设计等领域</p><hr><h2 id="结论与未来展望" tabindex="-1"><a class="header-anchor" href="#结论与未来展望"><span>结论与未来展望</span></a></h2><h3 id="主要贡献总结" tabindex="-1"><a class="header-anchor" href="#主要贡献总结"><span>主要贡献总结</span></a></h3><ol><li>系统分析并解决参考图像上色的&quot;分布问题&quot;，通过三种创新训练策略实现结构保真</li><li>设计互补的双模型架构，适应不同应用场景和用户需求</li><li>提出零样本文本调控技术，实现精确调整而无需重训练</li></ol><h3 id="局限性与未来改进" tabindex="-1"><a class="header-anchor" href="#局限性与未来改进"><span>局限性与未来改进</span></a></h3><ol><li><strong>局部调控精度</strong>：当目标区域相近时，PWM可能影响非预期区域</li><li><strong>参数调整复杂</strong>：用户需通过实验确定合适的阈值参数</li><li><strong>计算资源需求</strong>：模型规模较大，低端设备上应用受限</li></ol><h3 id="未来研究方向" tabindex="-1"><a class="header-anchor" href="#未来研究方向"><span>未来研究方向</span></a></h3><ul><li>开发自适应可训练模块，减少参数调整需求</li><li>探索更轻量级的模型设计，降低运行门槛</li><li>拓展到更多艺术风格，增强通用性</li></ul><hr><h3 id="参考链接" tabindex="-1"><a class="header-anchor" href="#参考链接"><span>参考链接</span></a></h3><ol><li><a href="https://arxiv.org/abs/2401.01456" target="_blank" rel="noopener noreferrer">论文原文</a></li><li><a href="https://github.com/tellurion-kanata/colorizeDiffusion" target="_blank" rel="noopener noreferrer">项目代码仓库</a></li><li><a href="https://huggingface.co/tellurion/colorizer" target="_blank" rel="noopener noreferrer">Hugging Face模型权重</a></li></ol>',74)]))}]]),r=JSON.parse('{"path":"/zh/posts/papers/colorizediffusion.html","title":"【论文精读】ColorizeDiffusion：基于参考图像和文本的可调整草图上色方法","lang":"zh-CN","frontmatter":{"description":"【论文精读】ColorizeDiffusion：基于参考图像和文本的可调整草图上色方法 ColorizeDiffusion TeaserColorizeDiffusion Teaser 摘要 ColorizeDiffusion由东京工业大学研究团队提出，旨在解决草图上色中的\\"分布问题\\"——参考图像与草图结构的平衡困境。基于扩散模型，该方法通过三种创新训...","gitInclude":[],"head":[["meta",{"property":"og:url","content":"https://neverbiasu.github.io/zh/posts/papers/colorizediffusion.html"}],["meta",{"property":"og:site_name","content":"Nlog"}],["meta",{"property":"og:title","content":"【论文精读】ColorizeDiffusion：基于参考图像和文本的可调整草图上色方法"}],["meta",{"property":"og:description","content":"【论文精读】ColorizeDiffusion：基于参考图像和文本的可调整草图上色方法 ColorizeDiffusion TeaserColorizeDiffusion Teaser 摘要 ColorizeDiffusion由东京工业大学研究团队提出，旨在解决草图上色中的\\"分布问题\\"——参考图像与草图结构的平衡困境。基于扩散模型，该方法通过三种创新训..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://arxiv.org/html/2401.01456v3/x1.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"【论文精读】ColorizeDiffusion：基于参考图像和文本的可调整草图上色方法\\",\\"image\\":[\\"https://arxiv.org/html/2401.01456v3/x1.png\\",\\"https://arxiv.org/html/2401.01456v3/x2.png\\",\\"https://arxiv.org/html/2401.01456v3/x4.png\\",\\"https://arxiv.org/html/2401.01456v3/x6.png\\",\\"https://arxiv.org/html/2401.01456v3/x7.png\\",\\"https://arxiv.org/html/2401.01456v3/x3.png\\",\\"https://arxiv.org/html/2401.01456v3/x11.png\\",\\"https://arxiv.org/html/2401.01456v3/x8.png\\",\\"https://arxiv.org/html/2401.01456v3/x15.png\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"neverbiasu\\",\\"url\\":\\"https://neverbiasu.github.io\\"}]}"]]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"目录","slug":"目录","link":"#目录","children":[]},{"level":2,"title":"背景与研究目标","slug":"背景与研究目标","link":"#背景与研究目标","children":[{"level":3,"title":"问题：参考图像上色的\\"分布问题\\"","slug":"问题-参考图像上色的-分布问题","link":"#问题-参考图像上色的-分布问题","children":[]},{"level":3,"title":"图像特征表示与CLS标记","slug":"图像特征表示与cls标记","link":"#图像特征表示与cls标记","children":[]},{"level":3,"title":"三大创新点","slug":"三大创新点","link":"#三大创新点","children":[]}]},{"level":2,"title":"方法与创新点","slug":"方法与创新点","link":"#方法与创新点","children":[{"level":3,"title":"架构概览","slug":"架构概览","link":"#架构概览","children":[]},{"level":3,"title":"三种解决\\"分布问题\\"的创新策略","slug":"三种解决-分布问题-的创新策略","link":"#三种解决-分布问题-的创新策略","children":[]},{"level":3,"title":"文本调控技术","slug":"文本调控技术","link":"#文本调控技术","children":[]}]},{"level":2,"title":"实验与结果分析","slug":"实验与结果分析","link":"#实验与结果分析","children":[{"level":3,"title":"训练策略比较与消融实验","slug":"训练策略比较与消融实验","link":"#训练策略比较与消融实验","children":[]},{"level":3,"title":"模型架构与调控技术评估","slug":"模型架构与调控技术评估","link":"#模型架构与调控技术评估","children":[]},{"level":3,"title":"与现有方法对比","slug":"与现有方法对比","link":"#与现有方法对比","children":[]},{"level":3,"title":"综合优势","slug":"综合优势","link":"#综合优势","children":[]}]},{"level":2,"title":"模型启发与方法延伸","slug":"模型启发与方法延伸","link":"#模型启发与方法延伸","children":[{"level":3,"title":"技术启示","slug":"技术启示","link":"#技术启示","children":[]},{"level":3,"title":"应用场景拓展","slug":"应用场景拓展","link":"#应用场景拓展","children":[]}]},{"level":2,"title":"结论与未来展望","slug":"结论与未来展望","link":"#结论与未来展望","children":[{"level":3,"title":"主要贡献总结","slug":"主要贡献总结","link":"#主要贡献总结","children":[]},{"level":3,"title":"局限性与未来改进","slug":"局限性与未来改进","link":"#局限性与未来改进","children":[]},{"level":3,"title":"未来研究方向","slug":"未来研究方向","link":"#未来研究方向","children":[]},{"level":3,"title":"参考链接","slug":"参考链接","link":"#参考链接","children":[]}]}],"readingTime":{"minutes":8.33,"words":2499},"filePathRelative":"zh/posts/papers/colorizediffusion.md","excerpt":"\\n<figure><img src=\\"https://arxiv.org/html/2401.01456v3/x1.png\\" alt=\\"ColorizeDiffusion Teaser\\" tabindex=\\"0\\" loading=\\"lazy\\"><figcaption>ColorizeDiffusion Teaser</figcaption></figure>\\n<h2>摘要</h2>\\n<p>ColorizeDiffusion由东京工业大学研究团队提出，旨在解决草图上色中的\\"分布问题\\"——参考图像与草图结构的平衡困境。基于扩散模型，该方法通过三种创新训练策略和零样本文本调控，实现精确可控的上色效果，支持动漫/漫画风格创作。</p>","autoDesc":true}')}}]);