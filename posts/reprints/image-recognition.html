<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.17" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.58" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="alternate" hreflang="zh-cn" href="https://neverbiasu.github.io/zh/posts/reprints/image-recognition.html"><meta property="og:url" content="https://neverbiasu.github.io/posts/reprints/image-recognition.html"><meta property="og:site_name" content="Nlog"><meta property="og:title" content="What Is Image Recognition? Algorithms and Applications"><meta property="og:description" content="What Is Image Recognition? Algorithms and Applications What Is Image RecognitionWhat Is Image Recognition Imagine a young girl named Emma who is fascinated by birds. Every weeke..."><meta property="og:type" content="article"><meta property="og:image" content="https://blog.roboflow.com/content/images/size/w1200/2025/06/Screenshot-2025-06-10-at-10.46.38---AM.png"><meta property="og:locale" content="en-US"><meta property="og:locale:alternate" content="zh-CN"><meta property="article:author" content="Timothy M."><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Image Recognition"><meta property="article:published_time" content="2025-06-10T17:51:41.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"What Is Image Recognition? Algorithms and Applications","image":["https://blog.roboflow.com/content/images/size/w1200/2025/06/Screenshot-2025-06-10-at-10.46.38---AM.png","https://blog.roboflow.com/content/images/2025/06/image_recognition.png","https://blog.roboflow.com/content/images/2025/06/grayscale_image_representation.png","https://blog.roboflow.com/content/images/2025/06/rgb_image_representation.png","https://blog.roboflow.com/content/images/2025/06/r_cnn.png","https://blog.roboflow.com/content/images/2025/06/workflow-example.png","https://blog.roboflow.com/content/images/2025/06/custom_models.png","https://blog.roboflow.com/content/images/2025/06/wood_model.png","https://blog.roboflow.com/content/images/2025/06/wk_1-1.png","https://blog.roboflow.com/content/images/2025/06/property_wk_1.png","https://blog.roboflow.com/content/images/2025/06/output_wk_1.png","https://blog.roboflow.com/content/images/2025/06/hand_dataset.png","https://blog.roboflow.com/content/images/2025/06/hand_model.png","https://blog.roboflow.com/content/images/2025/06/wk_hand_recog.png","https://blog.roboflow.com/content/images/2025/06/wk_hand_g_out.png","https://blog.roboflow.com/content/images/2025/06/wk_3.jpeg","https://blog.roboflow.com/content/images/2025/06/input_wk_3.jpeg","https://blog.roboflow.com/content/images/2025/06/florence_wk_3-1.jpeg","https://blog.roboflow.com/content/images/2025/06/vlm_conf.jpeg","https://blog.roboflow.com/content/images/2025/06/output_wk_3.jpeg"],"datePublished":"2025-06-10T17:51:41.000Z","dateModified":null,"author":[{"@type":"Person","name":"Timothy M."}]}</script><title>What Is Image Recognition? Algorithms and Applications | Nlog</title><meta name="description" content="What Is Image Recognition? Algorithms and Applications What Is Image RecognitionWhat Is Image Recognition Imagine a young girl named Emma who is fascinated by birds. Every weeke...">
    <link rel="stylesheet" href="/assets/css/styles.50126b1a.css">
    <link rel="preload" href="/assets/js/runtime~app.58509077.js" as="script"><link rel="preload" href="/assets/css/styles.50126b1a.css" as="style"><link rel="preload" href="/assets/js/9156.64e2dfa0.js" as="script"><link rel="preload" href="/assets/js/app.9e354a1b.js" as="script">
    <link rel="prefetch" href="/assets/js/posts_reprints_flux-qlora.html.e25c0732.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_flux-qlora.html.bdca01c9.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_news-agents-daily-recap.html.ca04913a.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ielts_simon-task2.html.2fe38674.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_crody's-model-merge-guide.html.07f44e7f.js" as="script"><link rel="prefetch" href="/assets/js/8300.853d6b0b.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ielts_usage-of-collocations-in-speaking_ielts-collocations.html.86f5c7c6.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_crody's-model-merge-guide.html.046f0899.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_image-recognition.html.07c5196c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_image-recognition.html.61c0e69e.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_blog-images.html.d2c6545e.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview.html.9d72dc96.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview.html.9fa0e201.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_what-is-block-merging.html.b7b3e018.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_original-character-lora-sdxl-character-training.html.ce54ff9d.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_step-by-step-visual-introduction-to-diffusion-models.html.cd2854e8.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_flux-kontext.html.ed243625.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_dgpst.html.d42b8362.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_original-character-lora-sdxl-character-training.html.52dcf985.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_repos_material.html.fbe2fd29.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression.html.fa0b7a73.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_X01.html.eb6ee3d6.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression.html.f7670351.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_sci_conda.html.1ba5b9e5.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_papers_workflow.html.e18adbce.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.html.a34aec93.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.html.d9238744.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_flux-kontext-optimization.html.1a4897dc.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_tutorials_comfyui_flux-kontext-beginner.html.be5a7e5e.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_flux-kontext-optimization.html.5532e6f6.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_step-by-step-visual-introduction-to-diffusion-models.html.e8bf942f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_repos_comfy-mind.html.6f1a4f44.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-impls_yolov9.html.cbf247d0.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_model-block-merge-1.html.3abbf8d4.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_ai-art-newsletter-jan-25.html.5faf4d51.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_006.html.76c84f58.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_announcing-illustrious-text‑enhancer-tag-booster-and-mood-enhancer.html.517a1ee1.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_model-block-merge-2.html.0f9c14de.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_sdo.html.b7ecccd0.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_announcing-illustrious-text‑enhancer-tag-booster-and-mood-enhancer.html.f6a8fbb8.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_resnet.html.6393e916.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_ai-art-newsletter-jan-25.html.971ce630.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_bagel.html.049a219c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_ming-omni.html.20e14b21.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_omniconsistency.html.4c6e9330.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything.html.314cbfd7.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_014.html.c41082ad.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_flux-1-kontext.html.6297069c.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_niji-lesson-2-the-terminator-line.html.ebdb33a7.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_comfyui-r1.html.bea016dd.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_niji-study-1-measuring-with-your-eyes.html.721630f8.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything.html.22fa5919.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_001.html.2986e89d.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_flux-1-kontext.html.6d0bc73c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_ecomimic-v3.html.a8a6b4b4.js" as="script"><link rel="prefetch" href="/assets/js/zh_demo_markdown.html.2b3004d0.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_colorizediffusion.html.c18d15cc.js" as="script"><link rel="prefetch" href="/assets/js/demo_markdown.html.a2081e01.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_niji-study-1-measuring-with-your-eyes.html.403ee9cf.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_repos_checklist.html.2c86bf22.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_hunyuancustom.html.c2b4ff7e.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ielts_simon-task1.html.34242439.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_dairys_250222.html.41739137.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_show-o2.html.6145cdd9.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_hf-weekly_001.html.89ae8f15.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_repos_workflow.html.5967ccd1.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_037.html.b0a4ddd3.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_templates_repos.html.b0b47564.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_033.html.3fc966cc.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_031.html.86f17a2c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_043.html.70bd2bb5.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_icedit.html.ba272966.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_032.html.30191816.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_026.html.6bc17810.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_042.html.51c7aa9e.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_omnigen2.html.7251ca67.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_transformer.html.a9f96585.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_alexnet.html.ffa5e652.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_009.html.b41f980f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_022.html.33930402.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_030.html.2f8b0739.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_039.html.e4384ea6.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_011.html.4c72c992.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_ai-art-gtc-paris-2025.html.35f9c6c5.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_046.html.7fe6c3c1.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_029.html.6bd2bbb4.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_how-and-when-to-build-multi-agent-systems.html.683f686f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_reptext.html.3e68da15.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_040.html.77ad6dd0.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_generative-ai-powered-design.html.29b4cfc0.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_045.html.bcee5b9e.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_how-and-when-to-build-multi-agent-systems.html.83239b53.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_034.html.bb3494e0.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_041.html.cc30477b.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_ai-art-gtc-paris-2025.html.dd4e6753.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_036.html.103493df.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_044.html.4316f4fc.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_015.html.f3bc358d.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_thoughts_platform-operation-thoughts-after-comfycon.html.0a6fc557.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_020.html.e178431c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_035.html.76b543ac.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_generative-ai-powered-design.html.8ea641cf.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_023.html.2c7192b4.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_021.html.1924500a.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_010.html.995fa31e.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_005.html.77979cff.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_qr-lora.html.12a884c9.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_hf-weekly_003.html.add9120d.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_003.html.fe38d2ee.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_038.html.54d019c6.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_047.html.7b249002.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_018.html.3aabc796.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_012.html.2ce3356f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_blip-3o.html.20d6a0b5.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_explaining-tokens-the-language-and-currency-of-ai-nvidia-blog.html.98d7044c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_025.html.828c6af4.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_027.html.a69f4cf8.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_hf-weekly_004.html.e20ccdbc.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_framepack.html.79cc8785.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_028.html.23f08d5c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_024.html.c9d95ff5.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_013.html.659095e1.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_vlv.html.66e5a328.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_hf-weekly_002.html.475e8a71.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_019.html.dbd20518.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_illustrious-xl-v2.0-the-best-training-base-model-in-1536-age.html.b5e190f7.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_illustrious-xl-v2.0-the-best-training-base-model-in-1536-age.html.824cc0cb.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_explaining-tokens-the-language-and-currency-of-ai-nvidia-blog.html.0dce2f26.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_017.html.cf727d63.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_niji-lesson-2-the-terminator-line.html.bcba9d63.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_niji-study-2-notan.html.bc893745.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_ovis-u1.html.f00080b2.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_016.html.c3ada0a7.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_mcp-flash-in-the-pan-or-future-standard.html.b142e62d.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_papers_checklist.html.153167b0.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_008.html.8b9058fe.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_002.html.5a4dd3d5.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_niji-video.html.211bd024.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_mcp-flash-in-the-pan-or-future-standard.html.19ff25fe.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_004.html.9173a01b.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_niji-video.html.26560c68.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_007.html.eab02414.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_niji-study-2-notan.html.70b1ed6f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_3steps-paper-reading.html.eda5237e.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_dairys_250223.html.b6746695.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_experiments-with-mcp-using-github-copilot.html.b89c4005.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_experimenting-with-mcp-using-github-copilot.html.f9a39fff.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_illustrious-lu-v0.03.html.dbc0dce0.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_illustrious-lu-v0.03.html.8b95f0b0.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_templates_papers.html.31aef926.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_templates_hf-weekly.html.5c334dac.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_hf-weekly_checklist.html.c3fa5a04.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_templates_ai-weekly.html.61881c27.js" as="script"><link rel="prefetch" href="/assets/js/zh_demo_page.html.0b7cffcc.js" as="script"><link rel="prefetch" href="/assets/js/demo_page.html.50e1e3b3.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_hf-weekly.prompt.html.a49236f9.js" as="script"><link rel="prefetch" href="/assets/js/zh_demo_layout.html.9b3fdc7f.js" as="script"><link rel="prefetch" href="/assets/js/demo_layout.html.25c357de.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_hf-weekly_workflow.html.da6dc285.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_translate.prompt.html.0e65a605.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_cover.prompt.html.57daed61.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_ai-weekly.prompt.html.2fe1eb8f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_papers.prompt.html.ba28b7ce.js" as="script"><link rel="prefetch" href="/assets/js/home.html.a72e8bb4.js" as="script"><link rel="prefetch" href="/assets/js/zh_home.html.7623eb73.js" as="script"><link rel="prefetch" href="/assets/js/demo_disable.html.aeb75044.js" as="script"><link rel="prefetch" href="/assets/js/zh_demo_disable.html.917844f9.js" as="script"><link rel="prefetch" href="/assets/js/zh_intro.html.efed7168.js" as="script"><link rel="prefetch" href="/assets/js/intro.html.50f0afc5.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_image-extract.prompt.html.9c9b8aa7.js" as="script"><link rel="prefetch" href="/assets/js/zh_index.html.40d3a237.js" as="script"><link rel="prefetch" href="/assets/js/index.html.f45e7c7b.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_web_vue-1.html.0755e363.js" as="script"><link rel="prefetch" href="/assets/js/zh_demo_index.html.34792277.js" as="script"><link rel="prefetch" href="/assets/js/demo_encrypt.html.25bd9519.js" as="script"><link rel="prefetch" href="/assets/js/zh_demo_encrypt.html.06a96e54.js" as="script"><link rel="prefetch" href="/assets/js/tag_artificial-intelligence_index.html.7e3793fa.js" as="script"><link rel="prefetch" href="/assets/js/demo_index.html.febe17c4.js" as="script"><link rel="prefetch" href="/assets/js/category_model-development_index.html.794f5d59.js" as="script"><link rel="prefetch" href="/assets/js/category_image-generation_index.html.2189f40e.js" as="script"><link rel="prefetch" href="/assets/js/category_model-training_index.html.be238a49.js" as="script"><link rel="prefetch" href="/assets/js/category_generative-ai_index.html.b909fb8b.js" as="script"><link rel="prefetch" href="/assets/js/tag_stable-diffusion_index.html.ae5f1c64.js" as="script"><link rel="prefetch" href="/assets/js/category_anime-style_index.html.3108accc.js" as="script"><link rel="prefetch" href="/assets/js/tag_amazon-bedrock_index.html.a67f2776.js" as="script"><link rel="prefetch" href="/assets/js/tag_resource-guide_index.html.5acb26ee.js" as="script"><link rel="prefetch" href="/assets/js/category_explainer_index.html.a8e8f462.js" as="script"><link rel="prefetch" href="/assets/js/category_reprints_index.html.4a06de7a.js" as="script"><link rel="prefetch" href="/assets/js/tag_kohya-ss-gui_index.html.27638eba.js" as="script"><link rel="prefetch" href="/assets/js/tag_fundamentals_index.html.374e901f.js" as="script"><link rel="prefetch" href="/assets/js/category_reprint_index.html.6eafee8b.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_模型上下文协议_index.html.e75af900.js" as="script"><link rel="prefetch" href="/assets/js/tag_illustrious_index.html.ff354070.js" as="script"><link rel="prefetch" href="/assets/js/tag_text2image_index.html.bb9c3a46.js" as="script"><link rel="prefetch" href="/assets/js/tag_taylorseer_index.html.d4b32963.js" as="script"><link rel="prefetch" href="/assets/js/category_aiml_index.html.35fc99fb.js" as="script"><link rel="prefetch" href="/assets/js/tag_diffusers_index.html.8f93bf3f.js" as="script"><link rel="prefetch" href="/assets/js/tag_inference_index.html.b69c0933.js" as="script"><link rel="prefetch" href="/assets/js/tag_replicate_index.html.61deee89.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_阿里巴巴ai研究_index.html.dd5f50e2.js" as="script"><link rel="prefetch" href="/assets/js/tag_markdown_index.html.ec598c41.js" as="script"><link rel="prefetch" href="/assets/js/tag_drawing_index.html.91b0ebb5.js" as="script"><link rel="prefetch" href="/assets/js/tag_flux.1_index.html.83ab9654.js" as="script"><link rel="prefetch" href="/assets/js/tag_lumina_index.html.27a09c4b.js" as="script"><link rel="prefetch" href="/assets/js/tag_prompt_index.html.d3dbd2e7.js" as="script"><link rel="prefetch" href="/assets/js/tag_script_index.html.eae6815c.js" as="script"><link rel="prefetch" href="/assets/js/tag_merge_index.html.ae63ce39.js" as="script"><link rel="prefetch" href="/assets/js/tag_model_index.html.acdbe3e4.js" as="script"><link rel="prefetch" href="/assets/js/tag_qlora_index.html.11905a67.js" as="script"><link rel="prefetch" href="/assets/js/tag_qwen3_index.html.e8eb5022.js" as="script"><link rel="prefetch" href="/assets/js/tag_vpred_index.html.426ee337.js" as="script"><link rel="prefetch" href="/assets/js/tag_notan_index.html.d9a4a654.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_创造者工坊_index.html.094f4c96.js" as="script"><link rel="prefetch" href="/assets/js/tag_flux_index.html.ad3867cf.js" as="script"><link rel="prefetch" href="/assets/js/tag_llms_index.html.67431ae9.js" as="script"><link rel="prefetch" href="/assets/js/tag_lora_index.html.c7ee546c.js" as="script"><link rel="prefetch" href="/assets/js/tag_qwen_index.html.e0838b98.js" as="script"><link rel="prefetch" href="/assets/js/tag_sdxl_index.html.37a19ff3.js" as="script"><link rel="prefetch" href="/assets/js/tag_niji_index.html.a70a9af6.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_大语言模型_index.html.72664ff8.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_计算机视觉_index.html.58b39887.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_输入预处理_index.html.ed9bbf63.js" as="script"><link rel="prefetch" href="/assets/js/tag_art_index.html.fd5de43a.js" as="script"><link rel="prefetch" href="/assets/js/tag_aws_index.html.49d0379d.js" as="script"><link rel="prefetch" href="/assets/js/tag_llm_index.html.3a8069a7.js" as="script"><link rel="prefetch" href="/assets/js/tag_mcp_index.html.5e93604a.js" as="script"><link rel="prefetch" href="/assets/js/tag_gtc_index.html.1e06b509.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_artificial-intelligence_index.html.8cfd6b09.js" as="script"><link rel="prefetch" href="/assets/js/category_index.html.30915709.js" as="script"><link rel="prefetch" href="/assets/js/tag_ai_index.html.691e8ede.js" as="script"><link rel="prefetch" href="/assets/js/tag_lu_index.html.f7da3bcc.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_llm基准测试_index.html.fc9da582.js" as="script"><link rel="prefetch" href="/assets/js/timeline_index.html.3f867224.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_阿里巴巴ai_index.html.c970b0e2.js" as="script"><link rel="prefetch" href="/assets/js/article_index.html.3131e8c3.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_使用指南_index.html.63c177db.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_图像生成_index.html.c3ae9a7c.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_教程指南_index.html.42c5df88.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_模型开发_index.html.185a36fd.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_模型研发_index.html.f6ab3ec2.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_模型训练_index.html.1785036a.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_视频生成_index.html.60de099e.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_论文精读_index.html.7276ab7f.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_动漫风格_index.html.8538e3dd.js" as="script"><link rel="prefetch" href="/assets/js/tag_model-context-protocol_index.html.25f141b3.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_index.html.8158d3a4.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ielts_usage-of-collocations-in-speaking_index.html.b2df067f.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_workflow-generation_index.html.4c431f19.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_人工智能_index.html.f0a2e682.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_使用指南_index.html.fe8c29d0.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_功能发布_index.html.d6a89296.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_反向传播_index.html.ac57d1ff.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_可控生成_index.html.142da7e4.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_图像生成_index.html.dfb3e561.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_图像识别_index.html.9bff49b9.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_基础模型_index.html.7953d298.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_开源项目_index.html.e3cfbec0.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_扩散模型_index.html.829f3a99.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_技术教程_index.html.9ee8291c.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_游戏开发_index.html.6e767c46.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_艺术课程_index.html.46238e22.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_视频生成_index.html.8068090f.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_计算优化_index.html.4e81f12a.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_阿里巴巴_index.html.df377e00.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_页面配置_index.html.c7985dc6.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_model-development_index.html.464b624b.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_机器学习_index.html.fb0ee906.js" as="script"><link rel="prefetch" href="/assets/js/tag_large-language-models_index.html.3bcf45b0.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_2048分辨率_index.html.6fb915df.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_生成式ai_index.html.946ab634.js" as="script"><link rel="prefetch" href="/assets/js/tag_feature-announcement_index.html.5aa6fb5c.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_image-generation_index.html.b26f5abb.js" as="script"><link rel="prefetch" href="/assets/js/star_index.html.0ffd513d.js" as="script"><link rel="prefetch" href="/assets/js/tag_index.html.fc1f1e0b.js" as="script"><link rel="prefetch" href="/assets/js/tag_alibaba-ai-research_index.html.adbbd683.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_epsilon预测_index.html.93831d46.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_多模态ai_index.html.f345a0db.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_多语言ai_index.html.fe70d6af.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_stable-diffusion_index.html.a72cbefd.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_生成式ai_index.html.5736baf2.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_初学者_index.html.f5485ca2.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_张吕敏_index.html.baa96ea6.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_model-training_index.html.7e4d037d.js" as="script"><link rel="prefetch" href="/assets/js/tag_epsilon-prediction_index.html.28d2e698.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_reasoning-model_index.html.1a71ba78.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_generative-ai_index.html.82923755.js" as="script"><link rel="prefetch" href="/assets/js/tag_image-recognition_index.html.a2fa1118.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_公众号_index.html.08bf5712.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_创作者_index.html.e962922e.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_多模态_index.html.0da8de43.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_编辑器_index.html.9af3b2cb.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_amazon-bedrock_index.html.8cef35a6.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_resource-guide_index.html.ea147e91.js" as="script"><link rel="prefetch" href="/assets/js/posts_index.html.efc2f348.js" as="script"><link rel="prefetch" href="/assets/js/tag_game-development_index.html.57d177bb.js" as="script"><link rel="prefetch" href="/assets/js/tag_image-generation_index.html.8b92bc54.js" as="script"><link rel="prefetch" href="/assets/js/tag_machine-learning_index.html.faec0f3f.js" as="script"><link rel="prefetch" href="/assets/js/tag_visual-hierarchy_index.html.a2caed5c.js" as="script"><link rel="prefetch" href="/assets/js/tag_diffusion-models_index.html.93147de9.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_anime-style_index.html.6dcca799.js" as="script"><link rel="prefetch" href="/assets/js/tag_2048-resolution_index.html.f3714f53.js" as="script"><link rel="prefetch" href="/assets/js/tag_computer-vision_index.html.16784659.js" as="script"><link rel="prefetch" href="/assets/js/tag_multilingual-ai_index.html.366e07d4.js" as="script"><link rel="prefetch" href="/assets/js/tag_stablediffusion_index.html.4cefc3f4.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_kohya-ss-gui_index.html.41ea145e.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_fundamentals_index.html.a2abb31f.js" as="script"><link rel="prefetch" href="/assets/js/tag_llm-benchmarks_index.html.a4c66890.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_ai推理_index.html.e5d2b110.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_ai模型_index.html.3bce6dfb.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_ai生成_index.html.7ea51825.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_新闻_index.html.0c095e7c.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_日记_index.html.aa249f5e.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_论文_index.html.bf9a7705.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_高级_index.html.b102e135.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_illustrious_index.html.1cf7c511.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_思考_index.html.cf92ff59.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_指南_index.html.e374501e.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_转载_index.html.5c33d05d.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_explainer_index.html.e16f7b83.js" as="script"><link rel="prefetch" href="/assets/js/tag_automatic1111_index.html.96f5b2d6.js" as="script"><link rel="prefetch" href="/assets/js/tag_generative-ai_index.html.ac7fe732.js" as="script"><link rel="prefetch" href="/assets/js/tag_multimodal-ai_index.html.8e502dbb.js" as="script"><link rel="prefetch" href="/assets/js/tag_agent-systems_index.html.cea033f6.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_text2image_index.html.bc1c311b.js" as="script"><link rel="prefetch" href="/assets/js/category_advanced_index.html.b0b65b54.js" as="script"><link rel="prefetch" href="/assets/js/category_ai-tools_index.html.2197e21d.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_taylorseer_index.html.46aded77.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_reprints_index.html.0826fe61.js" as="script"><link rel="prefetch" href="/assets/js/tag_quantization_index.html.d1270720.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_优化_index.html.33627e19.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_做饭_index.html.14d1e2ca.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_内容_index.html.bf4dd66c.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_创新_index.html.3233a6ef.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_加密_index.html.bfaa97bf.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_协议_index.html.d79c2e92.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_大会_index.html.671b60e0.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_平台_index.html.1473d3b1.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_开源_index.html.68009a9f.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_微调_index.html.00e98616.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_技术_index.html.7b9b8310.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_禁用_index.html.09d49f84.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_艺术_index.html.f039e09a.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_运营_index.html.01477a7c.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_随想_index.html.2130074f.js" as="script"><link rel="prefetch" href="/assets/js/tag_ai-reasoning_index.html.92107949.js" as="script"><link rel="prefetch" href="/assets/js/tag_optimization_index.html.91e14203.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_动漫_index.html.18233701.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_实习_index.html.99c01886.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_布局_index.html.c4128b34.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_训练_index.html.4a46a86c.js" as="script"><link rel="prefetch" href="/assets/js/tag_modelmerging_index.html.3aac36a9.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_辩论_index.html.08c79452.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_量化_index.html.9cd36f77.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_comfymind_index.html.780a1f96.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_framepack_index.html.316e41ca.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_inference_index.html.f694cf8d.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_replicate_index.html.01cc7a53.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_diffusers_index.html.16bfc806.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_reprint_index.html.03c0293c.js" as="script"><link rel="prefetch" href="/assets/js/tag_fine-tuning_index.html.ca8a4059.js" as="script"><link rel="prefetch" href="/assets/js/tag_news-agents_index.html.39bd1dd8.js" as="script"><link rel="prefetch" href="/assets/js/tag_page-config_index.html.fd4f47e9.js" as="script"><link rel="prefetch" href="/assets/js/tag_usage-guide_index.html.069580af.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_markdown_index.html.4742eec1.js" as="script"><link rel="prefetch" href="/assets/js/category_novice_index.html.7561c64e.js" as="script"><link rel="prefetch" href="/assets/js/category_papers_index.html.180b7727.js" as="script"><link rel="prefetch" href="/assets/js/tag_alibaba-ai_index.html.f5c3b72c.js" as="script"><link rel="prefetch" href="/assets/js/tag_art-lesson_index.html.6dc9ccf1.js" as="script"><link rel="prefetch" href="/assets/js/tag_base-model_index.html.3c68635b.js" as="script"><link rel="prefetch" href="/assets/js/tag_encryption_index.html.ba9c6633.js" as="script"><link rel="prefetch" href="/assets/js/tag_modelmerge_index.html.f79dfcbf.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_drawing_index.html.d6ef8b08.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_ovis-u1_index.html.aab779ab.js" as="script"><link rel="prefetch" href="/assets/js/category_guide_index.html.b6dbfe00.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_comfyui_index.html.52422e18.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_aiml_index.html.5317281e.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_flux.1_index.html.703ceaab.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_github_index.html.eacf79c2.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_lumina_index.html.451683ad.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_prompt_index.html.dbf5dc4c.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_script_index.html.0c284078.js" as="script"><link rel="prefetch" href="/assets/js/category_news_index.html.0acac10a.js" as="script"><link rel="prefetch" href="/assets/js/tag_ai-model_index.html.485c27f3.js" as="script"><link rel="prefetch" href="/assets/js/tag_amazon-q_index.html.9b480256.js" as="script"><link rel="prefetch" href="/assets/js/tag_creators_index.html.1bc91546.js" as="script"><link rel="prefetch" href="/assets/js/tag_protocol_index.html.715bc788.js" as="script"><link rel="prefetch" href="/assets/js/tag_training_index.html.3c2840b0.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_merge_index.html.9e5cebd3.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_model_index.html.c5e77c69.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_notan_index.html.d1fbfa1b.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_qlora_index.html.7832f634.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_qwen3_index.html.e94caed5.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_vpred_index.html.557ab6de.js" as="script"><link rel="prefetch" href="/assets/js/tag_disable_index.html.6ed89a53.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_aigc_index.html.51eda4be.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_flux_index.html.5425d666.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_llms_index.html.22e104c5.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_niji_index.html.697abc36.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_lora_index.html.eb2cf7fa.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_sdxl_index.html.aa7ab8d6.js" as="script"><link rel="prefetch" href="/assets/js/404.html.683d9275.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_qwen_index.html.ac1dad5c.js" as="script"><link rel="prefetch" href="/assets/js/tag_debate_index.html.347d53e0.js" as="script"><link rel="prefetch" href="/assets/js/tag_layout_index.html.5a5e41d2.js" as="script"><link rel="prefetch" href="/assets/js/tag_editor_index.html.2106f4d9.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_art_index.html.8deb8f79.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_aws_index.html.defb07a5.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_gtc_index.html.1f7cf941.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_llm_index.html.cdc71de0.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_mcp_index.html.2baec1bd.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_sdo_index.html.c2389a8d.js" as="script"><link rel="prefetch" href="/assets/js/tag_anime_index.html.a8a78e3b.js" as="script"><link rel="prefetch" href="/assets/js/tag_guide_index.html.5173996f.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_lu_index.html.42e4b29c.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_ai_index.html.5a7d4e11.js" as="script"><link rel="prefetch" href="/assets/js/tag_tech_index.html.68c57a8c.js" as="script"><link rel="prefetch" href="/assets/js/tag_tmux_index.html.569017df.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_hf-weekly_index.html.06e8093d.js" as="script"><link rel="prefetch" href="/assets/js/zh_timeline_index.html.e1f1ad46.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_index.html.682e7f3d.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_tutorials_comfyui_index.html.9ed7a546.js" as="script"><link rel="prefetch" href="/assets/js/zh_article_index.html.cd7570ad.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_papers_index.html.bb25e136.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_index.html.6f900118.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_hf-weekly_index.html.4d97c51d.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_templates_index.html.5a13aa05.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_tutorials_index.html.5b1c58de.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_index.html.2bfde6cd.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_index.html.3201a8c7.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_repos_index.html.d93bee28.js" as="script"><link rel="prefetch" href="/assets/js/zh_star_index.html.f6186c0f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-impls_index.html.dc9f8315.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_index.html.9a49e407.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_thoughts_index.html.e1e18711.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_index.html.e3e3d99c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_index.html.5e8ec60f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_dairys_index.html.6cd7d11b.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ielts_index.html.727f9a48.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_repos_index.html.ce9635f7.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_index.html.c78cbce3.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_web_index.html.6fe04daf.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_sci_index.html.e71977aa.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/" aria-label="Take me home"><img class="vp-nav-logo" src="/logo.svg" alt><!----><span class="vp-site-name hide-in-pad">Nlog</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/home.html" aria-label="home"><!---->home<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/demo/" aria-label="Features demo"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="laptop-code" width="1em" height="1em"></iconify-icon><!--]-->Features demo<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Posts"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon>Posts<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Apple</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/apple/1.html" aria-label="Apple1"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon><!--]-->Apple1<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/apple/2.html" aria-label="Apple2"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon><!--]-->Apple2<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/apple/3.html" aria-label="/posts/apple/3.html"><!---->/posts/apple/3.html<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/apple/4.html" aria-label="/posts/apple/4.html"><!---->/posts/apple/4.html<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Banana</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/banana/1.html" aria-label="Banana 1"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon><!--]-->Banana 1<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/banana/2.html" aria-label="Banana 2"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon><!--]-->Banana 2<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/banana/3.html" aria-label="/posts/banana/3.html"><!---->/posts/banana/3.html<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/banana/4.html" aria-label="/posts/banana/4.html"><!---->/posts/banana/4.html<!----></a></li></ul></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/posts/cherry.html" aria-label="Cherry"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon><!--]-->Cherry<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/posts/dragonfruit.html" aria-label="Dragon Fruit"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon><!--]-->Dragon Fruit<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/posts/tomato.html" aria-label="/posts/tomato.html"><!---->/posts/tomato.html<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/posts/strawberry.html" aria-label="/posts/strawberry.html"><!---->/posts/strawberry.html<!----></a></li></ul></button></div></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><div class="vp-nav-item"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Select language"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" name="i18n" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link route-link-active auto-link" href="/posts/reprints/image-recognition.html" aria-label="English"><!---->English<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/zh/posts/reprints/image-recognition.html" aria-label="简体中文"><!---->简体中文<!----></a></li></ul></button></div></div><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/vuepress-theme-hope/vuepress-theme-hope" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/" aria-label="Blog Home"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="home" width="1em" height="1em"></iconify-icon><!--]-->Blog Home<!----></a></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="laptop-code" width="1em" height="1em"></iconify-icon><a class="route-link auto-link vp-sidebar-title no-external-link-icon" href="/demo/" aria-label="Demo"><!---->Demo<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/demo/layout.html" aria-label="Layout"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="object-group" width="1em" height="1em"></iconify-icon><!--]-->Layout<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/demo/markdown.html" aria-label="Markdown Enhance"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="fab fa-markdown" width="1em" height="1em"></iconify-icon><!--]-->Markdown Enhance<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/demo/page.html" aria-label="Page Config"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="file" width="1em" height="1em"></iconify-icon><!--]-->Page Config<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/demo/disable.html" aria-label="Disabling layout and features"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="gears" width="1em" height="1em"></iconify-icon><!--]-->Disabling layout and features<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/demo/encrypt.html" aria-label="Encryption Article"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="lock" width="1em" height="1em"></iconify-icon><!--]-->Encryption Article<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header active"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="book" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">Articles</span><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">Reprints</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/generative-ai-powered-design.html" aria-label="Generative AI-Powered Design: Creating Game Environments with SD3.5 Large"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="openmoji:video-game" width="1em" height="1em"></iconify-icon><!--]-->Generative AI-Powered Design: Creating Game Environments with SD3.5 Large<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything.html" aria-label="Lesson 1: Fundamentals of Measurement and Abstraction: The Theory of (How to Draw) Everything"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="palette" width="1em" height="1em"></iconify-icon><!--]-->Lesson 1: Fundamentals of Measurement and Abstraction: The Theory of (How to Draw) Everything<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/mcp-flash-in-the-pan-or-future-standard.html" aria-label="MCP: Flash in the Pan or Future Standard?"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="openmoji:code-editor" width="1em" height="1em"></iconify-icon><!--]-->MCP: Flash in the Pan or Future Standard?<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/niji-lesson-2-the-terminator-line.html" aria-label="Lesson 2: The Terminator (Line)"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="palette" width="1em" height="1em"></iconify-icon><!--]-->Lesson 2: The Terminator (Line)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/niji-study-1-measuring-with-your-eyes.html" aria-label="Study 1: Measuring With Your Eyes"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="ruler" width="1em" height="1em"></iconify-icon><!--]-->Study 1: Measuring With Your Eyes<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/niji-study-2-notan.html" aria-label="Study 2: Notan"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="contrast" width="1em" height="1em"></iconify-icon><!--]-->Study 2: Notan<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/flux-qlora.html" aria-label="(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware"><!---->(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/model-block-merge-1.html" aria-label="[Experiment Report] Investigating the Influence of Each U-Net Layer with Model Block Merge #1"><!---->[Experiment Report] Investigating the Influence of Each U-Net Layer with Model Block Merge #1<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/model-block-merge-2.html" aria-label="[Experiment Report] Investigating the Influence of Each U-Net Layer with Model Block Merge #1"><!---->[Experiment Report] Investigating the Influence of Each U-Net Layer with Model Block Merge #1<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/announcing-illustrious-text%E2%80%91enhancer-tag-booster-and-mood-enhancer.html" aria-label="Announcing Illustrious Text‑Enhancer: Tag Booster &amp; Mood Enhancer"><!---->Announcing Illustrious Text‑Enhancer: Tag Booster &amp; Mood Enhancer<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/ai-art-gtc-paris-2025.html" aria-label="Artists, Fashion Designers Tap State-of-the-Art AI for NVIDIA GTC Paris Gallery"><!---->Artists, Fashion Designers Tap State-of-the-Art AI for NVIDIA GTC Paris Gallery<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/blog-images.html" aria-label="Blog Images Generator"><!---->Blog Images Generator<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/news-agents-daily-recap.html" aria-label="Building News Agents for Daily News Recaps with MCP, Q, and tmux"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="fa-solid:newspaper" width="1em" height="1em"></iconify-icon><!--]-->Building News Agents for Daily News Recaps with MCP, Q, and tmux<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/crody&#39;s-model-merge-guide.html" aria-label="Crody&#39;s Model Merge Guide // Team-C"><!---->Crody&#39;s Model Merge Guide // Team-C<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/experimenting-with-mcp-using-github-copilot.html" aria-label="Experimenting with MCP using GitHub Copilot"><!---->Experimenting with MCP using GitHub Copilot<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/explaining-tokens-the-language-and-currency-of-ai-nvidia-blog.html" aria-label="Explaining Tokens — the Language and Currency of AI"><!---->Explaining Tokens — the Language and Currency of AI<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/how-and-when-to-build-multi-agent-systems.html" aria-label="How and when to build multi-agent systems"><!---->How and when to build multi-agent systems<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/original-character-lora-sdxl-character-training.html" aria-label="How to create an original character LoRA [SDXL Training] SDXL Character Training"><!---->How to create an original character LoRA [SDXL Training] SDXL Character Training<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/niji-video.html" aria-label="How to make videos with niji・journey"><!---->How to make videos with niji・journey<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/flux-kontext-optimization.html" aria-label="How we optimized FLUX.1 Kontext [dev]"><!---->How we optimized FLUX.1 Kontext [dev]<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.html" aria-label="Illustrious XL 3.0-3.5-vpred: 2048 Resolution and Natural Language"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="mdi:paint-outline" width="1em" height="1em"></iconify-icon><!--]-->Illustrious XL 3.0-3.5-vpred: 2048 Resolution and Natural Language<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/illustrious-xl-v2.0-the-best-training-base-model-in-1536-age.html" aria-label="Illustrious XL v2.0—The best training base model in 1536 age"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="mdi:paint-outline" width="1em" height="1em"></iconify-icon><!--]-->Illustrious XL v2.0—The best training base model in 1536 age<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/illustrious-lu-v0.03.html" aria-label="Illustrious-LU v0.03"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="fa-solid:microscope" width="1em" height="1em"></iconify-icon><!--]-->Illustrious-LU v0.03<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/flux-1-kontext.html" aria-label="Introducing FLUX.1 Kontext and the BFL Playground"><!---->Introducing FLUX.1 Kontext and the BFL Playground<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression.html" aria-label="Introduction of prompts in AI illustration generation [composition / camera angle / facial expression]"><!---->Introduction of prompts in AI illustration generation [composition / camera angle / facial expression]<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview.html" aria-label="Qwen3: Next-Gen AI with Hybrid Thinking and Multilingual Mastery | 2025 Overview"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="fa-solid:robot" width="1em" height="1em"></iconify-icon><!--]-->Qwen3: Next-Gen AI with Hybrid Thinking and Multilingual Mastery | 2025 Overview<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.html" aria-label="Step by Step visual introduction to Diffusion Models"><!---->Step by Step visual introduction to Diffusion Models<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/ai-art-newsletter-jan-25.html" aria-label="The AI tools for Art Newsletter - Issue 1"><!---->The AI tools for Art Newsletter - Issue 1<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/what-is-block-merging.html" aria-label="What is Block Merging?"><!---->What is Block Merging?<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/posts/reprints/image-recognition.html" aria-label="What Is Image Recognition? Algorithms and Applications"><!---->What Is Image Recognition? Algorithms and Applications<!----></a></li></ul></section></li></ul></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/intro.html" aria-label="Intro Page"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="circle-info" width="1em" height="1em"></iconify-icon><!--]-->Intro Page<!----></a></li><li><a class="auto-link external-link vp-sidebar-link" href="https://ecosystem.vuejs.press/plugins/markdown/revealjs/demo.html" aria-label="Slides" rel="noopener noreferrer" target="_blank"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="person-chalkboard" width="1em" height="1em"></iconify-icon><!--]-->Slides<!----></a></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->What Is Image Recognition? Algorithms and Applications</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Timothy M.</span></span><span property="author" content="Timothy M."></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">June 10, 2025</span><meta property="datePublished" content="2025-06-10T17:51:41.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 14 min</span><meta property="timeRequired" content="PT14M"></span><!----><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color2 clickable" role="navigation">Computer Vision</span><span class="page-tag-item color7 clickable" role="navigation">Image Recognition</span><!--]--><meta property="keywords" content="Computer Vision,Image Recognition"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#how-does-image-recognition-work">How Does Image Recognition Work?</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#step-1-pixels-the-computer-s-vision-language">Step #1: Pixels (The Computer&#39;s Vision Language)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#step-2-feature-detection-finding-patterns">Step #2: Feature Detection (Finding Patterns)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#step-3-learning-from-examples-training">Step #3: Learning From Examples (Training)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#step-4-classification-prediction-time">Step #4: Classification (Prediction Time!)</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#types-of-image-recognition-models-in-ai">Types of Image Recognition Models in AI</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#image-classification-models">Image Classification Models</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#object-detection-models">Object Detection Models</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#instance-semantic-segmentation-models">Instance &amp; Semantic Segmentation Models</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#keypoint-detection-pose-estimation-models">Keypoint Detection &amp; Pose Estimation Models</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#face-detection-recognition">Face Detection &amp; Recognition</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#vision-language-models-vlms">Vision-Language Models (VLMs)</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#how-to-use-image-recognition-ai-using-roboflow">How to Use Image Recognition AI using Roboflow</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#step-1-create-a-project">Step #1: Create a Project</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#step-2-upload-your-dataset">Step #2: Upload Your Dataset</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#step-3-annotate-images">Step #3: Annotate Images</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#step-4-preprocess-augment-your-data">Step #4: Preprocess &amp; Augment Your Data</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#step-5-generate-dataset">Step #5: Generate Dataset</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#step-6-train-a-model">Step #6: Train a Model</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#step-7-deploy-your-model">Step #7: Deploy Your Model</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#how-roboflow-workflows-can-be-used-for-image-recognition">How Roboflow Workflows Can Be Used for Image Recognition</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#using-pre-trained-models-in-roboflow">Using pre-trained models in Roboflow</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#using-trained-or-fine-tuned-custom-models-with-your-data">Using trained or fine-tuned custom models with your data</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#building-image-recognition-ai-with-roboflow">Building Image Recognition AI with Roboflow</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#example-1-detecting-and-counting-wood-log">Example #1: Detecting and counting wood/log</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#example-2-recognizing-hand-gestures">Example #2: Recognizing Hand Gestures</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#example-3-vlm-for-image-recognition">Example #3: VLM for Image Recognition</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#image-recognition-software">Image Recognition Software</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#cite-this-post">Cite this Post</a></li><!----><!--]--></ul></li><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content" vp-content><h1 id="what-is-image-recognition-algorithms-and-applications" tabindex="-1"><a class="header-anchor" href="#what-is-image-recognition-algorithms-and-applications"><span>What Is Image Recognition? Algorithms and Applications</span></a></h1><figure><img src="https://blog.roboflow.com/content/images/size/w1200/2025/06/Screenshot-2025-06-10-at-10.46.38---AM.png" alt="What Is Image Recognition" tabindex="0" loading="lazy"><figcaption>What Is Image Recognition</figcaption></figure><p>Imagine a young girl named Emma who is fascinated by birds. Every weekend, she visits a nearby park to watch birds with her grandfather. Over time, Emma learns to recognize different bird species by their color, size, shape, and even their chirps. One afternoon, while flipping through a book, she effortlessly points to a picture and says, &quot;Look, Grandpa! It&#39;s a robin!&quot; She doesn&#39;t measure wingspans or analyze feather types; her brain instantly connects the image to her experiences and memories of robins at the park.</p><p>This natural human ability (to look, understand, and identify) let&#39;s human to see things and recognize it. But what if we want computers to do the same? This is exactly what image recognition aims to achieve.</p><p>Image recognition is a computer vision task that enables machines to interpret and identify objects, people, places, and actions in images. It mimics the human ability to understand visual data by analyzing patterns, shapes, and features in digital images.</p><p>At its core, image recognition works by analyzing the pixels of an image and identifying patterns. This is achieved through complex algorithms, most notably a type of computational model called neural network. These neural networks are inspired by the human brain&#39;s visual cortex and are trained on massive datasets of labeled images. By processing these images, the neural networks learns to recognize the features and characteristics of different objects.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/image_recognition.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>How computers recognize images</em></p><h2 id="how-does-image-recognition-work" tabindex="-1"><a class="header-anchor" href="#how-does-image-recognition-work"><span>How Does Image Recognition Work?</span></a></h2><p>To humans, seeing an image means instantly recognizing familiar shapes, colors, or people. But for a computer, seeing is completely different, images are just numbers. Here&#39;s a step-by-step explanation of how a computer sees an image</p><h3 id="step-1-pixels-the-computer-s-vision-language" tabindex="-1"><a class="header-anchor" href="#step-1-pixels-the-computer-s-vision-language"><span>Step #1: Pixels (The Computer&#39;s Vision Language)</span></a></h3><p>Every image is made up of tiny dots called pixels. Each pixel holds a numerical value that represents a color or shade. For example, a grayscale image is a 2D grid of numbers (as shown below), where each number ranges from 0 (black) to 255 (white). A colored image has three channels Red, Green, and Blue (RGB), making it a 3D array.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/grayscale_image_representation.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Grayscale image representation</em></p><figure><img src="https://blog.roboflow.com/content/images/2025/06/rgb_image_representation.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Color image representation</em></p><h3 id="step-2-feature-detection-finding-patterns" tabindex="-1"><a class="header-anchor" href="#step-2-feature-detection-finding-patterns"><span>Step #2: Feature Detection (Finding Patterns)</span></a></h3><p>Once the image is converted into numbers, the computer looks for patterns like Edges, Corners &amp; Junctions, Lines and Curves, Shapes, Textures etc. It uses filters or mathematical operations (like convolution etc.) to detect these features automatically.</p><h3 id="step-3-learning-from-examples-training" tabindex="-1"><a class="header-anchor" href="#step-3-learning-from-examples-training"><span>Step #3: Learning From Examples (Training)</span></a></h3><p>The computer is shown thousands of labeled images (like cats, cars, or birds). During training. It learns which features are common for each object type. It stores this information in a neural network, a model that acts like a simplified brain.</p><h3 id="step-4-classification-prediction-time" tabindex="-1"><a class="header-anchor" href="#step-4-classification-prediction-time"><span>Step #4: Classification (Prediction Time!)</span></a></h3><p>When a new image is shown, the computer analyzes the pixels, detects features, compares them with what it has learned and outputs a label (e.g., &quot;raccoon&quot;) with a confidence score (e.g., 94.7%).</p><p>Let&#39;s understand this whole process through <a href="https://blog.roboflow.com/what-is-r-cnn/" target="_blank" rel="noopener noreferrer">R-CNN</a> architecture. The R-CNN architecture diagram below illustrates how a computer sees and recognizes objects in an image using the R-CNN (Region-based Convolutional Neural Network) method. In step 1, the computer receives the input image, which is internally represented as a grid of pixel values. In step 2, rather than analyzing the whole image at once, R-CNN generates about 2,000 region proposals, smaller parts of the image that might contain an object. These regions are then warped to a fixed size and passed through a CNN in step 3, which applies a series of mathematical operations like convolution, pooling, and non-linear activation to extract distinctive features (such as edges, textures, or patterns). Finally, in step 4, each region&#39;s extracted features are classified using a classifier (like SVM), answering questions like &quot;Is this a person?&quot; or &quot;Is this a tv monitor?&quot; The process shows how a computer doesn&#39;t understand the whole image at once, instead, it breaks it into parts, looks for meaningful patterns, and uses learned data to recognize what&#39;s inside, mimicking the way humans visually scan scenes to identify familiar objects.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/r_cnn.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><a href="https://arxiv.org/pdf/1311.2524?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer"><em>R-CNN</em></a><em>Architecture</em></p><p>A computer does not see images as pictures, it sees grids of numbers. With machine learning, especially deep learning, it learns to recognize patterns in those numbers and match them to known objects. Just like a child learns to recognize a dog after seeing many dogs, the computer learns from data to &quot;see&quot; the world.</p><h2 id="types-of-image-recognition-models-in-ai" tabindex="-1"><a class="header-anchor" href="#types-of-image-recognition-models-in-ai"><span>Types of Image Recognition Models in AI</span></a></h2><p>Image recognition has wide application with different types of models, each designed for specific tasks such as classification, detection, segmentation, face recognition, or keypoint estimation.</p><h3 id="image-classification-models" tabindex="-1"><a class="header-anchor" href="#image-classification-models"><span>Image Classification Models</span></a></h3><p>Image classification assigns a single label to an entire image. The model looks at the image and predicts what object (or class) it contains, like &quot;cat,&quot; &quot;car,&quot; or &quot;banana.&quot;. These models assigns a single label to the entire image and are used for recognizing the dominant object or scene in an image (e.g., &quot;cat,&quot; &quot;airplane,&quot; &quot;fractured bone&quot;).</p><p><strong>Examples</strong>:</p><ul><li><a href="https://roboflow.com/model/resnet-32?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">ResNet 32</a>/ <a href="https://roboflow.com/model/resnet-50?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">ResNet-50</a>: Uses residual connections to solve deep network degradation.</li><li><a href="https://roboflow.com/model/vision-transformer?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Vision Transformers (ViT)</a>: The Vision Transformer uses powerful natural language processing embeddings (BERT) and applies them to images.</li></ul><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>Explore more classification models <a href="https://roboflow.com/models?type=Classification&amp;ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">here</a>.</p></div><h3 id="object-detection-models" tabindex="-1"><a class="header-anchor" href="#object-detection-models"><span>Object Detection Models</span></a></h3><p><a href="https://roboflow.com/model-task-type/object-detection?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Object detection models</a> locate and classify multiple objects in an image by drawing bounding boxes and assigning labels (e.g., &quot;person,&quot; &quot;dog&quot;). These models locates and classifies multiple objects in an image by drawing bounding boxes around them and are used for identifying and tracking multiple objects such as people, vehicles, animals, or products within an image or video.</p><p><strong>Examples</strong>:</p><ul><li><a href="https://roboflow.com/model/yolov12?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">YOLOv12</a>: Fast real-time object detection model.</li><li><a href="https://roboflow.com/model/detr?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">DETR</a>: End-to-end transformer based object detection model.</li><li><a href="https://roboflow.com/model/rf-detr?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">RF-DETR</a>: Transformer-based real time object detection model.</li><li><a href="https://roboflow.com/models?type=Object+Detection&amp;ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Grounding DINO</a>: State-of-the-art zero-shot object detection model.</li></ul><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>Explore more object detection models <a href="https://roboflow.com/models?type=Object+Detection&amp;ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">here</a>.</p></div><h3 id="instance-semantic-segmentation-models" tabindex="-1"><a class="header-anchor" href="#instance-semantic-segmentation-models"><span>Instance &amp; Semantic Segmentation Models</span></a></h3><p>These models perform pixel-level classification using:</p><ul><li>Semantic segmentation labels each pixel (e.g., sky, road, person)</li><li>Instance segmentation also distinguishes between individual objects (e.g., person 1 vs. person 2)</li></ul><p>These models are used for understanding the shape and exact boundaries of objects, for example, isolating road lanes, tumors, or leaves from the background.</p><p><strong>Examples</strong>:</p><ul><li><a href="https://roboflow.com/model/mask-rcnn?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Mask R-CNN</a>: Combines Faster R-CNN with pixel masks.</li><li><a href="https://roboflow.com/model/yolov8-instance-segmentation?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">YOLOv8 Instance Segmentation</a>: State-of-the-art YOLOv8 model comes with support for instance segmentation tasks.</li><li><a href="https://roboflow.com/model/segment-anything-2?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Segment Anything 2</a>: Open-world, prompt-based segmentation from Meta.</li><li><a href="https://roboflow.com/model/segformer?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">SegFormer</a>: Transformer based model for semantic segmentation tasks.</li></ul><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>Explore instance segmentation model <a href="https://roboflow.com/models?type=Instance+Segmentation&amp;ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">here</a> and semantic segmentation <a href="https://roboflow.com/models?type=Semantic+Segmentation&amp;ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">here</a>.</p></div><h3 id="keypoint-detection-pose-estimation-models" tabindex="-1"><a class="header-anchor" href="#keypoint-detection-pose-estimation-models"><span>Keypoint Detection &amp; Pose Estimation Models</span></a></h3><p><a href="https://roboflow.com/model-task-type/keypoint-detection?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Keypoint detection models</a> identifies specific landmarks on objects, commonly human joints (elbow, wrist, knee, etc.). Pose estimation uses these points to determine the posture or orientation of a body or object. These models are used for estimating human posture, gesture recognition, fitness analysis, and motion capture. These models typically returns coordinates of 17–33 body joints per person:</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>[</span></span>
<span class="line"><span>    { &quot;x&quot;: 100, &quot;y&quot;: 200, &quot;label&quot;: &quot;left_elbow&quot; },</span></span>
<span class="line"><span>    ...</span></span>
<span class="line"><span>]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Examples</strong>:</p><ul><li><a href="https://roboflow.com/model/yolo-nas-pose?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">YOLO-NAS Pose</a>: a keypoint detection model developed by Deci AI.</li></ul><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>Explore keypoint detection models supported in Roboflow <a href="https://roboflow.com/models?type=Keypoint+Detection&amp;ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">here</a>.</p></div><h3 id="face-detection-recognition" tabindex="-1"><a class="header-anchor" href="#face-detection-recognition"><span>Face Detection &amp; Recognition</span></a></h3><p>Face Detection models finds and localizes faces in an image and Face Recognition models identifies or verifies individuals based on facial features. These models are used for biometric authentication, security surveillance, face tagging in photos, and access control systems.</p><p><strong>Examples</strong>:</p><ul><li><a href="https://arxiv.org/abs/1905.00641?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">RetinaFace</a>: Highly accurate face detectors with landmark extraction.</li><li><a href="https://arxiv.org/abs/1503.03832?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">FaceNet</a> / <a href="https://arxiv.org/abs/1801.07698?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">ArcFace</a> / <a href="https://github.com/deepinsight/insightface?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">InsightFace</a>: Convert faces to embeddings for matching.</li><li><a href="https://github.com/serengil/deepface?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">DeepFace</a>: High-level wrapper supporting multiple backends like VGGFace, Dlib, etc.</li></ul><h3 id="vision-language-models-vlms" tabindex="-1"><a class="header-anchor" href="#vision-language-models-vlms"><span>Vision-Language Models (VLMs)</span></a></h3><p><a href="https://roboflow.com/model-task-type/vision-language?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">VLMs</a> combine image understanding with natural language. You can ask them:</p><p><em>&quot;What is happening in this image?&quot;</em> or <em>&quot;Where is the dog?&quot;</em></p><p>They understand both visual patterns and language to give smart, text-based answers. These models interpret images using natural language and can answer questions about them, generate captions, or find objects by name. These models are used for image captioning, visual question answering, object grounding (&quot;where is the dog?&quot;), and multimodal AI applications.</p><p><strong>Examples</strong>:</p><ul><li><a href="https://roboflow.com/model/metaclip?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">MetaCLIP</a>: Matches images to text (zero-shot).</li><li><a href="https://roboflow.com/model/florence-2?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Florence-2</a> / <a href="https://roboflow.com/model/kosmos-2?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Kosmos-2</a>: Used for grounding, captioning, and segmentation with language.</li><li><a href="https://roboflow.com/model/gpt-4o?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">GPT-4o</a>: Chat about images, generate captions, interpret documents.</li></ul><h2 id="how-to-use-image-recognition-ai-using-roboflow" tabindex="-1"><a class="header-anchor" href="#how-to-use-image-recognition-ai-using-roboflow"><span>How to Use Image Recognition AI using Roboflow</span></a></h2><p><a href="https://roboflow.com/?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Roboflow</a> allow you to <a href="https://roboflow.com/train?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">train</a>, test, and <a href="https://roboflow.com/deploy?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">deploy</a> computer vision models that can recognize images. You can build powerful image recognition systems with just a few steps. Following are the steps to build image recognition AI with Roboflow</p><h3 id="step-1-create-a-project" tabindex="-1"><a class="header-anchor" href="#step-1-create-a-project"><span>Step #1: Create a Project</span></a></h3><p>Choose the type of task for which you want to build image recognition model. Roboflow supports following project types.</p><ul><li>Image Classification (Assign label to entire image.)</li><li>Object Detection (Identify objects and their positions with bounding boxes.)</li><li>Instance Segmentation (Detect multiple objects and their shapes.)</li><li>Semantic Segmentation (Assign every pixel to a label.)</li><li>Keypoint Detection (Identify keypoints/skeletons on subject)</li><li>Multimodal (Describe images using text pair)</li></ul><h3 id="step-2-upload-your-dataset" tabindex="-1"><a class="header-anchor" href="#step-2-upload-your-dataset"><span>Step #2: Upload Your Dataset</span></a></h3><p>Once the project is created, upload/drag and drop your images into Roboflow. You can also import data from <a href="https://universe.roboflow.com/?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Roboflow universe</a>, YouTube URL, <a href="https://docs.roboflow.com/datasets/adding-data/upload-data-from-aws-gcp-and-azure?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Cloud Providers</a>, and <a href="https://docs.roboflow.com/developer/manage-images/upload-an-image?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Upload API</a>.</p><h3 id="step-3-annotate-images" tabindex="-1"><a class="header-anchor" href="#step-3-annotate-images"><span>Step #3: Annotate Images</span></a></h3><p>Label your images using Roboflow&#39;s built-in <a href="https://roboflow.com/annotate?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">annotation tool</a>. You may use following annotation techniques:</p><ul><li>Manual Annotation: Use Roboflow&#39;s web-based <a href="https://docs.roboflow.com/annotate/annotation-tools?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">annotation tool</a> to label objects (e.g., bounding boxes, polygons etc.).</li><li>Auto-Labeling: Use Roboflow&#39;s AI-assisted labeling (i.e. <a href="https://docs.roboflow.com/annotate/ai-labeling/model-assisted-labeling?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Label Assist</a>, <a href="https://docs.roboflow.com/annotate/ai-labeling/enhanced-smart-polygon-with-sam?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Smart Polygon</a>, <a href="https://docs.roboflow.com/annotate/ai-labeling/box-prompting-ai-labeling?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Box Prompting</a>, <a href="https://docs.roboflow.com/annotate/ai-labeling/automated-annotation-with-autodistill?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Auto Label</a>) to speed up the process.</li></ul><h3 id="step-4-preprocess-augment-your-data" tabindex="-1"><a class="header-anchor" href="#step-4-preprocess-augment-your-data"><span>Step #4: Preprocess &amp; Augment Your Data</span></a></h3><p>Roboflow provides preprocessing and augmentation options to improve model robustness:</p><p><strong>Preprocessing</strong>: Preprocessing involves modifying raw images to standardize them for model training. Common techniques include Auto-Orient, Isolate Objects, Static Crop, Dynamic Crop, Resize, Grayscale, Auto-Adjust Contrast, Tile etc.</p><p><strong>Augmentation</strong>: Augmentation artificially expands the dataset by applying random transformations to images. This helps prevent overfitting (when a model memorizes training data instead of learning general patterns). Common techniques include</p><ul><li>Image Level Augmentations such as Flip, 90° Rotate, Crop, Rotation, Shear, Grayscale, Hue, Saturation, Brightness, Exposure, Blur, Noise, Cutout, Mosaic.</li><li>Bounding Box Level Augmentations such as Flip, 90° Rotate, Crop, Rotation, Shear, Brightness, Exposure, Blur, Noise.</li></ul><h3 id="step-5-generate-dataset" tabindex="-1"><a class="header-anchor" href="#step-5-generate-dataset"><span>Step #5: Generate Dataset</span></a></h3><p>Click &quot;Create&quot; to create a dataset version with your chosen settings.</p><h3 id="step-6-train-a-model" tabindex="-1"><a class="header-anchor" href="#step-6-train-a-model"><span>Step #6: Train a Model</span></a></h3><p>You can now train the model with Roboflow. You can choose Roboflow&#39;s built-in auto training option via &quot;Custom Train&quot; button for a hosted model. Or you can export the dataset to train in YOLO, TensorFlow, or PyTorch locally. The following is the example to export dataset in YOLOv8 format for training.</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" data-title="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> roboflow </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> Roboflow</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">rf </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> Roboflow</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">api_key</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;YOUR_API_KEY&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">project </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> rf.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">workspace</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">().</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">project</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;your-project&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">dataset </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> project.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">version</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">download</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">&quot;yolov8&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>See example <a href="https://github.com/roboflow/notebooks?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">notebooks</a> to train your model.</p></div><h3 id="step-7-deploy-your-model" tabindex="-1"><a class="header-anchor" href="#step-7-deploy-your-model"><span>Step #7: Deploy Your Model</span></a></h3><p>Roboflow offers flexible deployment options that allow you to run your vision models on the cloud, locally, or on various edge devices. Once trained, Roboflow provides:</p><ul><li>Workflows deployment to quickly configure, integrate, and deploy models into applications.</li><li>Hosted image and video inference endpoints deployments, which are internet-dependent and easy to set up for non-real-time and batch processing needs.</li><li>Edge deployment for embedded devices like TPUs or Android phones using custom code, or to edge devices such as NVIDIA Jetson through Docker containers for scalable, real-time inference.</li><li>Additional deployment options include dedicated remote servers managed by Roboflow, mobile deployment on iOS, Snap AR Lens Studio integration, and more, enabling wide compatibility across platforms and use cases.</li></ul><h2 id="how-roboflow-workflows-can-be-used-for-image-recognition" tabindex="-1"><a class="header-anchor" href="#how-roboflow-workflows-can-be-used-for-image-recognition"><span>How Roboflow Workflows Can Be Used for Image Recognition</span></a></h2><p>Roboflow Workflows is a feature that allows you to combine multiple computer vision models into a single pipeline. Instead of running one model at a time, a workflow lets you automatically chain tasks like object detection, classification, and <a href="https://roboflow.com/ocr?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">OCR</a> (text recognition) together, and get the final result with just one API call. Within Roboflow Workflows, you can build an image recognition pipeline using different model types and functional blocks, each responsible for a specific task. These blocks can be combined in sequence to form a complete visual processing pipeline.</p><p>Roboflow Workflows is powerful tool because it supports:</p><ul><li>Pre-trained models</li><li>Custom-trained/fine tuned models</li></ul><h3 id="using-pre-trained-models-in-roboflow" tabindex="-1"><a class="header-anchor" href="#using-pre-trained-models-in-roboflow"><span>Using pre-trained models in Roboflow</span></a></h3><p>Roboflow offers several ready-to-use models (such as YOLOv8, YOLOv11, YOLO-NAS, RF-DETR-Base, VLMs/Multimodal Models) that you can try on your own images without training anything. You can use these models directly in your Roboflow Workflows with the help of different blocks. For example you can use RF-DETR-Base or YOLOv8 model using <a href="https://inference.roboflow.com/workflows/blocks/object_detection_model/?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Object Detection Model block</a>, YOLOv8n-Seg segmentation model using <a href="https://inference.roboflow.com/workflows/blocks/instance_segmentation_model/?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Instance Segmentation Model block</a>, YOLOv8n-Pose pose estimation model using <a href="https://inference.roboflow.com/workflows/blocks/keypoint_detection_model/?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Keypoint Detection Model block</a> GPT-4o model using <a href="https://inference.roboflow.com/workflows/blocks/open_ai/?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">OpenAI block</a>, gemini-2.0-flash model using <a href="https://inference.roboflow.com/workflows/blocks/google_gemini/?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Google Gemini block</a> and many more.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/workflow-example.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Roboflow Workflow Example</em></p><div class="hint-container tip"><p class="hint-container-title">Tips</p><p><a href="https://roboflow.com/workflows/build?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Roboflow Workflows</a> is a no-code computer vision application builder that allows users to create multi-step, complex computer vision applications in a web browser. It enables users to connect various blocks (pre-built functionalities) to design and construct vision pipelines without needing extensive coding expertise. These workflows can be deployed on the Roboflow Cloud or self-hosted on various hardware, including edge devices.</p></div><h3 id="using-trained-or-fine-tuned-custom-models-with-your-data" tabindex="-1"><a class="header-anchor" href="#using-trained-or-fine-tuned-custom-models-with-your-data"><span>Using trained or fine-tuned custom models with your data</span></a></h3><p>Roboflow is an end-to-end platform for computer vision development. It supports the entire lifecycle of building computer vision models from data collection and labeling to dataset generation, training, fine-tuning, inferencing, deployment, and integration with APIs. Once you train a custom computer vision model using Roboflow, it is hosted and readily available to be integrated in your application via APIs. You can also integrate these models in your Roboflow Workflow via your workspace or publicly available models within other users Workspace in the Roboflow platform.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/custom_models.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Custom Trained Model in Roboflow</em> Workspace</p><h2 id="building-image-recognition-ai-with-roboflow" tabindex="-1"><a class="header-anchor" href="#building-image-recognition-ai-with-roboflow"><span>Building Image Recognition AI with Roboflow</span></a></h2><p>Now let&#39;s see some example of how to build image recognition AI application with Roboflow. In this section we will use custom trained models (<a href="https://universe.roboflow.com/koba-nanyo/wood-zay26?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Wood/Log Detection</a>, <a href="https://universe.roboflow.com/tim-4ijf0/hand-gestures-cjowr/model/2?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Hand Gesture Recognition</a>) as well as pre-trained model (Florence-2) with Roboflow Workflows to build our application.</p><h3 id="example-1-detecting-and-counting-wood-log" tabindex="-1"><a class="header-anchor" href="#example-1-detecting-and-counting-wood-log"><span>Example #1: Detecting and counting wood/log</span></a></h3><p>In this example we will build a Roboflow Workflow application that will recognize and detect Wood/Log and count it. For create the object detection project, upload and label dataset and train the model using Roboflow Autotraining option. The trained model is available at Roboflow hosted inference server that we can use.</p><p>In this example, we build a Roboflow Workflow application designed to detect and count logs (wood pieces) in an image. The project follows an object detection approach using the Roboflow 2.0 Object Detection (Fast) model. To create this application, a custom dataset of 183 labeled images containing wood logs was uploaded to Roboflow. Each log in the image was annotated with the class label &quot;log&quot;. The model was trained using Roboflow&#39;s AutoML training pipeline. The model was trained and achieved an mAP@50 of 94.6%, with a precision of 95.0% and a recall of 91.4%. The trained model, identified as <a href="https://universe.roboflow.com/koba-nanyo/wood-zay26/model/1?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">wood-zay26/1</a>, is hosted on Roboflow&#39;s inference server and can be integrated into a workflow or called via API.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/wood_model.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><a href="https://universe.roboflow.com/koba-nanyo/wood-zay26/model/1?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer"><em>Wood/Log Detection</em></a><em>model at Roboflow</em></p><p>We will integrate this model into our Roboflow Workflow by creating a new workflow and adding an Object Detection Model block. This block is responsible for running inference using the trained model. In the block&#39;s configuration, set the Model property to <a href="https://universe.roboflow.com/koba-nanyo/wood-zay26/model/1?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">wood-zay26/1</a>, which points to the deployed custom object detection model hosted on Roboflow. This enables the workflow to automatically detect and label logs in input images using the trained model.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/wk_1-1.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Wood/Log detection and counting Workflow</em></p><p>Now add the Property definition block. This block is used to count the number of detections that helps to count the number of Wood/Logs in the image. Set the Operations property of this block to &quot;Count Items&quot; as shown below.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/property_wk_1.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Property block configuration</em></p><p>Finally, add a Bounding Box Visualization block to display the detection results with bounding boxes over the identified objects. Once the workflow is executed, it will generate an output image highlighting each detected wood log, allowing you to visually confirm the model&#39;s recognition of logs within the image.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/output_wk_1.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Output of Wood/Log Counting Workflow</em></p><p>The JSON output displays the result from the Property Block, which provides the total count of detected wood logs identified in the image.</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>&quot;property_definition&quot;: 29</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>This type of workflow is especially useful in forestry management, inventory tracking, and automated material handling, where counting and recognizing logs in stacked images is required.</p><p>You can also run this workflow locally or in real-time using webcam input or edge devices, and even customize it further by adjusting confidence thresholds and overlap settings.</p><h3 id="example-2-recognizing-hand-gestures" tabindex="-1"><a class="header-anchor" href="#example-2-recognizing-hand-gestures"><span>Example #2: Recognizing Hand Gestures</span></a></h3><p>In this example, we build a <a href="https://universe.roboflow.com/tim-4ijf0/hand-gestures-cjowr/model/2?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">Hand Gesture Recognition</a> application using a custom-trained object detection model from Roboflow. The model is designed to detect and identify different hand gestures, as shown in the following image, based on the shape of the hand in an image. These hand gestures are used for controlling the AC light bulb.</p><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>Read the full blog <a href="https://blog.roboflow.com/gesture-light-system/" target="_blank" rel="noopener noreferrer">Build a Gesture-Based Light Controller with Computer Vision</a>.</p></div><p>The model, trained using Roboflow&#39;s AutoML pipeline, is based on the Roboflow 3.0 Object Detection (Accurate) architecture with the COCOs checkpoint as its foundation. The training dataset consists of annotated images representing various hand gestures, each labeled with the corresponding gesture name as the class.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/hand_dataset.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Hand Gesture Dataset</em></p><p>In the inference shown below, the model has successfully detected a hand gesture and labeled it as &quot;on&quot; with 96% confidence. The model was trained and achieved an mAP@50 of 99.5%, with a precision of 99.7% and a recall of 100.0%.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/hand_model.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Hand Gesture Recognition Model</em></p><p>We will use this model in a Roboflow Workflow to build a Hand Gesture Recognition application. To set it up, create a new workflow and add an Object Detection Model block, configuring the Model property to <a href="https://universe.roboflow.com/tim-4ijf0/hand-gestures-cjowr/model/2?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">hand-gestures-cjowr/2</a>. Then, include both a Bounding Box Visualization block and a Label Visualization block to display the detected hand gestures along with their corresponding class names.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/wk_hand_recog.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Hand Gesture Recognition Workflow</em></p><p>Once the workflow is executed on an input image, it will highlight each recognized gesture with labeled bounding boxes.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/wk_hand_g_out.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Output of Hand Gesture Recognition Workflow</em></p><p>Apart from processing static images, the workflow can also handle video files and live video streams. Using the Inference Pipeline SDK, you can deploy and run workflow locally on an edge device to process video inputs in real time. This makes it suitable for interactive applications such as gesture-based control systems, smart home interfaces, assistive technologies, and sign language detection system.</p><h3 id="example-3-vlm-for-image-recognition" tabindex="-1"><a class="header-anchor" href="#example-3-vlm-for-image-recognition"><span>Example #3: VLM for Image Recognition</span></a></h3><p>In this example, we use the Microsoft Florence-2 Vision-Language Model (VLM) to build an intelligent image recognition application. The application is powered by a Roboflow workflow that integrates a pre-trained Florence-2 model capable of identifying and locating specific objects within an image. By providing a text prompt, such as the name or description of the object, the model is guided to detect and highlight the target object in the image. This approach leverages the power of multimodal AI, combining vision and language understanding to perform flexible, prompt-based object recognition.</p><p>Create a Roboflow Workflows as following. Add the Florence-2 block, VLM as Detector block, Bounding Box Visualization Block and Label Visualization Block.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/wk_3.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Object Detection using VLM Workflow</em></p><p>Add a text parameter in input block to specify the text based prompt along with input image.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/input_wk_3.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Input Block Configuration</em></p><p>Configure the Florence-2 block as following. Choose the Task Type as &quot;Prompted Object Detection&quot;. Bind the Prompt property with the &quot;prompt&quot; parameter added in the Input Block.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/florence_wk_3-1.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Florence-2 Block Configuration</em></p><p>Now, configure the VLM as Detector block as follows. The VLM Output and Classes properties are set with the output from Florence-2 block and Model Type property is set to Florence-2 and Task Type to Prompted Object Detection.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/vlm_conf.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>VLM as Detector lock Configuration</em></p><p>Run the Workflow by specifying prompt and uploading input image. In the case the prompt is &quot;Where is the ball?&quot; and input image is of a baseball player (batter) hitting the ball. We want to identify the ball and it&#39;s position in the image.</p><figure><img src="https://blog.roboflow.com/content/images/2025/06/output_wk_3.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>Output of Workflow</em></p><p>Vision-Language Models (VLMs) offer a powerful and flexible approach to image recognition by combining visual understanding with natural language prompts. Instead of relying solely on predefined classes, VLMs allow users to describe what they want to detect using simple text inputs. This enables prompt-based object detection, where the model can identify and localize objects in an image based on a given description. Whether used for locating specific items, generating image captions, or answering visual questions, VLMs make image recognition more intuitive and adaptable to a wide range of real-world scenarios.</p><h2 id="image-recognition-software" tabindex="-1"><a class="header-anchor" href="#image-recognition-software"><span>Image Recognition Software</span></a></h2><p>Image recognition is a powerful application of computer vision that allows machines to understand and interpret visual data just like humans. With platforms like Roboflow, building and deploying intelligent image recognition systems becomes accessible, even without deep coding expertise. Whether it&#39;s detecting logs, recognizing hand gestures, or using vision-language models for prompt-based detection, Roboflow Workflows empower developers to create custom or multimodal AI pipelines with ease. These capabilities open the door to real-world applications in industries like forestry, security, <a href="https://roboflow.com/industries/retail-and-ecommerce?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">retail</a>, <a href="https://roboflow.com/industries/healthcare-and-medicine?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">healthcare</a>, and beyond.</p><p>The following are the key takeaways from this blog to help you understand and apply image recognition using Roboflow:</p><ul><li><strong>Image Recognition</strong>: Computers interpret images as numerical data, and through training, they learn to recognize patterns and objects using neural networks.</li><li><strong>Types of Models</strong>: There are different models for various tasks including classification, object detection, segmentation, keypoint detection, hand gesture recognition, and vision-language understanding.</li><li><strong>Roboflow Workflows</strong>: A no-code/low-code visual pipeline builder that lets users chain multiple model types and functions to create full image recognition systems.</li><li><strong>Pre-trained vs Custom Models</strong>: Roboflow supports both, use ready-to-go models or train your own on custom datasets using AutoML.</li><li><strong>Real-World Applications</strong>: From wood log counting to real-time hand gesture recognition and prompt-based object detection with VLMs, image recognition has a broad range of use cases.</li><li><strong>Flexible Deployment</strong>: Roboflow supports cloud-based, <a href="https://roboflow.com/ai/edge?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">edge device</a>, and mobile deployment, along with hosted APIs and SDKs for real-time or batch inference.</li></ul><h3 id="cite-this-post" tabindex="-1"><a class="header-anchor" href="#cite-this-post"><span><strong>Cite this Post</strong></span></a></h3><p>Use the following entry to cite this post in your research:</p><p><em><a href="https://blog.roboflow.com/author/timothy/" target="_blank" rel="noopener noreferrer">Timothy M.</a>. (Jun 10, 2025). What Is Image Recognition? Algorithms and Applications. Roboflow Blog: <a href="https://blog.roboflow.com/image-recognition/" target="_blank" rel="noopener noreferrer">https://blog.roboflow.com/image-recognition/</a></em></p></div><!--[--><div class="theme-hope-content"><Share colorful services="email,facebook,line,linkedin,messenger,qrcode,reddit,telegram,twitter"></Share></div><!--]--><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/vuepress-theme-hope/vuepress-theme-hope/edit/main/src/posts/reprints/image-recognition.md" aria-label="Edit this page on GitHub" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page on GitHub<!----></a></div><div class="vp-meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/posts/reprints/what-is-block-merging.html" aria-label="What is Block Merging?"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><!---->What is Block Merging?</div></a><!----></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">My own footer</div><div class="vp-copyright">Copyright © 2025 Timothy M. </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script src="/assets/js/runtime~app.58509077.js" defer></script><script src="/assets/js/9156.64e2dfa0.js" defer></script><script src="/assets/js/app.9e354a1b.js" defer></script>
  </body>
</html>
