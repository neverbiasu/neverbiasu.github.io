<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.17" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.58" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="alternate" hreflang="zh-cn" href="https://neverbiasu.github.io/zh/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.html"><meta property="og:url" content="https://neverbiasu.github.io/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.html"><meta property="og:site_name" content="Nlog"><meta property="og:title" content="Step by Step visual introduction to Diffusion Models"><meta property="og:description" content="Step by Step visual introduction to Diffusion Models What is a diffusion model? The idea of the diffusion model is not that old. In the 2015 paper called “Deep Unsupervised Lear..."><meta property="og:type" content="article"><meta property="og:image" content="https://erdem.pl/static/c5e1011bc11d7f13fdf19295c0e94c3e/eea4a/linear_noise.jpg"><meta property="og:locale" content="en-US"><meta property="og:locale:alternate" content="zh-CN"><meta property="article:author" content="Kemal Erdem"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Diffusion Models"><meta property="article:published_time" content="2023-11-01T00:00:00.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Step by Step visual introduction to Diffusion Models","image":["https://erdem.pl/static/c5e1011bc11d7f13fdf19295c0e94c3e/eea4a/linear_noise.jpg","https://erdem.pl/static/6bf29ce5feb6b15753aff769147e7be8/21b4d/lin_schedule_beta.png","https://erdem.pl/static/eb1c5091b8115b21499e408a8edf7b82/eea4a/cos_noise.jpg","https://erdem.pl/869cd128ff69bb1d304675166e8b7b34/gan_diagram_generator.svg","https://erdem.pl/static/897bdd95a9ca14cff82e0257de0dccc3/ee515/graphical_diffusion_model.png","https://erdem.pl/static/512d5abbaa2b507956d1b737f85cee1b/34e70/one_diffusion_step.png","https://erdem.pl/static/90207efb3cb80cde5f359b2205b0303a/0f840/sample_full_noise_removal.jpg","https://erdem.pl/static/18fc014ed4e2d9fe4d1439b3431ccad9/d38a6/conv_block.png","https://erdem.pl/static/7767bbec58461aa09c8032a5dc9bf06c/409e6/downsample.png","https://erdem.pl/static/2f26fadad0f8290c51b1b8579c008aeb/41d3c/attention.png","https://erdem.pl/static/e56f851d5e6c5e8347979cede14afc27/42a8d/upsample.png"],"datePublished":"2023-11-01T00:00:00.000Z","dateModified":null,"author":[{"@type":"Person","name":"Kemal Erdem"}]}</script><title>Step by Step visual introduction to Diffusion Models | Nlog</title><meta name="description" content="Step by Step visual introduction to Diffusion Models What is a diffusion model? The idea of the diffusion model is not that old. In the 2015 paper called “Deep Unsupervised Lear...">
    <link rel="stylesheet" href="/assets/css/styles.50126b1a.css">
    <link rel="preload" href="/assets/js/runtime~app.d5565785.js" as="script"><link rel="preload" href="/assets/css/styles.50126b1a.css" as="style"><link rel="preload" href="/assets/js/9156.64e2dfa0.js" as="script"><link rel="preload" href="/assets/js/app.02df9061.js" as="script">
    <link rel="prefetch" href="/assets/js/posts_reprints_flux-qlora.html.e25c0732.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_flux-qlora.html.bdca01c9.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_news-agents-daily-recap.html.ca04913a.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ielts_simon-task2.html.2fe38674.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_crody's-model-merge-guide.html.07f44e7f.js" as="script"><link rel="prefetch" href="/assets/js/8300.853d6b0b.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ielts_usage-of-collocations-in-speaking_ielts-collocations.html.86f5c7c6.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_crody's-model-merge-guide.html.046f0899.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_image-recognition.html.07c5196c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_image-recognition.html.61c0e69e.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_tutorials_comfyui_wan2-2-beginner.html.c1b11b51.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_blog-images.html.d2c6545e.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview.html.9d72dc96.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview.html.9fa0e201.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_what-is-block-merging.html.b7b3e018.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_original-character-lora-sdxl-character-training.html.ce54ff9d.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_step-by-step-visual-introduction-to-diffusion-models.html.cd2854e8.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_flux-kontext.html.ed243625.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_dgpst.html.d42b8362.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_original-character-lora-sdxl-character-training.html.52dcf985.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_repos_material.html.fbe2fd29.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression.html.fa0b7a73.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_X01.html.eb6ee3d6.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression.html.f7670351.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_sci_conda.html.1ba5b9e5.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_papers_workflow.html.e18adbce.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.html.a34aec93.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.html.d9238744.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_flux-kontext-optimization.html.1a4897dc.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_tutorials_comfyui_flux-kontext-beginner.html.be5a7e5e.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_flux-kontext-optimization.html.5532e6f6.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_step-by-step-visual-introduction-to-diffusion-models.html.e8bf942f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_repos_comfy-mind.html.6f1a4f44.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-impls_yolov9.html.cbf247d0.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_model-block-merge-1.html.3abbf8d4.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_ai-art-newsletter-jan-25.html.5faf4d51.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_006.html.76c84f58.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_announcing-illustrious-text‑enhancer-tag-booster-and-mood-enhancer.html.517a1ee1.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_model-block-merge-2.html.0f9c14de.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_generate-consistent-characters.html.6bf8ea7d.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_sdo.html.b7ecccd0.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_announcing-illustrious-text‑enhancer-tag-booster-and-mood-enhancer.html.f6a8fbb8.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_resnet.html.6393e916.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_ai-art-newsletter-jan-25.html.971ce630.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_bagel.html.049a219c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_generate-consistent-characters.html.1a8fae43.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_ming-omni.html.20e14b21.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_omniconsistency.html.4c6e9330.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything.html.314cbfd7.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_014.html.c41082ad.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_flux-1-kontext.html.6297069c.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_niji-lesson-2-the-terminator-line.html.ebdb33a7.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_comfyui-r1.html.bea016dd.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_niji-study-1-measuring-with-your-eyes.html.721630f8.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything.html.22fa5919.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_tutorials_qwen-code.html.aaaec313.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_001.html.2986e89d.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_flux-1-kontext.html.6d0bc73c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_ecomimic-v3.html.a8a6b4b4.js" as="script"><link rel="prefetch" href="/assets/js/zh_demo_markdown.html.2b3004d0.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_colorizediffusion.html.c18d15cc.js" as="script"><link rel="prefetch" href="/assets/js/demo_markdown.html.a2081e01.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_niji-study-1-measuring-with-your-eyes.html.403ee9cf.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_repos_checklist.html.2c86bf22.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_hunyuancustom.html.c2b4ff7e.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ielts_simon-task1.html.34242439.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_dairys_250222.html.41739137.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_show-o2.html.6145cdd9.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_hf-weekly_001.html.89ae8f15.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_repos_workflow.html.5967ccd1.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_037.html.b0a4ddd3.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_templates_repos.html.b0b47564.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_033.html.3fc966cc.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_031.html.86f17a2c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_043.html.70bd2bb5.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_icedit.html.ba272966.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_032.html.30191816.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_026.html.6bc17810.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_042.html.51c7aa9e.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_omnigen2.html.7251ca67.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_transformer.html.a9f96585.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_alexnet.html.ffa5e652.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_009.html.b41f980f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_022.html.33930402.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_030.html.2f8b0739.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_039.html.e4384ea6.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_011.html.4c72c992.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_ai-art-gtc-paris-2025.html.35f9c6c5.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_046.html.7fe6c3c1.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_029.html.6bd2bbb4.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_how-and-when-to-build-multi-agent-systems.html.683f686f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_reptext.html.3e68da15.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_040.html.77ad6dd0.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_generative-ai-powered-design.html.29b4cfc0.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_045.html.bcee5b9e.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_how-and-when-to-build-multi-agent-systems.html.83239b53.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_034.html.bb3494e0.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_041.html.cc30477b.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_ai-art-gtc-paris-2025.html.dd4e6753.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_036.html.103493df.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_044.html.4316f4fc.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_015.html.f3bc358d.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_thoughts_platform-operation-thoughts-after-comfycon.html.0a6fc557.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_020.html.e178431c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_035.html.76b543ac.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_048.html.0e9f2ac7.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_generative-ai-powered-design.html.8ea641cf.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_023.html.2c7192b4.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_021.html.1924500a.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_010.html.995fa31e.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_005.html.77979cff.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_qr-lora.html.12a884c9.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_hf-weekly_003.html.add9120d.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_hf-weekly_005.html.0005f756.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_003.html.fe38d2ee.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_038.html.54d019c6.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_047.html.7b249002.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_018.html.3aabc796.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_012.html.2ce3356f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_blip-3o.html.20d6a0b5.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_explaining-tokens-the-language-and-currency-of-ai-nvidia-blog.html.98d7044c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_025.html.828c6af4.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_027.html.a69f4cf8.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_hf-weekly_004.html.e20ccdbc.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_framepack.html.79cc8785.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_028.html.23f08d5c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_024.html.c9d95ff5.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_013.html.659095e1.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_vlv.html.66e5a328.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_hf-weekly_002.html.475e8a71.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_019.html.dbd20518.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_illustrious-xl-v2.0-the-best-training-base-model-in-1536-age.html.b5e190f7.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_illustrious-xl-v2.0-the-best-training-base-model-in-1536-age.html.824cc0cb.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_explaining-tokens-the-language-and-currency-of-ai-nvidia-blog.html.0dce2f26.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_017.html.cf727d63.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_niji-lesson-2-the-terminator-line.html.bcba9d63.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_niji-study-2-notan.html.bc893745.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_ovis-u1.html.f00080b2.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_016.html.c3ada0a7.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_mcp-flash-in-the-pan-or-future-standard.html.b142e62d.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_papers_checklist.html.153167b0.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_008.html.8b9058fe.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_nhr.html.4a137534.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_002.html.5a4dd3d5.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_niji-video.html.211bd024.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_mcp-flash-in-the-pan-or-future-standard.html.19ff25fe.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_004.html.9173a01b.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_niji-video.html.26560c68.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_007.html.eab02414.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_niji-study-2-notan.html.70b1ed6f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_3steps-paper-reading.html.eda5237e.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_dairys_250223.html.b6746695.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_experiments-with-mcp-using-github-copilot.html.b89c4005.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_experimenting-with-mcp-using-github-copilot.html.f9a39fff.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_illustrious-lu-v0.03.html.dbc0dce0.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_illustrious-lu-v0.03.html.8b95f0b0.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_templates_papers.html.31aef926.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_templates_hf-weekly.html.5c334dac.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_hf-weekly_checklist.html.c3fa5a04.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_templates_ai-weekly.html.61881c27.js" as="script"><link rel="prefetch" href="/assets/js/zh_demo_page.html.0b7cffcc.js" as="script"><link rel="prefetch" href="/assets/js/demo_page.html.50e1e3b3.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_hf-weekly.prompt.html.a49236f9.js" as="script"><link rel="prefetch" href="/assets/js/zh_demo_layout.html.9b3fdc7f.js" as="script"><link rel="prefetch" href="/assets/js/demo_layout.html.25c357de.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_hf-weekly_workflow.html.da6dc285.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_translate.prompt.html.0e65a605.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_cover.prompt.html.57daed61.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_ai-weekly.prompt.html.2fe1eb8f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_papers.prompt.html.ba28b7ce.js" as="script"><link rel="prefetch" href="/assets/js/home.html.a72e8bb4.js" as="script"><link rel="prefetch" href="/assets/js/zh_home.html.7623eb73.js" as="script"><link rel="prefetch" href="/assets/js/demo_disable.html.aeb75044.js" as="script"><link rel="prefetch" href="/assets/js/zh_demo_disable.html.917844f9.js" as="script"><link rel="prefetch" href="/assets/js/zh_intro.html.efed7168.js" as="script"><link rel="prefetch" href="/assets/js/intro.html.50f0afc5.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_image-extract.prompt.html.9c9b8aa7.js" as="script"><link rel="prefetch" href="/assets/js/zh_index.html.40d3a237.js" as="script"><link rel="prefetch" href="/assets/js/index.html.f45e7c7b.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_web_vue-1.html.0755e363.js" as="script"><link rel="prefetch" href="/assets/js/zh_demo_index.html.34792277.js" as="script"><link rel="prefetch" href="/assets/js/demo_encrypt.html.25bd9519.js" as="script"><link rel="prefetch" href="/assets/js/zh_demo_encrypt.html.06a96e54.js" as="script"><link rel="prefetch" href="/assets/js/tag_artificial-intelligence_index.html.7e3793fa.js" as="script"><link rel="prefetch" href="/assets/js/demo_index.html.febe17c4.js" as="script"><link rel="prefetch" href="/assets/js/category_model-development_index.html.794f5d59.js" as="script"><link rel="prefetch" href="/assets/js/category_image-generation_index.html.2189f40e.js" as="script"><link rel="prefetch" href="/assets/js/category_model-training_index.html.be238a49.js" as="script"><link rel="prefetch" href="/assets/js/category_generative-ai_index.html.b909fb8b.js" as="script"><link rel="prefetch" href="/assets/js/tag_stable-diffusion_index.html.ae5f1c64.js" as="script"><link rel="prefetch" href="/assets/js/category_anime-style_index.html.3108accc.js" as="script"><link rel="prefetch" href="/assets/js/tag_amazon-bedrock_index.html.a67f2776.js" as="script"><link rel="prefetch" href="/assets/js/tag_resource-guide_index.html.5acb26ee.js" as="script"><link rel="prefetch" href="/assets/js/category_explainer_index.html.a8e8f462.js" as="script"><link rel="prefetch" href="/assets/js/category_reprints_index.html.4a06de7a.js" as="script"><link rel="prefetch" href="/assets/js/tag_kohya-ss-gui_index.html.27638eba.js" as="script"><link rel="prefetch" href="/assets/js/tag_fundamentals_index.html.374e901f.js" as="script"><link rel="prefetch" href="/assets/js/category_reprint_index.html.6eafee8b.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_模型上下文协议_index.html.e75af900.js" as="script"><link rel="prefetch" href="/assets/js/tag_illustrious_index.html.ff354070.js" as="script"><link rel="prefetch" href="/assets/js/tag_text2image_index.html.bb9c3a46.js" as="script"><link rel="prefetch" href="/assets/js/tag_taylorseer_index.html.d4b32963.js" as="script"><link rel="prefetch" href="/assets/js/category_aiml_index.html.35fc99fb.js" as="script"><link rel="prefetch" href="/assets/js/tag_diffusers_index.html.8f93bf3f.js" as="script"><link rel="prefetch" href="/assets/js/tag_inference_index.html.b69c0933.js" as="script"><link rel="prefetch" href="/assets/js/tag_replicate_index.html.61deee89.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_阿里巴巴ai研究_index.html.dd5f50e2.js" as="script"><link rel="prefetch" href="/assets/js/tag_markdown_index.html.ec598c41.js" as="script"><link rel="prefetch" href="/assets/js/tag_drawing_index.html.91b0ebb5.js" as="script"><link rel="prefetch" href="/assets/js/tag_flux.1_index.html.83ab9654.js" as="script"><link rel="prefetch" href="/assets/js/tag_lumina_index.html.27a09c4b.js" as="script"><link rel="prefetch" href="/assets/js/tag_prompt_index.html.d3dbd2e7.js" as="script"><link rel="prefetch" href="/assets/js/tag_script_index.html.eae6815c.js" as="script"><link rel="prefetch" href="/assets/js/tag_merge_index.html.ae63ce39.js" as="script"><link rel="prefetch" href="/assets/js/tag_model_index.html.acdbe3e4.js" as="script"><link rel="prefetch" href="/assets/js/tag_qlora_index.html.11905a67.js" as="script"><link rel="prefetch" href="/assets/js/tag_qwen3_index.html.e8eb5022.js" as="script"><link rel="prefetch" href="/assets/js/tag_vpred_index.html.426ee337.js" as="script"><link rel="prefetch" href="/assets/js/tag_notan_index.html.d9a4a654.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_创造者工坊_index.html.094f4c96.js" as="script"><link rel="prefetch" href="/assets/js/tag_flux_index.html.ad3867cf.js" as="script"><link rel="prefetch" href="/assets/js/tag_llms_index.html.67431ae9.js" as="script"><link rel="prefetch" href="/assets/js/tag_lora_index.html.c7ee546c.js" as="script"><link rel="prefetch" href="/assets/js/tag_qwen_index.html.e0838b98.js" as="script"><link rel="prefetch" href="/assets/js/tag_sdxl_index.html.37a19ff3.js" as="script"><link rel="prefetch" href="/assets/js/tag_niji_index.html.a70a9af6.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_大语言模型_index.html.72664ff8.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_计算机视觉_index.html.58b39887.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_输入预处理_index.html.ed9bbf63.js" as="script"><link rel="prefetch" href="/assets/js/tag_art_index.html.fd5de43a.js" as="script"><link rel="prefetch" href="/assets/js/tag_aws_index.html.49d0379d.js" as="script"><link rel="prefetch" href="/assets/js/tag_llm_index.html.3a8069a7.js" as="script"><link rel="prefetch" href="/assets/js/tag_mcp_index.html.5e93604a.js" as="script"><link rel="prefetch" href="/assets/js/tag_gtc_index.html.1e06b509.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_artificial-intelligence_index.html.8cfd6b09.js" as="script"><link rel="prefetch" href="/assets/js/category_index.html.30915709.js" as="script"><link rel="prefetch" href="/assets/js/tag_ai_index.html.691e8ede.js" as="script"><link rel="prefetch" href="/assets/js/tag_lu_index.html.f7da3bcc.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_llm基准测试_index.html.fc9da582.js" as="script"><link rel="prefetch" href="/assets/js/timeline_index.html.3f867224.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_阿里巴巴ai_index.html.c970b0e2.js" as="script"><link rel="prefetch" href="/assets/js/article_index.html.3131e8c3.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_使用指南_index.html.63c177db.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_图像生成_index.html.c3ae9a7c.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_教程指南_index.html.42c5df88.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_模型开发_index.html.185a36fd.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_模型研发_index.html.f6ab3ec2.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_模型训练_index.html.1785036a.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_视频生成_index.html.60de099e.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_论文精读_index.html.7276ab7f.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_动漫风格_index.html.8538e3dd.js" as="script"><link rel="prefetch" href="/assets/js/tag_model-context-protocol_index.html.25f141b3.js" as="script"><link rel="prefetch" href="/assets/js/posts_reprints_index.html.8158d3a4.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ielts_usage-of-collocations-in-speaking_index.html.b2df067f.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_workflow-generation_index.html.4c431f19.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_人工智能_index.html.f0a2e682.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_使用指南_index.html.fe8c29d0.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_功能发布_index.html.d6a89296.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_反向传播_index.html.ac57d1ff.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_可控生成_index.html.142da7e4.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_图像生成_index.html.dfb3e561.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_图像识别_index.html.9bff49b9.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_基础模型_index.html.7953d298.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_开源项目_index.html.e3cfbec0.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_扩散模型_index.html.829f3a99.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_技术教程_index.html.9ee8291c.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_游戏开发_index.html.6e767c46.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_艺术课程_index.html.46238e22.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_视频生成_index.html.8068090f.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_计算优化_index.html.4e81f12a.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_阿里巴巴_index.html.df377e00.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_页面配置_index.html.c7985dc6.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_model-development_index.html.464b624b.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_机器学习_index.html.fb0ee906.js" as="script"><link rel="prefetch" href="/assets/js/tag_large-language-models_index.html.3bcf45b0.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_2048分辨率_index.html.6fb915df.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_生成式ai_index.html.946ab634.js" as="script"><link rel="prefetch" href="/assets/js/tag_feature-announcement_index.html.5aa6fb5c.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_image-generation_index.html.b26f5abb.js" as="script"><link rel="prefetch" href="/assets/js/star_index.html.0ffd513d.js" as="script"><link rel="prefetch" href="/assets/js/tag_index.html.fc1f1e0b.js" as="script"><link rel="prefetch" href="/assets/js/tag_alibaba-ai-research_index.html.adbbd683.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_epsilon预测_index.html.93831d46.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_多模态ai_index.html.f345a0db.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_多语言ai_index.html.fe70d6af.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_stable-diffusion_index.html.a72cbefd.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_生成式ai_index.html.5736baf2.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_初学者_index.html.f5485ca2.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_张吕敏_index.html.baa96ea6.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_model-training_index.html.7e4d037d.js" as="script"><link rel="prefetch" href="/assets/js/tag_epsilon-prediction_index.html.28d2e698.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_reasoning-model_index.html.1a71ba78.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_generative-ai_index.html.82923755.js" as="script"><link rel="prefetch" href="/assets/js/tag_image-recognition_index.html.a2fa1118.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_公众号_index.html.08bf5712.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_创作者_index.html.e962922e.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_多模态_index.html.0da8de43.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_编辑器_index.html.9af3b2cb.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_amazon-bedrock_index.html.8cef35a6.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_resource-guide_index.html.ea147e91.js" as="script"><link rel="prefetch" href="/assets/js/posts_index.html.efc2f348.js" as="script"><link rel="prefetch" href="/assets/js/tag_game-development_index.html.57d177bb.js" as="script"><link rel="prefetch" href="/assets/js/tag_image-generation_index.html.8b92bc54.js" as="script"><link rel="prefetch" href="/assets/js/tag_machine-learning_index.html.faec0f3f.js" as="script"><link rel="prefetch" href="/assets/js/tag_visual-hierarchy_index.html.a2caed5c.js" as="script"><link rel="prefetch" href="/assets/js/tag_diffusion-models_index.html.93147de9.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_anime-style_index.html.6dcca799.js" as="script"><link rel="prefetch" href="/assets/js/tag_2048-resolution_index.html.f3714f53.js" as="script"><link rel="prefetch" href="/assets/js/tag_computer-vision_index.html.16784659.js" as="script"><link rel="prefetch" href="/assets/js/tag_multilingual-ai_index.html.366e07d4.js" as="script"><link rel="prefetch" href="/assets/js/tag_stablediffusion_index.html.4cefc3f4.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_kohya-ss-gui_index.html.41ea145e.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_fundamentals_index.html.a2abb31f.js" as="script"><link rel="prefetch" href="/assets/js/tag_llm-benchmarks_index.html.a4c66890.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_ai推理_index.html.e5d2b110.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_ai模型_index.html.3bce6dfb.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_ai生成_index.html.7ea51825.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_新闻_index.html.0c095e7c.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_日记_index.html.aa249f5e.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_论文_index.html.bf9a7705.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_高级_index.html.b102e135.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_illustrious_index.html.1cf7c511.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_思考_index.html.cf92ff59.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_指南_index.html.e374501e.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_转载_index.html.5c33d05d.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_explainer_index.html.e16f7b83.js" as="script"><link rel="prefetch" href="/assets/js/tag_automatic1111_index.html.96f5b2d6.js" as="script"><link rel="prefetch" href="/assets/js/tag_generative-ai_index.html.ac7fe732.js" as="script"><link rel="prefetch" href="/assets/js/tag_multimodal-ai_index.html.8e502dbb.js" as="script"><link rel="prefetch" href="/assets/js/tag_agent-systems_index.html.cea033f6.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_text2image_index.html.bc1c311b.js" as="script"><link rel="prefetch" href="/assets/js/category_advanced_index.html.b0b65b54.js" as="script"><link rel="prefetch" href="/assets/js/category_ai-tools_index.html.2197e21d.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_taylorseer_index.html.46aded77.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_reprints_index.html.0826fe61.js" as="script"><link rel="prefetch" href="/assets/js/tag_quantization_index.html.d1270720.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_优化_index.html.33627e19.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_做饭_index.html.14d1e2ca.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_内容_index.html.bf4dd66c.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_创新_index.html.3233a6ef.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_加密_index.html.bfaa97bf.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_协议_index.html.d79c2e92.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_大会_index.html.671b60e0.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_平台_index.html.1473d3b1.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_开源_index.html.68009a9f.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_微调_index.html.00e98616.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_技术_index.html.7b9b8310.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_禁用_index.html.09d49f84.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_艺术_index.html.f039e09a.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_运营_index.html.01477a7c.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_随想_index.html.2130074f.js" as="script"><link rel="prefetch" href="/assets/js/tag_ai-reasoning_index.html.92107949.js" as="script"><link rel="prefetch" href="/assets/js/tag_optimization_index.html.91e14203.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_动漫_index.html.18233701.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_实习_index.html.99c01886.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_布局_index.html.c4128b34.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_训练_index.html.4a46a86c.js" as="script"><link rel="prefetch" href="/assets/js/tag_modelmerging_index.html.3aac36a9.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_辩论_index.html.08c79452.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_量化_index.html.9cd36f77.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_comfymind_index.html.780a1f96.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_framepack_index.html.316e41ca.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_inference_index.html.f694cf8d.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_replicate_index.html.01cc7a53.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_diffusers_index.html.16bfc806.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_reprint_index.html.03c0293c.js" as="script"><link rel="prefetch" href="/assets/js/tag_fine-tuning_index.html.ca8a4059.js" as="script"><link rel="prefetch" href="/assets/js/tag_news-agents_index.html.39bd1dd8.js" as="script"><link rel="prefetch" href="/assets/js/tag_page-config_index.html.fd4f47e9.js" as="script"><link rel="prefetch" href="/assets/js/tag_usage-guide_index.html.069580af.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_markdown_index.html.4742eec1.js" as="script"><link rel="prefetch" href="/assets/js/category_novice_index.html.7561c64e.js" as="script"><link rel="prefetch" href="/assets/js/category_papers_index.html.180b7727.js" as="script"><link rel="prefetch" href="/assets/js/tag_alibaba-ai_index.html.f5c3b72c.js" as="script"><link rel="prefetch" href="/assets/js/tag_art-lesson_index.html.6dc9ccf1.js" as="script"><link rel="prefetch" href="/assets/js/tag_base-model_index.html.3c68635b.js" as="script"><link rel="prefetch" href="/assets/js/tag_encryption_index.html.ba9c6633.js" as="script"><link rel="prefetch" href="/assets/js/tag_modelmerge_index.html.f79dfcbf.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_drawing_index.html.d6ef8b08.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_ovis-u1_index.html.aab779ab.js" as="script"><link rel="prefetch" href="/assets/js/category_guide_index.html.b6dbfe00.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_comfyui_index.html.52422e18.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_aiml_index.html.5317281e.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_flux.1_index.html.703ceaab.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_github_index.html.eacf79c2.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_lumina_index.html.451683ad.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_prompt_index.html.dbf5dc4c.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_script_index.html.0c284078.js" as="script"><link rel="prefetch" href="/assets/js/category_news_index.html.0acac10a.js" as="script"><link rel="prefetch" href="/assets/js/tag_ai-model_index.html.485c27f3.js" as="script"><link rel="prefetch" href="/assets/js/tag_amazon-q_index.html.9b480256.js" as="script"><link rel="prefetch" href="/assets/js/tag_creators_index.html.1bc91546.js" as="script"><link rel="prefetch" href="/assets/js/tag_protocol_index.html.715bc788.js" as="script"><link rel="prefetch" href="/assets/js/tag_training_index.html.3c2840b0.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_merge_index.html.9e5cebd3.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_model_index.html.c5e77c69.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_notan_index.html.d1fbfa1b.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_qlora_index.html.7832f634.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_qwen3_index.html.e94caed5.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_vpred_index.html.557ab6de.js" as="script"><link rel="prefetch" href="/assets/js/tag_disable_index.html.6ed89a53.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_aigc_index.html.51eda4be.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_flux_index.html.5425d666.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_llms_index.html.22e104c5.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_niji_index.html.697abc36.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_lora_index.html.eb2cf7fa.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_sdxl_index.html.aa7ab8d6.js" as="script"><link rel="prefetch" href="/assets/js/404.html.683d9275.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_qwen_index.html.ac1dad5c.js" as="script"><link rel="prefetch" href="/assets/js/tag_debate_index.html.347d53e0.js" as="script"><link rel="prefetch" href="/assets/js/tag_layout_index.html.5a5e41d2.js" as="script"><link rel="prefetch" href="/assets/js/tag_editor_index.html.2106f4d9.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_art_index.html.8deb8f79.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_aws_index.html.defb07a5.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_gtc_index.html.1f7cf941.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_llm_index.html.cdc71de0.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_mcp_index.html.2baec1bd.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_sdo_index.html.c2389a8d.js" as="script"><link rel="prefetch" href="/assets/js/tag_anime_index.html.a8a78e3b.js" as="script"><link rel="prefetch" href="/assets/js/tag_guide_index.html.5173996f.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_lu_index.html.42e4b29c.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_ai_index.html.5a7d4e11.js" as="script"><link rel="prefetch" href="/assets/js/tag_tech_index.html.68c57a8c.js" as="script"><link rel="prefetch" href="/assets/js/tag_tmux_index.html.569017df.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_hf-weekly_index.html.06e8093d.js" as="script"><link rel="prefetch" href="/assets/js/zh_timeline_index.html.e1f1ad46.js" as="script"><link rel="prefetch" href="/assets/js/zh_category_index.html.682e7f3d.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_tutorials_comfyui_index.html.9ed7a546.js" as="script"><link rel="prefetch" href="/assets/js/zh_article_index.html.cd7570ad.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_papers_index.html.bb25e136.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-weekly_index.html.6f900118.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_hf-weekly_index.html.4d97c51d.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_templates_index.html.5a13aa05.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_tutorials_index.html.5b1c58de.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_index.html.2bfde6cd.js" as="script"><link rel="prefetch" href="/assets/js/zh_tag_index.html.3201a8c7.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_workflows_repos_index.html.d93bee28.js" as="script"><link rel="prefetch" href="/assets/js/zh_star_index.html.f6186c0f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ai-impls_index.html.dc9f8315.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_reprints_index.html.9a49e407.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_thoughts_index.html.e1e18711.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_prompts_index.html.e3e3d99c.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_papers_index.html.5e8ec60f.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_dairys_index.html.6cd7d11b.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_ielts_index.html.727f9a48.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_repos_index.html.ce9635f7.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_index.html.c78cbce3.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_web_index.html.6fe04daf.js" as="script"><link rel="prefetch" href="/assets/js/zh_posts_sci_index.html.e71977aa.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/" aria-label="Take me home"><img class="vp-nav-logo" src="/logo.svg" alt><!----><span class="vp-site-name hide-in-pad">Nlog</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/home.html" aria-label="home"><!---->home<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/demo/" aria-label="Features demo"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="laptop-code" width="1em" height="1em"></iconify-icon><!--]-->Features demo<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Posts"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon>Posts<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Apple</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/apple/1.html" aria-label="Apple1"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon><!--]-->Apple1<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/apple/2.html" aria-label="Apple2"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon><!--]-->Apple2<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/apple/3.html" aria-label="/posts/apple/3.html"><!---->/posts/apple/3.html<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/apple/4.html" aria-label="/posts/apple/4.html"><!---->/posts/apple/4.html<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Banana</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/banana/1.html" aria-label="Banana 1"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon><!--]-->Banana 1<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/banana/2.html" aria-label="Banana 2"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon><!--]-->Banana 2<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/banana/3.html" aria-label="/posts/banana/3.html"><!---->/posts/banana/3.html<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/posts/banana/4.html" aria-label="/posts/banana/4.html"><!---->/posts/banana/4.html<!----></a></li></ul></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/posts/cherry.html" aria-label="Cherry"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon><!--]-->Cherry<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/posts/dragonfruit.html" aria-label="Dragon Fruit"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="pen-to-square" width="1em" height="1em"></iconify-icon><!--]-->Dragon Fruit<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/posts/tomato.html" aria-label="/posts/tomato.html"><!---->/posts/tomato.html<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/posts/strawberry.html" aria-label="/posts/strawberry.html"><!---->/posts/strawberry.html<!----></a></li></ul></button></div></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><div class="vp-nav-item"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Select language"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" name="i18n" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link route-link-active auto-link" href="/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.html" aria-label="English"><!---->English<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/zh/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.html" aria-label="简体中文"><!---->简体中文<!----></a></li></ul></button></div></div><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/vuepress-theme-hope/vuepress-theme-hope" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/" aria-label="Blog Home"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="home" width="1em" height="1em"></iconify-icon><!--]-->Blog Home<!----></a></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="laptop-code" width="1em" height="1em"></iconify-icon><a class="route-link auto-link vp-sidebar-title no-external-link-icon" href="/demo/" aria-label="Demo"><!---->Demo<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/demo/layout.html" aria-label="Layout"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="object-group" width="1em" height="1em"></iconify-icon><!--]-->Layout<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/demo/markdown.html" aria-label="Markdown Enhance"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="fab fa-markdown" width="1em" height="1em"></iconify-icon><!--]-->Markdown Enhance<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/demo/page.html" aria-label="Page Config"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="file" width="1em" height="1em"></iconify-icon><!--]-->Page Config<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/demo/disable.html" aria-label="Disabling layout and features"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="gears" width="1em" height="1em"></iconify-icon><!--]-->Disabling layout and features<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/demo/encrypt.html" aria-label="Encryption Article"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="lock" width="1em" height="1em"></iconify-icon><!--]-->Encryption Article<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header active"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="book" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">Articles</span><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">Reprints</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/generative-ai-powered-design.html" aria-label="Generative AI-Powered Design: Creating Game Environments with SD3.5 Large"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="openmoji:video-game" width="1em" height="1em"></iconify-icon><!--]-->Generative AI-Powered Design: Creating Game Environments with SD3.5 Large<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/niji-lesson-1-fundamentals-of-measurement-and-abstraction-the-theory-of-how-to-draw-everything.html" aria-label="Lesson 1: Fundamentals of Measurement and Abstraction: The Theory of (How to Draw) Everything"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="palette" width="1em" height="1em"></iconify-icon><!--]-->Lesson 1: Fundamentals of Measurement and Abstraction: The Theory of (How to Draw) Everything<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/mcp-flash-in-the-pan-or-future-standard.html" aria-label="MCP: Flash in the Pan or Future Standard?"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="openmoji:code-editor" width="1em" height="1em"></iconify-icon><!--]-->MCP: Flash in the Pan or Future Standard?<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/niji-lesson-2-the-terminator-line.html" aria-label="Lesson 2: The Terminator (Line)"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="palette" width="1em" height="1em"></iconify-icon><!--]-->Lesson 2: The Terminator (Line)<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/niji-study-1-measuring-with-your-eyes.html" aria-label="Study 1: Measuring With Your Eyes"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="ruler" width="1em" height="1em"></iconify-icon><!--]-->Study 1: Measuring With Your Eyes<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/niji-study-2-notan.html" aria-label="Study 2: Notan"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="contrast" width="1em" height="1em"></iconify-icon><!--]-->Study 2: Notan<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/flux-qlora.html" aria-label="(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware"><!---->(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/model-block-merge-1.html" aria-label="[Experiment Report] Investigating the Influence of Each U-Net Layer with Model Block Merge #1"><!---->[Experiment Report] Investigating the Influence of Each U-Net Layer with Model Block Merge #1<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/model-block-merge-2.html" aria-label="[Experiment Report] Investigating the Influence of Each U-Net Layer with Model Block Merge #1"><!---->[Experiment Report] Investigating the Influence of Each U-Net Layer with Model Block Merge #1<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/announcing-illustrious-text%E2%80%91enhancer-tag-booster-and-mood-enhancer.html" aria-label="Announcing Illustrious Text‑Enhancer: Tag Booster &amp; Mood Enhancer"><!---->Announcing Illustrious Text‑Enhancer: Tag Booster &amp; Mood Enhancer<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/ai-art-gtc-paris-2025.html" aria-label="Artists, Fashion Designers Tap State-of-the-Art AI for NVIDIA GTC Paris Gallery"><!---->Artists, Fashion Designers Tap State-of-the-Art AI for NVIDIA GTC Paris Gallery<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/blog-images.html" aria-label="Blog Images Generator"><!---->Blog Images Generator<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/news-agents-daily-recap.html" aria-label="Building News Agents for Daily News Recaps with MCP, Q, and tmux"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="fa-solid:newspaper" width="1em" height="1em"></iconify-icon><!--]-->Building News Agents for Daily News Recaps with MCP, Q, and tmux<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/crody&#39;s-model-merge-guide.html" aria-label="Crody&#39;s Model Merge Guide // Team-C"><!---->Crody&#39;s Model Merge Guide // Team-C<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/experimenting-with-mcp-using-github-copilot.html" aria-label="Experimenting with MCP using GitHub Copilot"><!---->Experimenting with MCP using GitHub Copilot<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/explaining-tokens-the-language-and-currency-of-ai-nvidia-blog.html" aria-label="Explaining Tokens — the Language and Currency of AI"><!---->Explaining Tokens — the Language and Currency of AI<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/generate-consistent-characters.html" aria-label="Generate consistent characters – Replicate blog"><!---->Generate consistent characters – Replicate blog<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/how-and-when-to-build-multi-agent-systems.html" aria-label="How and when to build multi-agent systems"><!---->How and when to build multi-agent systems<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/original-character-lora-sdxl-character-training.html" aria-label="How to create an original character LoRA [SDXL Training] SDXL Character Training"><!---->How to create an original character LoRA [SDXL Training] SDXL Character Training<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/niji-video.html" aria-label="How to make videos with niji・journey"><!---->How to make videos with niji・journey<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/flux-kontext-optimization.html" aria-label="How we optimized FLUX.1 Kontext [dev]"><!---->How we optimized FLUX.1 Kontext [dev]<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/illustrious-xl-3.0-3.5-vpred-2048-resolution-and-natural-language.html" aria-label="Illustrious XL 3.0-3.5-vpred: 2048 Resolution and Natural Language"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="mdi:paint-outline" width="1em" height="1em"></iconify-icon><!--]-->Illustrious XL 3.0-3.5-vpred: 2048 Resolution and Natural Language<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/illustrious-xl-v2.0-the-best-training-base-model-in-1536-age.html" aria-label="Illustrious XL v2.0—The best training base model in 1536 age"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="mdi:paint-outline" width="1em" height="1em"></iconify-icon><!--]-->Illustrious XL v2.0—The best training base model in 1536 age<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/illustrious-lu-v0.03.html" aria-label="Illustrious-LU v0.03"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="fa-solid:microscope" width="1em" height="1em"></iconify-icon><!--]-->Illustrious-LU v0.03<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/flux-1-kontext.html" aria-label="Introducing FLUX.1 Kontext and the BFL Playground"><!---->Introducing FLUX.1 Kontext and the BFL Playground<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/introduction-of-prompts-ai-illustration-generation-camera-angle-composition-facial-expression.html" aria-label="Introduction of prompts in AI illustration generation [composition / camera angle / facial expression]"><!---->Introduction of prompts in AI illustration generation [composition / camera angle / facial expression]<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview.html" aria-label="Qwen3: Next-Gen AI with Hybrid Thinking and Multilingual Mastery | 2025 Overview"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="fa-solid:robot" width="1em" height="1em"></iconify-icon><!--]-->Qwen3: Next-Gen AI with Hybrid Thinking and Multilingual Mastery | 2025 Overview<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.html" aria-label="Step by Step visual introduction to Diffusion Models"><!---->Step by Step visual introduction to Diffusion Models<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/ai-art-newsletter-jan-25.html" aria-label="The AI tools for Art Newsletter - Issue 1"><!---->The AI tools for Art Newsletter - Issue 1<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/what-is-block-merging.html" aria-label="What is Block Merging?"><!---->What is Block Merging?<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/posts/reprints/image-recognition.html" aria-label="What Is Image Recognition? Algorithms and Applications"><!---->What Is Image Recognition? Algorithms and Applications<!----></a></li></ul></section></li></ul></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/intro.html" aria-label="Intro Page"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="circle-info" width="1em" height="1em"></iconify-icon><!--]-->Intro Page<!----></a></li><li><a class="auto-link external-link vp-sidebar-link" href="https://ecosystem.vuejs.press/plugins/markdown/revealjs/demo.html" aria-label="Slides" rel="noopener noreferrer" target="_blank"><!--[--><iconify-icon class="font-icon icon" style="" mode="style" inline icon="person-chalkboard" width="1em" height="1em"></iconify-icon><!--]-->Slides<!----></a></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Step by Step visual introduction to Diffusion Models</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Kemal Erdem</span></span><span property="author" content="Kemal Erdem"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">November 1, 2023</span><meta property="datePublished" content="2023-11-01T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 14 min</span><meta property="timeRequired" content="PT14M"></span><!----><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color5 clickable" role="navigation">Machine Learning</span><span class="page-tag-item color1 clickable" role="navigation">Diffusion Models</span><!--]--><meta property="keywords" content="Machine Learning,Diffusion Models"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#what-is-a-diffusion-model">What is a diffusion model?</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#forward-diffusion-process">Forward diffusion process</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#reverse-diffusion-process">Reverse diffusion process</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#reverse-diffusion-misconception">Reverse diffusion misconception</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#some-math-you-can-skip-but-probably-worth-reading">Some math (you can skip but probably worth reading)</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#reverse-diffusion-output-visualization">Reverse diffusion output visualization</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#architecture">Architecture</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#training">Training</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#conclusions">Conclusions</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#references">References</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#citation">Citation</a></li><!----><!--]--></ul></li><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content" vp-content><h1 id="step-by-step-visual-introduction-to-diffusion-models" tabindex="-1"><a class="header-anchor" href="#step-by-step-visual-introduction-to-diffusion-models"><span>Step by Step visual introduction to Diffusion Models</span></a></h1><h2 id="what-is-a-diffusion-model" tabindex="-1"><a class="header-anchor" href="#what-is-a-diffusion-model"><span>What is a diffusion model?</span></a></h2><p>The idea of the diffusion model is not that old. In the 2015 paper called <strong>“Deep Unsupervised Learning using Nonequilibrium Thermodynamics”</strong>[1], the Authors described it like this:</p><blockquote><p>The essential idea, inspired by non-equilibrium statistical physics, is to systematically and <strong>slowly destroy structure in a data distribution</strong> through an <strong>iterative forward diffusion process</strong>. We then learn a <strong>reverse diffusion process</strong> that <strong>restores structure in data</strong>, yielding a highly flexible and tractable generative model of the data.</p></blockquote><p>Here <strong><em>diffusion process</em></strong> is split into <strong><em>forward</em></strong> and <strong><em>reverse</em></strong> diffusion processes. The forward diffusion process is a process of turning an image into noise, and the reverse diffusion process is supposed to turn that noise into the image again.</p><h2 id="forward-diffusion-process" tabindex="-1"><a class="header-anchor" href="#forward-diffusion-process"><span>Forward diffusion process</span></a></h2><p>If the previous definition doesn’t explain much, don’t worry, we can explain why and how it works. First, you need to know how to destroy structure in a data distribution.</p><div style="display:flex;gap:2%;align-items:flex-start;"><div style="flex:1;text-align:center;"><img src="https://erdem.pl/static/82f2bcfc15e077527beecb52281869e5/e4a55/noise0.jpg" alt="Original Image" style="max-width:100%;height:auto;"><div>(a) Original Image.</div></div><div style="flex:1;text-align:center;"><img src="https://erdem.pl/static/3c7977d706ed0a2c269e61c2e91af0ce/e4a55/noise10.jpg" alt="Pure noise" style="max-width:100%;height:auto;"><div>(b) Pure noise.</div></div></div><p>Figure 1: Input and output of the forward diffusion process</p><p>If we take any image (Fig. 1a), it has some none-random distribution. We don’t know the distribution, but our goal is to destroy it so we can do it by adding a noise to it. At the end of that process, we should end up with noise similar to pure noise.</p><p>Figure 2: Forward diffusion process using only 10 steps,</p><p>Each step of the forward diffusion process is defined as</p><p>$$ q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I) $$</p><p>Where <strong>q</strong> is our forward process, $x_t$ is the output of the forward process at step $t$ (naturally $x_{t-1}$ is an input at step $t$). $\mathcal{N}$ is a normal distribution, $\sqrt{1-\beta_t}x_{t-1}$ is our mean and $\beta_t I$ defines a variance.</p><h4 id="schedule" tabindex="-1"><a class="header-anchor" href="#schedule"><span>Schedule</span></a></h4><p>$\beta_t$ refers to something called <strong>schedule</strong> and values can range from 0 to 1. The values are usually kept low to prevent variance from exploding. The 2020 paper[2] uses a <strong>linear schedule</strong> so the output looks like that:</p><figure><img src="https://erdem.pl/static/c5e1011bc11d7f13fdf19295c0e94c3e/eea4a/linear_noise.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Figure 3: Entire forward diffusion process using <strong>linear schedule</strong> with 1000 time steps.</p><p>In this case, β t\beta_t β t​ ranges from <strong>0.0001</strong> to <strong>0.02</strong> for the mean and variance behaves like in the Fig. 4.</p><figure><img src="https://erdem.pl/static/6bf29ce5feb6b15753aff769147e7be8/21b4d/lin_schedule_beta.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Figure 4: Change in <em>mean</em> and <em>variance</em> for a given timestep</p><p>Researchers from OpenAI in their 2021 paper[3] decided that using a linear schedule is not that efficient. As you’ve seen before, most of the information from the original image is lost after around half of the total steps. They designed their own schedule and called it the <strong>cosine schedule</strong> (Fig. 5). The improvement in schedule allowed them to reduce the number of steps to <strong>50</strong>.</p><figure><img src="https://erdem.pl/static/eb1c5091b8115b21499e408a8edf7b82/eea4a/cos_noise.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Figure 5: Forward diffusion process using <strong>cosine schedule</strong>.</p><h4 id="real-noising-process-only-last-equation-is-important" tabindex="-1"><a class="header-anchor" href="#real-noising-process-only-last-equation-is-important"><span>Real noising process (only last equation is important)</span></a></h4><p>As you can imagine, adding noise to the image using a forward diffusion process is going to be slow. <strong>The training process doesn’t use examples in line with the forward process but rather it uses samples from arbitrary timestep t</strong>. This means at each training step we would need to iterate through t steps to generate 1 training sample. But 2020 paper has the solution, but first, we have to define the entire noise to be added at $T$ as:</p><p>$$ q(x_{1:T}|x_0) := \prod_{t=1}^{T} q(x_t|x_{t-1}) $$</p><p>The paper describes the whole transition very poorly (maybe because the authors are just doing that kind of math in their heads). We’re going to do it manually. If you think about the diffusion process, it works like a function composition.</p><p>$$ q_t(q_{t-1}(q_{t-2}(\cdots q_1(x_0)))) $$</p><p>If we now want to write the normal distribution formula for $t=1$ it will look like</p><p>$$ q(x_1|x_0) = \mathcal{N}(x_1; \sqrt{1-\beta_1}x_0, \beta_1 I) $$</p><p>Then in the next step the formula uses the result from the $t=1$ step:</p><p>$$ q(x_2|x_1) = \mathcal{N}(x_2; \sqrt{1-\beta_2}x_1, \beta_2 I) $$</p><p>We can see that only mean is dependent on the previous step ($\beta$ is known for every $t$, so we don’t have to worry about variance). Authors introduced some notation:</p><ul><li>$\alpha_t = 1 - \beta_t$</li><li>$\bar\alpha_t := \prod_{s=1}^t \alpha_s$</li></ul><p>This notation is only for the ease of transformation. Now our formula looks like this:</p><p>$$ q(x_1|x_0) = \mathcal{N}(x_1; \sqrt{\alpha_1}x_0, (1-\alpha_1)I) $$</p><p>Because we know how the mean is changing through the steps:</p><p>$$ \mu_t = \sqrt{\bar\alpha_t}x_0 $$</p><p>If we know how to calculate mean for the entire process between $t=0$ and $t$ we can finally write down the entire formula:</p><p>$$ q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar\alpha_t}x_0, (1-\bar\alpha_t)I) $$</p><p>Everything is great but that is not computable. First, we need to apply the reparameterization trick ($\mathcal{N}(\mu, \sigma^2) = \mu + \sigma \epsilon$) on a single forward step definition.</p><p>$$ q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I) = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t}\epsilon $$</p><p>where $\epsilon \sim \mathcal{N}(0, 1)$.</p><ul><li>ϵ\epsilon ϵ is from N(0,1)\mathcal{N}(0,1)N(0,1)</li></ul><p>Because we know what is the formula for <strong>t=1..T</strong>, we could use this trick to get the computable equation</p><p>$$ q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar\alpha_t}x_0, (1-\bar\alpha_t)I) = \sqrt{\bar\alpha_t}x_0 + \sqrt{1- \bar\alpha_t}\epsilon $$</p><p>With that equation, we can calculate noise at any arbitrary step <strong>t</strong> (α ˉ t\bar\alpha_t α ˉ t​ is known because β t\beta_t β t​ is known) without going through the process.</p><h2 id="reverse-diffusion-process" tabindex="-1"><a class="header-anchor" href="#reverse-diffusion-process"><span>Reverse diffusion process</span></a></h2><p>As you probably figured out, the goal of the reverse diffusion process is to convert pure noise into an image. To do that, we’re going to use some neural network (ignore architecture for now, we’ll get into it soon). If you’re familiar with GANs (Generative Adversarial Networks) (Fig. 6), we’re trying to train something similar to the <em>generator network</em>. The only difference is that our network will have an easier job because it doesn’t have to do all the work in one step.</p><figure><img src="https://erdem.pl/869cd128ff69bb1d304675166e8b7b34/gan_diagram_generator.svg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Figure 6: GAN architecture.</p><p>Okay, so why not just use GANs? It took some very smart people a long time to figure out how to achieve decent results with that architecture. It is extremely difficult to train a network to convert some random noise into a meaningful image. The authors of the 2015 paper [1] figured out that switching to a multistep framework and removing some noise at the time is just more efficient and easy to train.</p><blockquote><p>Learning in this framework involves estimating small perturbations to a diffusion process. <strong>Estimating small perturbations is more tractable</strong> than explicitly describing the full distribution with a single, non-analytically-normalizable, potential function. Furthermore, since a diffusion process exists for any smooth target distribution, this method can capture data distributions of arbitrary form.</p></blockquote><h3 id="reverse-diffusion-misconception" tabindex="-1"><a class="header-anchor" href="#reverse-diffusion-misconception"><span>Reverse diffusion misconception</span></a></h3><p>You’ve probably heard that the <em>“Diffusion probabilistic model is a parameterized Markov Chain”</em>. That is true, but for some reason, people have a wrong idea about what the <strong>neural network</strong> does in the diffusion model. In the 2020 paper [2] authors are using this graph to describe the process.</p><figure><img src="https://erdem.pl/static/897bdd95a9ca14cff82e0257de0dccc3/ee515/graphical_diffusion_model.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Figure 7: The directed graphical model.</p><p>Usually, the neural network is visualized like this:</p><figure><img src="https://erdem.pl/static/512d5abbaa2b507956d1b737f85cee1b/34e70/one_diffusion_step.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Figure 8: High level visualization of one step from the reverse diffusion process.</p><p>Because of that, a lot of people think that a neural network (called <em>diffusion model</em> for even more confusion) is removing noise from an input image or predicting the noise to be removed from an input. Both are incorrect. <strong>Diffusion model predicts the entire noise to be removed in a given timestep</strong>. This means that if we have timestep <strong>t=600</strong> then our Diffusion model tries to predict the entire noise on which removal we should get to <strong>t=0</strong>, not <strong>t=599</strong>. I’ll explain everything in a second but first, just look at the step by step reverse diffusion process.</p><p>Please notice that I’ve scaled the number of steps from 1000 to 10. This is because it wouldn’t be very distinguishable for humans to compare results between steps 785 and 784.</p><p>Figure 9: Reverse diffusion process</p><h3 id="some-math-you-can-skip-but-probably-worth-reading" tabindex="-1"><a class="header-anchor" href="#some-math-you-can-skip-but-probably-worth-reading"><span>Some math (you can skip but probably worth reading)</span></a></h3><p>The process looks very simple, but you probably have some questions like <em>“where did you get that equation for output from?”</em>. First, we need to copy the equation for the reverse process from the 2020 paper[2]:</p><p>$$ p_\theta(x_{0:T}) := p(x_T) \prod_{t=1}^{T} p_\theta(x_{t-1}|x_t) $$</p><p>where:</p><p>$$ p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t,t), \sum_\theta(x_t,t)) $$</p><p>This might seem complicated but it basically says that p θ(x 0:T)p_\theta(x_{0:T})p θ​(x 0:T​) (diffusion process) is a <strong>chain of gaussian transitions</strong> starting at p(x T)p(x_T)p(x T​) and <strong>iterating T times</strong> using the equation for one diffusion process step p θ(x t−1​∣x t​).</p><p>Now it’s time to explain how the single step works and how to get something to implement. N(x t−1​,μ θ​(x t​,t),∑θ​(x t​,t))\mathcal{N}(x_{t-1}, \mu_\theta(x_t,t), \sum_{\theta}(x_t,t))N(x t−1​,μ θ​(x t​,t),∑θ​(x t​,t)) has 2 parts:</p><ul><li>μ θ(x t,t)\mu_\theta(x_t,t)μ θ​(x t​,t) (mean)</li><li>∑θ(x t,t)\sum_{\theta}(x_t,t)∑θ​(x t​,t) which equals σ t 2 I\sigma_t^2I σ t 2​I (variance)</li></ul><p>The Authors of the 2020 paper decided to set the second part to be <strong>time-dependent but not trainable</strong>. It’s not set to be constant but rather set to equal β T I\beta_TI β T​I. This is the same beta from the schedule before. Now the only thing that is left is the first part (mean). To be honest, I’m not the best person to explain what happens next (mostly because I don’t consider myself to be a mathematical mid). There are far smarter people, and one of them is <em>Lilian Weng</em> who described the hard math of the reverse process [6] (also check Appendix A in the 2020 paper [2]). What we need to know from all of this is that</p><p>$$ \mu_\theta(x_t,t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar\alpha_t}} \epsilon_\theta(x_t, t) \right) $$</p><p>and that gives us</p><p>$$ x_{t-1} = \mathcal{N}\left(x_{t-1}, \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1 - \bar\alpha_t}} \epsilon_\theta(x_t, t)), \sqrt{\beta_t}\epsilon\right) $$</p><p>which we can use to calculate the output value for a given timestep t</p><p>$$ x_{t-1} = \frac{1}{\sqrt{a_t}} \left(x_t - \frac{\beta_t}{\sqrt{1-\bar\alpha}} \epsilon_\theta(x_t,t)\right) + \sqrt{\beta_t}\epsilon $$</p><p>where:</p><ul><li>ϵ θ(x t,t)\epsilon_\theta(x_t,t)ϵ θ​(x t​,t) is our <strong>model’s output</strong> (predicted noise)</li></ul><p>As shown in Fig. 9, the last iteration of the reverse process doesn’t add the noise β t ϵ\sqrt{\beta_t}\epsilon β t​​ϵ to the output. This is because we’re at the last step, and we wouldn’t be able to remove it.</p><h3 id="reverse-diffusion-output-visualization" tabindex="-1"><a class="header-anchor" href="#reverse-diffusion-output-visualization"><span>Reverse diffusion output visualization</span></a></h3><p>Before we get into architecture, I want to show you one thing that was very interesting to me (and might be for you). As described previously, each time we predict the noise using a neural network, we subtract part of it and move to the next step. That is how the diffusion process works. But <strong>what will happen if we just subtract all the noise?</strong> I’ve generated an example of such subtraction for every timestep (&lt;1,50&gt; with a linear schedule).</p><blockquote><p><strong>Notice!</strong> Because this is a reverse process when we say <strong>t=1</strong>, the value of the β t\beta_t β t​ is set to β T−t+1\beta_{T-t+1}β T−t+1​, where <strong>T</strong> is the total number of steps. E.g. when <em>t=1</em> we’re using β 50\beta_{50}β 50​ for <em>t=2</em> we’re using β 49\beta_{49}β 49​ and so on.</p></blockquote><figure><img src="https://erdem.pl/static/90207efb3cb80cde5f359b2205b0303a/0f840/sample_full_noise_removal.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Figure 10: Result of full noise removal</p><p>The middle image is an input at step t. At t=0 that input is just a random noise. The Image on the right is the noise predicted by the neural network. The Image on the left is just the input at step t with the whole noise subtracted. This is done separately at each step. I’ve picked only some of the steps, but if you want to see all of them please check the <a href="https://drive.google.com/file/d/13UCkMZCs_AktbkEAAmJ_jeumuZlGFC0e/view?usp=sharing" target="_blank" rel="noopener noreferrer">full diffusion process with noise removed (gDrive)</a>.</p><p>As said at the start of the article, a diffusion model used in the diffusion process works similarly to GAN’s generator only worse. Well, maybe not worse but rather worse in one-step denoising. If you look at the result of removing the entire noise generated at <em>t=1</em>, you can see that the result is kind of similar to the end image generated by the whole process. The reason for it is that we’re training the network to predict the whole noise, not the diff. A perfect model should predict exactly the noise that would produce the right image. It doesn’t because it would require learning exact data distribution, which is practically impossible.</p><p>There are 2 things you can take from this example:</p><ol><li>You can use <strong>fewer timesteps in your schedule</strong> when doing the inference after the model is trained.</li><li>You can use a <strong>different schedule</strong> when doing the inference.</li></ol><p>First should be obvious when your network predicts the noise that is already quite good, you can make larger “jumps”. Larger because the β\beta β range remains the same, only the slope changes. The second is less straightforward, but you can use different schedules with different slopes (e.g. you can train with a <em>linear schedule</em> and inference with a <em>cosine schedule</em>).</p><h2 id="architecture" tabindex="-1"><a class="header-anchor" href="#architecture"><span>Architecture</span></a></h2><p>Finally, we can move to discussing the architecture.</p><p>For those who like reading pytorch visualizations, <a href="https://drive.google.com/file/d/1XVwlD8wuTazW2myf4sV-TZF_YkIfwL7z/view?usp=sharing" target="_blank" rel="noopener noreferrer">HERE (gDrive)</a> is a full model (including label embedder).</p><p>Figure 11: Diffusion model architecture.</p><p>Model architecture is a modified U-Net [7] architecture. It is quite simple, but it gets complicated later with further improvements to diffusion models (e.g. stable diffusion added the entire latent layer for image data embedding). We’re discussing only the first versions of the diffusion model because if you know that you’ll be able to understand improvements later.</p><h4 id="embeddings" tabindex="-1"><a class="header-anchor" href="#embeddings"><span>Embeddings</span></a></h4><p>Before we get to the architecture of individual blocks, we should discuss how the information about timestep and prompt is passed to the network. If you look at Fig. 8, it shows as if the diffusion model just processes the input image with noise. This isn’t true, each step in the process adds embedding with information about the current timestep and prompt (if the model supports prompting, first diffusion models weren’t). To do that, we need to use <strong>sinusoidal encoding for encoding timestep t</strong>, and some kind of <strong>embedder for our prompt</strong>. I’ve described how this type of encoding works in my article about <a href="https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers#positional-encoding-visualization" target="_blank" rel="noopener noreferrer">positional encoding in transformers</a>.</p><h5 id="embedder" tabindex="-1"><a class="header-anchor" href="#embedder"><span>Embedder</span></a></h5><p>Embedder could be any network that embeds your prompt. In the first conditional diffusion models (ones with prompting) there was no reason to use complicated embedders. E.g. in the network I’ve trained for this article, I’ve used the <a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener noreferrer">CIFAR-10 dataset</a> which has only 10 classes. In this case, the job of the embedder is just to encode these classes. If you’re working with more complicated datasets, especially those without annotations, you might want to use embedders like CLIP. Then you’ll be able to prompt the model with any text you want to generate images. At the same time, you need to use that embedder in the training process.</p><p>Outputs from the positional encoding and text embedder are added to each other and passed into downsample and upsample blocks.</p><h5 id="resnet-block" tabindex="-1"><a class="header-anchor" href="#resnet-block"><span>ResNet Block</span></a></h5><figure><img src="https://erdem.pl/static/18fc014ed4e2d9fe4d1439b3431ccad9/d38a6/conv_block.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Figure 12: ResNet block</p><p>The First block we’re going to discuss is ResNet Block. The the ResNet block in this version is simple and linear. This block is used later as a part of downsample and upsample blocks.</p><h5 id="downsample-block" tabindex="-1"><a class="header-anchor" href="#downsample-block"><span>Downsample Block</span></a></h5><figure><img src="https://erdem.pl/static/7767bbec58461aa09c8032a5dc9bf06c/409e6/downsample.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Figure 13: Downsample block</p><p>Downsample block is the first block that receives not only data from the previous layer but also data about timestep and prompt. <strong>This block has 2 inputs</strong> and behaves as a standard downsample from the U-Net architecture. It gets an input and downsample it to the size of the next layer. It uses the MaxPool2d layer (kernel size 2) which halves the input size (64x64 -&gt; 32x32). After that, we have <strong>2 ResNet blocks</strong> (the same as the entire layer just before in Fig. 12).</p><p>Embeddings are processed with a Sigmoid Linear Unit and sent through a simple linear layer to achieve the same shape as the output from the ResNet block. After that, <strong>two tensors are added to each other</strong> and sent to the next block.</p><h5 id="self-attention-block" tabindex="-1"><a class="header-anchor" href="#self-attention-block"><span>Self-Attention Block</span></a></h5><figure><img src="https://erdem.pl/static/2f26fadad0f8290c51b1b8579c008aeb/41d3c/attention.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Figure 14: Self-Attention block</p><p>Some of the ResNet blocks from the original U-Net were replaced with Attention Blocks. If you don’t know what Attention is, I’ve written an <a href="https://erdem.pl/2021/05/introduction-to-attention-mechanism" target="_blank" rel="noopener noreferrer">article about it</a>.</p><p>To make attention work, we need to reshape our input. All attention blocks are structured the same, so I’m going to use the first one to describe all of them (the one just after the first downsample). It receives a downsampled tensor with shape (128, 32, 32). This is going to be Multi-Head Attention (MHA) with embedding dimension set to 128 and 4 attention heads. <strong>Embedding dimension changes</strong> between attention blocks (depending on the input length), and the <strong>number of heads stays the same</strong>.</p><p>To use our MHA we need to modify the input and create <strong>Q,K,V</strong> tensors. Our input has a length of 128 and a size of 32x32. First, we <strong>squash the last two dimensions and then flip the resulting tensor</strong> (128,32,32) -&gt; (128, 1024) -&gt; (1024, 128). We pass that through the Layer Normalization and use it as all 3 input tensors (Q,K,V).</p><p>Inside the block, the authors added <strong>2 skip connections</strong> to be combined with self-attention output. The first one adds pure reshaped input to the output from the attention layer and passes it through the forward layer (Normalization -&gt; Linear -&gt; GELU -&gt; Linera). The second one gets output from that layer and adds it back to the output from the attention. In the end, we need to <strong>reverse initial reshaping</strong> of the tensor (1024, 128) -&gt; (128, 32, 32).</p><h5 id="upsample-block" tabindex="-1"><a class="header-anchor" href="#upsample-block"><span>Upsample block</span></a></h5><figure><img src="https://erdem.pl/static/e56f851d5e6c5e8347979cede14afc27/42a8d/upsample.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Figure 15: Upsample block</p><p>Upsample is a little more complicated because we have 3 inputs. Input from the previous layer (in the case 5th self-attention block) <strong>has incompatible dimensions</strong>. Because it is the “upsample” block, it uses a simple Upsample layer with a scale factor of 2. After passing the input tensor through the upsample layer, we can <strong>concatenate it with residual connection</strong> (from the first ResNet block). Now both of them have the same shapes (64, 64, 64). Concatenated tensors are then sent through 2 ResNet blocks (the same as we did in the downsample block).</p><p>3rd input is (the same as in the downsample block again) sent through SILU and Linear layer and later added to the result from the 2nd ResNet block.</p><p>The Entire architecture ends with Conv2d layer, which uses a kernel size of 1 to scale our tensor from (64,64,64) back to (3, 64,64). <strong>This is our predicted noise</strong>.</p><h2 id="training" tabindex="-1"><a class="header-anchor" href="#training"><span>Training</span></a></h2><p>This is going to be the shortest section of them all. Training is just dead simple, here is the entire pseudocode from the paper:</p><p>1: repeat<br> 2: $x_0 \sim q(x_0)$<br> 3: $t \sim \mathrm{Uniform}({1,\ldots,T})$<br> 4: $\epsilon \sim \mathcal{N}(0,I)$<br> 5: Take gradient descent step on $\nabla_\theta | \epsilon - \epsilon_\theta(\sqrt{\bar\alpha_t} x_0 + \sqrt{1-\bar\alpha_t} \epsilon, t) |^2$<br> 6: until converged</p><p>First, we sample an image from our dataset (2:). Then we sample timestep <strong>t</strong> (3:) and finally sample noise from the normal distribution (4:). As you remember, we’ve defined how to apply noise at <strong>t</strong> without going through the iterative process:</p><p>$$ q(x_t|x_0) = \sqrt{\bar\alpha_t}x_0 + \sqrt{1-\bar\alpha_t}\epsilon $$</p><p>Now we optimize the objective via the gradient descent (5:). That’s it, now we repeat the entire process until it converges and the model is ready.</p><h2 id="conclusions" tabindex="-1"><a class="header-anchor" href="#conclusions"><span>Conclusions</span></a></h2><p>I know that the article is quite lengthy, and it could be shortened to half of it (if we drop the math part), but I hope it’s easy to read and understand. This is just an explanation of the early versions of the diffusion models. It doesn’t include some further work like CFG (Classifier-Free Guidance [5]), negative prompting, LORAs, ControlNets, and many others. I’ll try to describe all of that in the upcoming articles, but for now, let me summarize the most important things you should remember about diffusion models:</p><ul><li>The Diffusion process consists of <strong>forward diffusion</strong> and <strong>reverse diffusion</strong></li><li>Forward diffusion is used to add noise to the input image using a <strong>schedule</strong></li><li>There are different types of schedules (we’ve used linear), and they decide how much noise is added at the given step <strong>t</strong></li><li>We don’t have to use an iterative process to add noise, it can be done in one step with the equation described in the <strong>Real noising process</strong> section</li><li>Reverse diffusion consists of multiple steps in which a <strong>small amount of noise is removed at every step</strong> (<strong>equation</strong>)</li><li><strong>Diffusion model predicts the entire noise</strong>, not the difference between step <strong>t</strong> and <strong>t-1</strong></li><li>You might use a different schedule for inference than for training (including a different number of steps)</li><li>Diffusion model uses a <strong>modified U-Net architecture</strong></li><li>To add information about timestep <strong>t</strong> and prompt, we use <strong>sinusoidal encoding</strong> and <strong>text embedder</strong></li><li>Some of the ResNet blocks are replaced with <strong>Self-Attention blocks</strong></li></ul><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references"><span>References</span></a></h2><ol><li><a href="https://arxiv.org/abs/1503.03585" target="_blank" rel="noopener noreferrer">Deep Unsupervised Learning using Nonequilibrium Thermodynamics（论文）</a></li><li><a href="https://arxiv.org/abs/2006.11239" target="_blank" rel="noopener noreferrer">Denoising Diffusion Probabilistic Models（论文）</a></li><li><a href="https://arxiv.org/abs/2102.09672" target="_blank" rel="noopener noreferrer">Improved Denoising Diffusion Probabilistic Models（论文）</a></li><li><a href="https://arxiv.org/abs/2105.05233" target="_blank" rel="noopener noreferrer">Diffusion Models Beat GANs on Image Synthesis（论文）</a></li><li><a href="https://arxiv.org/abs/2207.12598" target="_blank" rel="noopener noreferrer">Classifier-Free Diffusion Guidance（论文）</a></li><li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#reverse-diffusion-process" target="_blank" rel="noopener noreferrer">What are diffusion models?（博客）</a></li><li><a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener noreferrer">U-Net: Convolutional Networks for Biomedical Image Segmentation（论文）</a></li><li><a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener noreferrer">Learning Transferable Visual Models From Natural Language Supervision（论文）</a></li></ol><hr><h3 id="citation" tabindex="-1"><a class="header-anchor" href="#citation"><span>Citation</span></a></h3><blockquote><p>Kemal Erdem, (Nov 2023). &quot;Step by Step visual introduction to Diffusion Models.&quot;. <a href="https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models" target="_blank" rel="noopener noreferrer">https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models</a></p></blockquote><p><strong>or</strong></p><div class="language-bibtex line-numbers-mode" data-highlighter="shiki" data-ext="bibtex" data-title="bibtex" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">@article</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">erdem2023stepByStepVisualIntroductionToDiffusionModels</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">        title</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">   = </span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Step by Step visual introduction to Diffusion Models.</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">        author</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">  = </span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Kemal Erdem</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">        journal</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> = </span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">https://erdem.pl</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">        year</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    = </span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">2023</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">        month</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">   = </span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">Nov</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">        url</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">     = </span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">&quot;</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr><p>ML Developer, Software Architect, JS Engineer, Ultra-distance cyclist</p><p><strong>Kemal Erdem</strong> on Twitter: <a href="https://www.twitter.com/burnpiro" target="_blank" rel="noopener noreferrer">https://www.twitter.com/burnpiro</a></p></div><!--[--><div class="theme-hope-content"><Share colorful services="email,facebook,line,linkedin,messenger,qrcode,reddit,telegram,twitter"></Share></div><!--]--><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/vuepress-theme-hope/vuepress-theme-hope/edit/main/src/posts/reprints/step-by-step-visual-introduction-to-diffusion-models.md" aria-label="Edit this page on GitHub" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page on GitHub<!----></a></div><div class="vp-meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/posts/reprints/qwen3-next-gen-ai-with-hybrid-thinking-and-multilingual-mastery-2025-overview.html" aria-label="Qwen3: Next-Gen AI with Hybrid Thinking and Multilingual Mastery | 2025 Overview"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="fa-solid:robot" width="1em" height="1em"></iconify-icon>Qwen3: Next-Gen AI with Hybrid Thinking and Multilingual Mastery | 2025 Overview</div></a><a class="route-link auto-link next" href="/posts/reprints/ai-art-newsletter-jan-25.html" aria-label="The AI tools for Art Newsletter - Issue 1"><div class="hint">Next<span class="arrow end"></span></div><div class="link">The AI tools for Art Newsletter - Issue 1<!----></div></a></nav><!----><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">My own footer</div><div class="vp-copyright">Copyright © 2025 Kemal Erdem </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script src="/assets/js/runtime~app.d5565785.js" defer></script><script src="/assets/js/9156.64e2dfa0.js" defer></script><script src="/assets/js/app.02df9061.js" defer></script>
  </body>
</html>
