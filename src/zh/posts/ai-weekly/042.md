# MiniCPM4发布高效端侧模型 | Seedance 1.0引领视频生成进阶 | ComfyUI‑R1打造智能工作流引擎【AI周报】

![封面源自C站作者Koal2](https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/bc7e4820-82c0-4d95-a196-9627d8cb4578/original=true,quality=90/00003-2892726208.jpeg)

## 摘要

本周亮点：MiniCPM4发布超高效端侧模型；Seedance 1.0引领视频生成；ComfyUI-R1自动化工作流创建；PartCrafter革新3D生成；Magistral推出透明多语言推理系统；AniMaker实现AI动画制作。

---

## 目录  

1. [MiniCPM4：超高效端侧大模型系列](#minicpm4超高效端侧大模型系列)
2. [Seedance 1.0：高性能文本和图像驱动视频生成模型](#seedance-10高性能文本和图像驱动视频生成模型)
3. [PartCrafter：结构化 3D 网格生成的创新探索](#partcrafter结构化-3d-网格生成的创新探索)
4. [ComfyUI‑R1：首个大规模推理模型驱动的自动工作流生成系统](#comfyuir1首个大规模推理模型驱动的自动工作流生成系统)
5. [Magistral：首个开源高透明多语种推理模型](#magistral首个开源高透明多语种推理模型)
6. [AniMaker：多智能体动画叙事生成系统](#animaker多智能体动画叙事生成系统)
7. [Frame Guidance：无需训练的帧级视频扩散可控框架](#frame-guidance-video无需训练的帧级视频扩散可控框架)

---

## MiniCPM4：超高效端侧大模型系列

![MiniCPM4 Evaluation 图](https://arxiv.org/html/2506.07900v1/x1.png)

**概要**：**MiniCPM4** 是 **OpenBMB** 团队推出的高效端侧大模型家族，提供 0.5B 和 8B 两个版本。该系列通过在模型结构、预训练数据、高效训练算法与推理系统四个维度的联动优化，实现了在主流相同性能下的极致加速体验。例如 8B 版本采用可训练稀疏注意力 InfLLM v2，构建 UltraClean 高质量语料并配合 UltraChat v2 精调数据，同时辅以 ModelTunnel v2 的训练调度，和 CPM.cu 的推理优化，在长文本处理效率方面超越 Qwen3-8B，并在端侧设备上实现 5 倍以上生成速度提升。

**标签**：#端侧LLM #稀疏注意力 #高效预训练 #模型量化 #长文本处理

---

## Seedance 1.0：高性能文本和图像驱动视频生成模型

![Seedance Leaderboard 图](hhttps://arxiv.org/html/2506.09113v1/x4.png)

**概要**：**Seedance 1.0** 是由 **字节跳动 Seed 团队** 最新发布的基础视频生成模型，旨在解决文本或图像驱动视频生成中的关键挑战：提示执行、运动连贯性和视觉质量之间的平衡。研究团队通过多源数据精细标注训练样本，并设计了支持单段与多段生成任务的高效架构，自然融合了文本到视频和图像到视频的任务能力。同时，项目采用后训练阶段精调与 RLHF 的强化学习策略，并通过多阶段蒸馏与系统级优化，实现了约10倍的推理加速。在 NVIDIA L20 环境下，Seedance 可在约41秒内生成一段1080p、5秒长的高质量视频，展现出出色的时空流畅性、结构稳定性与镜头叙事能力。该模型已在六月集成至多平台，并在业内指标中全面领先，同时提供 API 接入与上线支持。

**标签**：#视频生成 #文本到视频 #图像到视频 #RLHF #多段生成 #高效推理

---

## PartCrafter：结构化 3D 网格生成的创新探索

![PartCrafter Teaser 图](https://wgsxm.github.io/projects/partcrafter/static/images/teaser.png)

**概要**：**PartCrafter** 是由**北京大学、字节跳动和卡内基·梅隆大学**联合提出的首个结构化 3D 生成模型，能够从单张 RGB 图像一次性生成多个语义明确、几何独立的 3D 网格部件。该方法摒弃了传统的 “先分割再重建” 流程，通过组合式潜在空间和层次注意力机制，实现端到端的零预分割多部件生成。模型基于预训练的 3D Mesh Diffusion Transformer（DiT），构建可独立编辑和组合的部件标记，并使用本地–全局注意力维持整体一致性。团队进一步制作了覆盖 50 k+ 多部件对象和 300 k 部件的数据集，在 Objaverse、ShapeNet 等基准上展示出优于现有方法的分割网格生成质量与生成速度：生成时间从 18 分钟缩至 34 秒，F‑Score 提升至约 0.7472，Chamfer Distance 降至 0.1726 。

**标签**：#3D生成 #结构化网格 #DiffusionTransformer #多部件

---

## ComfyUI‑R1：首个大规模推理模型驱动的自动工作流生成系统

![ComfyUI‑R1 Comparison 图](https://arxiv.org/html/2506.09790v1/x3.png)

**概要**：**ComfyUI‑R1** 是 **阿里巴巴 AIDC 团队** 专为 **ComfyUI** 平台设计的首个大型推理模型，采用长链式思维（Chain-of-Thought）和强化学习策略，显著提升自动生成图像创作工作流的能力。研究团队构建了包含 \~4 K 完整 ComfyUI 工作流的数据集，基于 Qwen2.5-Coder-7B-Instruct 进行两阶段训练：首先以监督微调方式适应领域，随后使用规则与评估指标混合奖励进行强化学习优化，确保生成工作流在格式、结构及节点精度方面都非常可靠。实验结果表明，该模型达到了 97% 格式有效性，并在节点级别和图级别的 F1 分数等指标上大幅超越 GPT‑4o 和闭源模型，进一步证明了长链推理模型在 AI 艺术生成中的实用性与潜力。

**标签**：#工作流生成 #长链推理 #强化学习 #自动化 #ComfyUI 

---

## Magistral：首个开源高透明多语种推理模型

![Magistral Comparison 图](https://arxiv.org/html/2506.10910v1/extracted/6535757/images/magistral-vs-r1.png)

**概要**：**Magistral** 是由 **Mistral AI** 发布的首个高透明度、多语种推理语言模型系列，分为 Medium 版和 Small（24B）版，采用自研纯强化学习（RL）训练流程，自底向上构建推理能力。该模型的核心亮点在于其透明可审计的逐步逻辑链（chain-of-thought）推理，覆盖专业领域问题解答，并且在多语言（包括英语、法语、西班牙语、中文等）中都保持一致的推理连贯性。Magistral 能以极高的速度处理复杂任务，在本地实现对话级 reasoning 实时响应，适合合规要求高、需要检验逻辑来源的场景。

**标签**：#推理语言模型 #透明推理 #强化学习 #多语种 #可审计

---

## AniMaker：多智能体动画叙事生成系统

![AniMaker Overview 图](https://arxiv.org/html/2506.10540v1/extracted/6535659/figures/model.png)

**概要**：**AniMaker** 是由 **哈尔滨工业大学影视与多模态生成团队（HITsz-TMG）** 提出的多智能体动画叙事生成系统，旨在从文本输入自动生成连贯的多场景动画视频。系统引入了导演（Director）代理生成分镜脚本、摄影（Photography）代理生成多个候选动画片段、评审（Reviewer）代理基于 AniEval 模型挑选最佳片段，以及后期（Post-Production）代理负责剪辑整合与配音。AniMaker 采用蒙特卡罗树搜索（MCTS-Gen）策略提升生成效率，并使用 AniEval 模型衡量剧情连贯性、动作完成度等叙事指标。实验显示，其生成的动画在 VBench 和 AniEval 等多项评价体系中均优于现有方法，同时在资源使用效率上也显著提升 。

**标签**：#多智能体 #动画生成 #MCTS #叙事一致性 #视频生成框架

---

## Frame Guidance：无需训练的帧级视频扩散可控框架

![Frame Guidance Overview 图](https://frame-guidance-video.github.io/figure/overview.jpg)
**概要**：**Frame Guidance** 是一个基于视频扩散模型的新型训练免疫控制方法，由 **KAIST**，**Adobe Research** 等提出。该方法通过帧级引导信息（如关键帧、风格图、草图或深度图）直接作用于扩散过程，无需对视频模型进行任何微调或训练。其核心在于对潜在隐变量进行“帧级优化”，显著降低显存开销，并实现全球一致的视频质量。该机制支持多样化控制任务，包括关键帧重定向、风格迁移、循环视频生成等，并可与任何现有视频生成模型兼容。实验证明，在保持高视频质量和时序连贯性的同时，Frame Guidance 能有效增强可控性，是一种高效、通用的多任务视频扩散指导手段。

**标签**：#视频生成 #帧级控制 #扩散模型 #训练免疫 #多任务可控 

---

### **参考链接**
1. [MiniCPM4 Github 仓库](https://github.com/openbmb/minicpm)
2. [MiniCPM4 论文](https://arxiv.org/html/2506.07900v1)
3. [Seedance 项目页面](https://seed.bytedance.com/zh/seedance)
4. [Seedance 论文](https://arxiv.org/html/2506.09113)
5. [PartCrafter 项目页面](https://wgsxm.github.io/projects/partcrafter/)
6. [PartCrafter Github 仓库](https://github.com/wgsxm/PartCrafter)
7. [PartCrafter 论文](https://arxiv.org/html/2506.05573)
8. [ComfyUI-Copilot Github 仓库](https://github.com/AIDC-AI/ComfyUI-Copilot)
9. [ComfyUI-R1 论文](https://arxiv.org/html/2506.09790)
10. [Magistral 论文](https://arxiv.org/html/2506.10910)
11. [AniMaker Github 仓库](https://github.com/HITsz-TMG/Anim-Director)
12. [AniMaker 论文](https://arxiv.org/html/2506.10540)
13. [Frame Guidance 项目页面](https://frame-guidance-video.github.io/)
14. [Frame Guidance Github 仓库](https://github.com/agwmon/frame-guidance)
15. [Frame Guidance 论文](https://arxiv.org/html/2506.07177)
