# MILS零训练多模态理解|LayerTracer优化SVG生成|OmniHuman高保真数字人【AI周报】

![封面源自C站作者Clear_Note](https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/e958ef00-1183-47ee-91f5-c1f1023f08a3/anim=false,width=450/55557070.jpeg)

## 摘要  

本周亮点：MILS展示了无需训练的多模态理解方法；LayerTracer通过DiT改进SVG生成；MakeAnything利用DiT优化程序化序列生成；OmniHuman生成高保真数字人视频；MatAnyone提供稳定的视频抠像方案；s1模型在推理任务中展现强劲实力。详情见正文。

---

## 目录

1. [MILS：无需训练的多模态理解系统](#mils无需训练的多模态理解系统)  
2. [LayerTracer：通过扩散Transformer实现认知对齐的分层SVG合成](#layertracer通过扩散transformer实现认知对齐的分层svg合成)  
3. [MakeAnything：利用扩散Transformer进行多领域程序化序列生成](#makeanything利用扩散transformer进行多领域程序化序列生成)  
4. [OmniHuman-1：字节跳动推出的AI模型，实现从单张图像生成高度逼真的人类视频](#omnihuman-1字节跳动推出的ai模型实现从单张图像生成高度逼真的人类视频)  
5. [MatAnyone：稳定的视频抠像框架](#matanyone稳定的视频抠像框架)  
6. [s1 模型：李飞飞团队提出的高效推理模型](#s1-模型李飞飞团队提出的高效推理模型)  

---

## MILS：无需训练的多模态理解系统

![MILS Teaser 图](https://github.com/facebookresearch/MILS/raw/main/teaser.png)

**概要**：**MILS**（Multimodal Iterative LLM
Solver）是由**Meta AI**提出的一种新型多模态理解系统。与传统的多模态模型不同，MILS **不需要额外训练**，而是利用现有大型语言模型（LLMs）的推理能力，让其通过文本描述的方式来处理图像和音频数据。核心思想是**将非文本模态转换为语言提示**，让 LLMs 能够直接解析图像和音频内容，而无需调整模型参数。这种方法表明，LLMs 具备强大的多模态推理能力，即便未经过专门的视觉或音频训练，也能在多模态任务上表现出色。MILS 在多个多模态任务中取得了接近或超越现有专门训练模型的效果，表明这一方向的可行性。  

**标签**：#MILS #Meta #多模态理解 #零样本学习 #LLM

---

## LayerTracer：通过扩散Transformer实现认知对齐的分层SVG合成

![LayerTracer Teaser 图](https://github.com/showlab/LayerTracer/raw/main/img/teaser.png)
**概要**：**LayerTracer** 是由新加坡国立大学的 **Show Lab** 提出的一个框架，旨在生成与人类认知相一致的分层SVG。该方法采用Diffusion Transformer架构，模拟设计师创建分层SVG的过程。具体而言，LayerTracer首先通过文本条件的DiT生成多阶段的栅格化蓝图，模拟人类设计工作流程。随后，通过逐层矢量化和路径去重，生成干净、可编辑的SVG。实验结果表明，LayerTracer在生成质量和可编辑性方面优于基于优化和神经网络的基线方法，有效地将AI生成的矢量与专业设计认知对齐。

**标签**：#LayerTracer #SVG合成 #DiT #ShowLab

---

## MakeAnything：利用扩散Transformer进行多领域程序化序列生成

![MakeAnything Teaser图](https://raw.githubusercontent.com/showlab/MakeAnything/main/images/teaser.png)

**概要**：**MakeAnything** 是由 **Show Lab** 团队开发的一个框架，旨在生成多步骤的程序化教程，涵盖绘画、手工艺、烹饪等多种任务。该框架基于Diffusion Transformer（DiT），通过微调激活DiT的上下文能力，以生成逻辑连贯且视觉一致的步骤序列。此外，MakeAnything引入了用于图像生成的非对称低秩适应（LoRA）方法，在冻结编码器参数的同时，自适应地调整解码器层，以平衡泛化能力和任务特定性能。其ReCraft模型通过时空一致性约束，实现了从静态图像到过程的生成，使静态图像能够被分解为合理的创建序列。实验结果表明，MakeAnything在程序化生成任务上超越了现有方法，设定了新的性能基准。

**标签**：#程序化生成 #DiffusionTransformer #多领域 #LoRA 

---

## OmniHuman-1：字节跳动推出的AI模型，实现从单张图像生成高度逼真的人类视频

![OmniHuman-1 Framework 图](https://arxiv.org/html/2502.01061v1/extracted/6173608/imgs/framework.jpg)
**概要**：**字节跳动（ByteDance）** 近期推出了一款名为 **OmniHuman-1** 的模型，该模型能够从单张图像和音频生成高度逼真的人类视频。OmniHuman-1采用了多模态运动条件混合训练策略，使其能够利用音频、视频或两者结合的运动信号，生成逼真的人类视频。 该模型支持任意纵横比的图像输入，包括肖像、半身和全身图像，生成的结果在动作、光照和纹理细节等方面都极为逼真。

**标签**：#AI #OmniHuman #字节跳动 #深度伪造 #多模态训练 

---

## MatAnyone：稳定的视频抠像框架

![MatAnyone Pipeline 图](https://pq-yang.github.io/projects/MatAnyone/assets/images_mat/pipeline.png)

**概要**：**MatAnyone** 是由 **南洋理工大学 S-Lab** 与 **商汤科技新加坡研究团队** 合作开发的一个实用性强的人物视频抠像框架。该框架支持目标指定，在核心区域的语义和精细边界细节方面表现稳定。通过引入一致的内存传播模块，MatAnyone能够自适应地融合前一帧的内存信息，确保核心区域的语义稳定性，同时保留对象边界的精细细节。此外，研究团队还提出了一种新的训练策略，有效利用大规模分割数据，提升抠像的稳定性。实验结果表明，MatAnyone在多种真实世界场景中实现了稳健且准确的视频抠像效果，优于现有方法。 

**标签**：#视频抠像 #内存传播 #目标指定 #S-Lab

---

## s1 模型：李飞飞团队提出的高效推理模型

![s1 Scaling 图](https://github.com/simplescaling/s1/raw/main/visuals/scaling.png)

**概要**：**李飞飞团队** 近期发布了名为 **s1** 的人工智能推理模型。该模型通过在阿里巴巴开源的 Qwen2.5-32B-Instruct 模型上进行监督微调，仅使用 1000 个精心设计的问题数据集，训练耗时约 26 分钟，成本不到 50 美元。实验结果显示，s1 在数学和编码能力测试中的表现与 OpenAI 的 o1 和 DeepSeek 的 R1 等尖端推理模型相当，甚至在竞赛数学问题上的表现比 o1-preview 高出 27%。

**标签**：#推理模型 #李飞飞 #s1 #Qwen2.5-32B-Instruct 

---

### **参考文献**  

1. [MILS GitHub](https://github.com/facebookresearch/MILS)
2. [MILS 论文](https://arxiv.org/pdf/2501.18096)
3. [LayerTracer GitHub](https://github.com/showlab/LayerTracer)
4. [LayerTracer 论文](https://arxiv.org/html/2502.01105)
5. [MakeAnything GitHub](https://github.com/showlab/MakeAnything)
6. [MakeAnything 论文](https://arxiv.org/html/2502.01572v2)
7. [OmniHuman 官网](https://omnihuman-lab.github.io/)
8. [OmniHuman 论文](https://arxiv.org/pdf/2502.01061)
9. [MatAnyone 官网](https://pq-yang.github.io/projects/MatAnyone/)
10. [MatAnyone 论文](https://arxiv.org/html/2501.14677v1)
11. [s1 GitHub](https://github.com/simplescaling/s1)
12. [s1 论文](https://arxiv.org/html/2501.19393v2)
